{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 15:39:17.391087: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-07 15:39:17.668482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Explainable AI tools:\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from alibi.explainers import AnchorTabular # why not used the original anchor package?\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam  # Import the Adam optimizer\n",
    "\n",
    "# MCDM:\n",
    "import pymcdm\n",
    "\n",
    "# Rank Aggregation:\n",
    "from ranx import Run, fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Job</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Saving accounts</th>\n",
       "      <th>Checking account</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Credit Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>NaN</td>\n",
       "      <td>little</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>moderate</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>education</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>car</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
       "0           0   67    male    2     own             NaN           little   \n",
       "1           1   22  female    2     own          little         moderate   \n",
       "2           2   49    male    1     own          little              NaN   \n",
       "3           3   45    male    2    free          little           little   \n",
       "4           4   53    male    2    free          little           little   \n",
       "\n",
       "   Credit amount  Duration              Purpose  Credit Risk  \n",
       "0           1169         6             radio/TV            1  \n",
       "1           5951        48             radio/TV            2  \n",
       "2           2096        12            education            1  \n",
       "3           7882        42  furniture/equipment            1  \n",
       "4           4870        24                  car            2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Job</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Credit Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>476.500000</td>\n",
       "      <td>35.501048</td>\n",
       "      <td>1.909853</td>\n",
       "      <td>3279.112159</td>\n",
       "      <td>20.780922</td>\n",
       "      <td>1.302935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>275.540378</td>\n",
       "      <td>11.379668</td>\n",
       "      <td>0.649681</td>\n",
       "      <td>2853.315158</td>\n",
       "      <td>12.046483</td>\n",
       "      <td>0.459768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>238.250000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1360.250000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>476.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2302.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>714.750000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3975.250000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>953.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18424.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         Age         Job  Credit amount    Duration  \\\n",
       "count  954.000000  954.000000  954.000000     954.000000  954.000000   \n",
       "mean   476.500000   35.501048    1.909853    3279.112159   20.780922   \n",
       "std    275.540378   11.379668    0.649681    2853.315158   12.046483   \n",
       "min      0.000000   19.000000    0.000000     250.000000    4.000000   \n",
       "25%    238.250000   27.000000    2.000000    1360.250000   12.000000   \n",
       "50%    476.500000   33.000000    2.000000    2302.500000   18.000000   \n",
       "75%    714.750000   42.000000    2.000000    3975.250000   24.000000   \n",
       "max    953.000000   75.000000    3.000000   18424.000000   72.000000   \n",
       "\n",
       "       Credit Risk  \n",
       "count   954.000000  \n",
       "mean      1.302935  \n",
       "std       0.459768  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       2.000000  \n",
       "max       2.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        954 non-null    int64 \n",
      " 1   Age               954 non-null    int64 \n",
      " 2   Sex               954 non-null    object\n",
      " 3   Job               954 non-null    int64 \n",
      " 4   Housing           954 non-null    object\n",
      " 5   Saving accounts   779 non-null    object\n",
      " 6   Checking account  576 non-null    object\n",
      " 7   Credit amount     954 non-null    int64 \n",
      " 8   Duration          954 non-null    int64 \n",
      " 9   Purpose           954 non-null    object\n",
      " 10  Credit Risk       954 non-null    int64 \n",
      "dtypes: int64(6), object(5)\n",
      "memory usage: 82.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of the categorical features:\n",
      "\t- Sex: ['male' 'female']\n",
      "\t- Housing: ['own' 'free' 'rent']\n",
      "\t- Saving accounts: [nan 'little' 'quite rich' 'rich' 'moderate']\n",
      "\t- Checking account: ['little' 'moderate' nan 'rich']\n",
      "\t- Purpose: ['radio/TV' 'education' 'furniture/equipment' 'car' 'business'\n",
      " 'domestic appliances' 'repairs' 'vacation/others']\n"
     ]
    }
   ],
   "source": [
    "original_data = pd.read_csv('data/german_credit_data_updated.csv')\n",
    "\n",
    "# Dataset overview - German Credit Risk (from Kaggle):\n",
    "# 1. Age (numeric)\n",
    "# 2. Sex (text: male, female)\n",
    "# 3. Job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\n",
    "# 4. Housing (text: own, rent, or free)\n",
    "# 5. Saving accounts (text - little, moderate, quite rich, rich)\n",
    "# 6. Checking account (numeric, in DM - Deutsch Mark)\n",
    "# 7. Credit amount (numeric, in DM)\n",
    "# 8. Duration (numeric, in month)\n",
    "# 9. Purpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)\n",
    "\n",
    "display(original_data.head())\n",
    "display(original_data.describe())\n",
    "display(original_data.info())\n",
    "\n",
    "# Display the unique values of thprecision=3, e categorical features:\n",
    "print('Unique values of the categorical features:')\n",
    "for col in original_data.select_dtypes(include='object'):\n",
    "    print(f'\\t- {col}: {original_data[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: Index(['Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account',\n",
      "       'Purpose'],\n",
      "      dtype='object')\n",
      "Numerical features: Index(['Age', 'Credit amount', 'Duration'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Credit_amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Credit_Risk</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Job_highlyskilled</th>\n",
       "      <th>Job_skilled</th>\n",
       "      <th>Job_unskilled_nonresident</th>\n",
       "      <th>Job_unskilled_resident</th>\n",
       "      <th>Housing_free</th>\n",
       "      <th>Housing_own</th>\n",
       "      <th>Housing_rent</th>\n",
       "      <th>Saving_accounts_little</th>\n",
       "      <th>Saving_accounts_moderate</th>\n",
       "      <th>Saving_accounts_none</th>\n",
       "      <th>Saving_accounts_quite_rich</th>\n",
       "      <th>Saving_accounts_rich</th>\n",
       "      <th>Checking_account_little</th>\n",
       "      <th>Checking_account_moderate</th>\n",
       "      <th>Checking_account_none</th>\n",
       "      <th>Checking_account_rich</th>\n",
       "      <th>Purpose_business</th>\n",
       "      <th>Purpose_car</th>\n",
       "      <th>Purpose_domestic_appliances</th>\n",
       "      <th>Purpose_education</th>\n",
       "      <th>Purpose_furniture_equipment</th>\n",
       "      <th>Purpose_radio_TV</th>\n",
       "      <th>Purpose_repairs</th>\n",
       "      <th>Purpose_vacation_others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Credit_amount  Duration  Credit_Risk  Sex_female  Sex_male  \\\n",
       "0   67           1169         6            0           0         1   \n",
       "1   22           5951        48            1           1         0   \n",
       "2   49           2096        12            0           0         1   \n",
       "3   45           7882        42            0           0         1   \n",
       "4   53           4870        24            1           0         1   \n",
       "\n",
       "   Job_highlyskilled  Job_skilled  Job_unskilled_nonresident  \\\n",
       "0                  0            1                          0   \n",
       "1                  0            1                          0   \n",
       "2                  0            0                          0   \n",
       "3                  0            1                          0   \n",
       "4                  0            1                          0   \n",
       "\n",
       "   Job_unskilled_resident  Housing_free  Housing_own  Housing_rent  \\\n",
       "0                       0             0            1             0   \n",
       "1                       0             0            1             0   \n",
       "2                       1             0            1             0   \n",
       "3                       0             1            0             0   \n",
       "4                       0             1            0             0   \n",
       "\n",
       "   Saving_accounts_little  Saving_accounts_moderate  Saving_accounts_none  \\\n",
       "0                       0                         0                     1   \n",
       "1                       1                         0                     0   \n",
       "2                       1                         0                     0   \n",
       "3                       1                         0                     0   \n",
       "4                       1                         0                     0   \n",
       "\n",
       "   Saving_accounts_quite_rich  Saving_accounts_rich  Checking_account_little  \\\n",
       "0                           0                     0                        1   \n",
       "1                           0                     0                        0   \n",
       "2                           0                     0                        0   \n",
       "3                           0                     0                        1   \n",
       "4                           0                     0                        1   \n",
       "\n",
       "   Checking_account_moderate  Checking_account_none  Checking_account_rich  \\\n",
       "0                          0                      0                      0   \n",
       "1                          1                      0                      0   \n",
       "2                          0                      1                      0   \n",
       "3                          0                      0                      0   \n",
       "4                          0                      0                      0   \n",
       "\n",
       "   Purpose_business  Purpose_car  Purpose_domestic_appliances  \\\n",
       "0                 0            0                            0   \n",
       "1                 0            0                            0   \n",
       "2                 0            0                            0   \n",
       "3                 0            0                            0   \n",
       "4                 0            1                            0   \n",
       "\n",
       "   Purpose_education  Purpose_furniture_equipment  Purpose_radio_TV  \\\n",
       "0                  0                            0                 1   \n",
       "1                  0                            0                 1   \n",
       "2                  1                            0                 0   \n",
       "3                  0                            1                 0   \n",
       "4                  0                            0                 0   \n",
       "\n",
       "   Purpose_repairs  Purpose_vacation_others  \n",
       "0                0                        0  \n",
       "1                0                        0  \n",
       "2                0                        0  \n",
       "3                0                        0  \n",
       "4                0                        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 30 columns):\n",
      " #   Column                       Non-Null Count  Dtype\n",
      "---  ------                       --------------  -----\n",
      " 0   Age                          954 non-null    int64\n",
      " 1   Credit_amount                954 non-null    int64\n",
      " 2   Duration                     954 non-null    int64\n",
      " 3   Credit_Risk                  954 non-null    int64\n",
      " 4   Sex_female                   954 non-null    int64\n",
      " 5   Sex_male                     954 non-null    int64\n",
      " 6   Job_highlyskilled            954 non-null    int64\n",
      " 7   Job_skilled                  954 non-null    int64\n",
      " 8   Job_unskilled_nonresident    954 non-null    int64\n",
      " 9   Job_unskilled_resident       954 non-null    int64\n",
      " 10  Housing_free                 954 non-null    int64\n",
      " 11  Housing_own                  954 non-null    int64\n",
      " 12  Housing_rent                 954 non-null    int64\n",
      " 13  Saving_accounts_little       954 non-null    int64\n",
      " 14  Saving_accounts_moderate     954 non-null    int64\n",
      " 15  Saving_accounts_none         954 non-null    int64\n",
      " 16  Saving_accounts_quite_rich   954 non-null    int64\n",
      " 17  Saving_accounts_rich         954 non-null    int64\n",
      " 18  Checking_account_little      954 non-null    int64\n",
      " 19  Checking_account_moderate    954 non-null    int64\n",
      " 20  Checking_account_none        954 non-null    int64\n",
      " 21  Checking_account_rich        954 non-null    int64\n",
      " 22  Purpose_business             954 non-null    int64\n",
      " 23  Purpose_car                  954 non-null    int64\n",
      " 24  Purpose_domestic_appliances  954 non-null    int64\n",
      " 25  Purpose_education            954 non-null    int64\n",
      " 26  Purpose_furniture_equipment  954 non-null    int64\n",
      " 27  Purpose_radio_TV             954 non-null    int64\n",
      " 28  Purpose_repairs              954 non-null    int64\n",
      " 29  Purpose_vacation_others      954 non-null    int64\n",
      "dtypes: int64(30)\n",
      "memory usage: 223.7 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.7694545 , -0.7399179 , -1.22763429, ...,  1.62518349,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-1.18704073,  0.93690642,  2.26068929, ...,  1.62518349,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [ 1.18685641, -0.41486224, -0.72930235, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       ...,\n",
       "       [-1.0111965 , -0.39768023,  1.26402541, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-0.65950803,  0.29240557,  0.26736153, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-0.83535227,  2.69823821,  1.26402541, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_data = original_data.copy()\n",
    "\n",
    "# For savings and checking accounts, we will replace the missing values with 'none':\n",
    "preprocessed_data['Saving accounts'].fillna('none', inplace=True)\n",
    "preprocessed_data['Checking account'].fillna('none', inplace=True)\n",
    "\n",
    "# Dropping index column:\n",
    "preprocessed_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Using pd.dummies to one-hot-encode the categorical features\n",
    "preprocessed_data[\"Job\"] = preprocessed_data[\"Job\"].map({0: 'unskilled_nonresident', 1: 'unskilled_resident',\n",
    "                                                         2: 'skilled', 3: 'highlyskilled'})\n",
    "\n",
    "categorical_features = preprocessed_data.select_dtypes(include='object').columns\n",
    "numerical_features = preprocessed_data.select_dtypes(include='number').columns.drop('Credit Risk')\n",
    "print(f'Categorical features: {categorical_features}')\n",
    "print(f'Numerical features: {numerical_features}')\n",
    "\n",
    "preprocessed_data = pd.get_dummies(preprocessed_data, columns=categorical_features, dtype='int64')\n",
    "\n",
    "# Remapping the target variable to 0 and 1:\n",
    "preprocessed_data['Credit Risk'] = preprocessed_data['Credit Risk'].map({1: 0, 2: 1})\n",
    "\n",
    "# Make sure all column names are valid python identifiers (important for pd.query() calls):\n",
    "preprocessed_data.columns = preprocessed_data.columns.str.replace(' ', '_')\n",
    "preprocessed_data.columns = preprocessed_data.columns.str.replace('/', '_')\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "scaled_preprocessed_data = scaler.fit_transform(preprocessed_data)\n",
    "\n",
    "display(preprocessed_data.head())\n",
    "display(preprocessed_data.info())\n",
    "\n",
    "display(scaled_preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = preprocessed_data['Credit_Risk']\n",
    "X = preprocessed_data.drop(columns='Credit_Risk')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7696335078534031\n",
      "ROC AUC: 0.6830357142857143\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining explanation model wrapper (for standard behaviour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainerWrapper:\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        self.clf = clf\n",
    "\n",
    "        if hasattr(clf, 'predict_proba') and predict_proba is None:\n",
    "            self.predict_proba = clf.predict_proba\n",
    "        elif predict_proba is not None:\n",
    "            self.predict_proba = predict_proba\n",
    "        else:\n",
    "            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.categorical_feature_names = categorical_feature_names\n",
    "\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "class LimeWrapper(ExplainerWrapper):\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "        \n",
    "        self.explainer = LimeTabularExplainer(self.X_train.values, feature_names=self.X_train.columns, discretize_continuous=False)\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        lime_exp = self.explainer.explain_instance(instance_data_row, self.predict_proba, num_features=len(self.X_train.columns))\n",
    "        \n",
    "        ranking = pd.DataFrame(lime_exp.as_list(), columns=['feature', 'score'])\n",
    "        return ranking\n",
    "\n",
    "class ShapTabularTreeWrapper(ExplainerWrapper):\n",
    "    \n",
    "        def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None, **additional_explainer_args):\n",
    "            super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "            \n",
    "            self.explainer = shap.TreeExplainer(clf, self.X_train, **additional_explainer_args)\n",
    "        \n",
    "        def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "            shap_values = self.explainer.shap_values(instance_data_row)\n",
    "    \n",
    "            ranking = pd.DataFrame(list(zip(self.X_train.columns, shap_values[:, 0])), columns=['feature', 'score'])\n",
    "            ranking = ranking.sort_values(by='score', ascending=False, key=lambda x: abs(x)).reset_index(drop=True)\n",
    "            \n",
    "            return ranking\n",
    "\n",
    "class AnchorWrapper(ExplainerWrapper):\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "        \n",
    "        self.explainer = AnchorTabular(predictor=self.predict_proba, feature_names=self.X_train.columns) # TODO: fix parameters\n",
    "        self.explainer.fit(self.X_train.values)\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        if isinstance(instance_data_row, pd.Series):\n",
    "            instance_data_row = instance_data_row.to_numpy()\n",
    "\n",
    "        feature_importances = {feature: 0 for feature in self.X_train.columns}\n",
    "        explanation = self.explainer.explain(instance_data_row)\n",
    "        \n",
    "        for rule in explanation.anchor:\n",
    "            # Extract the feature name from the rule string\n",
    "            # This method won't work for column names that have spaces in them or that don't contain any letters\n",
    "            for expression_element in rule.split():\n",
    "                if any(c.isalpha() for c in expression_element):\n",
    "                    referenced_feature = expression_element\n",
    "                    break\n",
    "\n",
    "            rule_coverage = self.X_train.query(rule).shape[0] / self.X_train.shape[0]\n",
    "            feature_importances[referenced_feature] = 1 - rule_coverage\n",
    "        \n",
    "        return pd.DataFrame(list(feature_importances.items()), columns=['feature', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining autoencoder noisy data generator\n",
    "(to be used on calculating sensitivity metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderNoisyDataGenerator():\n",
    "    def __init__(self, X: pd.DataFrame, ohe_categorical_features_names: list[str], encoding_dim: int = 5, epochs=500):\n",
    "        self.X = X\n",
    "        self.categorical_features_names = ohe_categorical_features_names\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.epochs = epochs\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        self.X_scaled = scaler.fit_transform(self.X)\n",
    "        \n",
    "        input_dim = self.X_scaled.shape[1]\n",
    "\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(self.encoding_dim, activation='relu')(input_layer)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        self.encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "        self.was_fit = False\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        self.autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        self.autoencoder.fit(self.X_scaled, self.X_scaled, epochs=self.epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "        # Extract hidden layer representation:\n",
    "        self.hidden_representation = self.encoder.predict(self.X_scaled)\n",
    "        self.was_fit = True\n",
    "\n",
    "\n",
    "    def generate_noisy_data(self, num_features_to_replace: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame containing a noisy variation of the data.\n",
    "\n",
    "        The noise is generated by swapping the values of a small number of features between a sample and a random close neighbor.\n",
    "        To determine the neighbors, we use an autoencoder to reduce the dimensionality of the data and then calculate the use the NearestNeightbors algorithm in the reduced space.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.was_fit:\n",
    "            raise ValueError('The autoencoder has not been fitted yet. Call the fit() method before generating noisy data.')\n",
    "\n",
    "        # Compute Nearest Neighbors using hidden_representation\n",
    "        nbrs = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(self.hidden_representation)\n",
    "        distances, indices = nbrs.kneighbors(self.hidden_representation)\n",
    "\n",
    "        X_noisy = self.X.copy()\n",
    "\n",
    "        # Get id's of columns that belong to the same categorical feature (after being one-hot-encodeded);\n",
    "        # Columns that belong to the same categorical feature start with the same name, and will be treated as a single feature when adding noise.\n",
    "        categorical_features_indices = [\n",
    "            [self.X.columns.get_loc(col_name) for col_name in self.X.columns if col_name.startswith(feature)]\n",
    "            for feature in self.categorical_features_names\n",
    "        ]\n",
    "\n",
    "        # Replace features with random neighbor's features\n",
    "        for i in range(self.X.shape[0]):  # Iterate over each sample\n",
    "            available_features_to_replace = list(range(self.X.shape[1]))\n",
    "            for j in range(num_features_to_replace):\n",
    "                # Select features to replace; if the feture selected belong to one of the lists in categorical_features_indices, we will replace all the features in that list\n",
    "                features_to_replace = np.random.choice(available_features_to_replace, 1)\n",
    "                for feature_indices in categorical_features_indices:\n",
    "                    if features_to_replace in feature_indices:\n",
    "                        features_to_replace = feature_indices\n",
    "                        break\n",
    "                \n",
    "                # Remove the selected features from the list of available features to replace\n",
    "                available_features_to_replace = [f for f in available_features_to_replace if f not in features_to_replace]\n",
    "\n",
    "                # Choose a random neighbor from the nearest neighbors\n",
    "                neighbor_idx = np.random.choice(indices[i][1:])\n",
    "\n",
    "                # Replace the selected features with the neighbor's features\n",
    "                X_noisy.iloc[i, features_to_replace] = self.X.iloc[neighbor_idx, features_to_replace]\n",
    "\n",
    "        return X_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2269 - val_loss: 1.2240\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2481 - val_loss: 1.2068\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2139 - val_loss: 1.1903\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1858 - val_loss: 1.1742\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1953 - val_loss: 1.1585\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1587 - val_loss: 1.1429\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1240 - val_loss: 1.1272\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1496 - val_loss: 1.1121\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0885 - val_loss: 1.0969\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1249 - val_loss: 1.0827\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mean Absolute Difference: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.676097075970533"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage Example:\n",
    "autoencoder_noisy_data_generator = AutoencoderNoisyDataGenerator(X_train, categorical_features, encoding_dim=5, epochs=10)\n",
    "autoencoder_noisy_data_generator.fit()\n",
    "noisy_data = autoencoder_noisy_data_generator.generate_noisy_data(num_features_to_replace=2)\n",
    "display(\"Mean Absolute Difference: \", np.mean(np.abs(X_train - noisy_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining evaluator class to hold evaluation metric methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XaiEvaluator:\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, ohe_categorical_feature_names: list[str], predict_proba: callable = None, noise_gen_args: dict = {}):\n",
    "        self.clf = clf\n",
    "        if hasattr(clf, 'predict_proba') and predict_proba is None:\n",
    "            self.predict_proba = clf.predict_proba\n",
    "        elif predict_proba is not None:\n",
    "            self.predict_proba = predict_proba\n",
    "        else:\n",
    "            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.ohe_categorical_feature_names = ohe_categorical_feature_names\n",
    "\n",
    "        self.categorical_features_indices = [\n",
    "            [self.X_train.columns.get_loc(col_name) for col_name in self.X_train.columns if col_name.startswith(feature)]\n",
    "            for feature in self.ohe_categorical_feature_names\n",
    "        ]\n",
    "\n",
    "        self.noisy_data_generator = AutoencoderNoisyDataGenerator(X_train, ohe_categorical_feature_names, **noise_gen_args)\n",
    "\n",
    "        self.was_initialized = False\n",
    "    \n",
    "    # Initialization opeations that take a long time to run\n",
    "    def init(self):\n",
    "        self.noisy_data_generator.fit()\n",
    "        self.was_initialized = True\n",
    "            \n",
    "    def faithfullness_correlation(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, len_subset: int = None,\n",
    "                                  iterations: int = 100, baseline_strategy: Literal[\"zeros\", \"mean\"] = \"zeros\") -> float:\n",
    "        if not isinstance(explainer, ExplainerWrapper):\n",
    "            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "        \n",
    "        dimension = len(instance_data_row)  \n",
    "\n",
    "        importance_sums = []\n",
    "        delta_fs = []\n",
    "\n",
    "        f_x = self.predict_proba(instance_data_row.to_numpy().reshape(1, -1))[0][1]\n",
    "        g_x = explainer.explain_instance(instance_data_row)\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            # Select a subset of features to perturb\n",
    "            subset = np.random.choice(instance_data_row.index.values, len_subset if len_subset else dimension/4, replace=False)\n",
    "\n",
    "            perturbed_instance = instance_data_row.copy()\n",
    "\n",
    "            if baseline_strategy == \"zeros\":\n",
    "                baseline = np.zeros(dimension)  # either mean on all zeros\n",
    "            elif baseline_strategy == \"mean\":\n",
    "                baseline = np.mean(self.X_train, axis=0)\n",
    "                for feature_index in self.categorical_features_indices:\n",
    "                    baseline[feature_index] = 0\n",
    "                \n",
    "            perturbed_instance[subset] = baseline[instance_data_row.index.get_indexer(subset)]\n",
    "\n",
    "            importance_sum = 0\n",
    "            for feature in subset:\n",
    "                importance_sum += g_x[g_x['feature'] == feature]['score'].values[0] # should I take the abs value here?\n",
    "            importance_sums.append(importance_sum)\n",
    "\n",
    "            f_x_perturbed = self.predict_proba(perturbed_instance.to_numpy().reshape(1, -1))[0][1]\n",
    "            delta_f = np.abs(f_x - f_x_perturbed)\n",
    "            delta_fs.append(delta_f)\n",
    "        \n",
    "        return abs(pearsonr(importance_sums, delta_fs).statistic)\n",
    "    \n",
    "    def sensitivity(self, ExplainerType: Type[ExplainerWrapper], instance_data_row: pd.Series, iterations: int = 10, method: Literal['mean_squared', 'spearman', 'pearson'] = 'spearman',\n",
    "                    custom_method: Callable[[pd.DataFrame, pd.DataFrame], float]=None) -> float:\n",
    "        if not self.was_initialized:\n",
    "            raise ValueError('The XaiEvaluator has not been initialized yet. Call the init() method before evaluating sensitivity.')\n",
    "        \n",
    "        original_explainer = ExplainerType(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "\n",
    "        results: list[float] = []\n",
    "        for _ in range(iterations):\n",
    "            # Obtain the original explanation:\n",
    "            original_explanation = original_explainer.explain_instance(instance_data_row)\n",
    "\n",
    "            # Obtain the noisy explanation:\n",
    "            noisy_data = self.noisy_data_generator.generate_noisy_data(num_features_to_replace=2)\n",
    "            noisy_explainer = ExplainerType(self.clf, noisy_data, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "            noisy_explanation = noisy_explainer.explain_instance(instance_data_row)\n",
    "\n",
    "            if custom_method is not None:\n",
    "                results.append(custom_method(original_explanation, noisy_explanation))\n",
    "            elif method == 'mean_squared':\n",
    "                mean_squared_difference = ((original_explanation['score'] - noisy_explanation['score']) ** 2).mean()\n",
    "                results.append(mean_squared_difference)\n",
    "            elif method == 'spearman':\n",
    "                spearman_correlation = spearmanr(original_explanation['score'], noisy_explanation['score']).correlation\n",
    "                results.append(abs(spearman_correlation))\n",
    "            elif method == 'pearson':\n",
    "                pearson_correlation = pearsonr(original_explanation['score'], noisy_explanation['score']).correlation\n",
    "                results.append(abs(pearson_correlation))\n",
    "        \n",
    "        return np.mean(results)\n",
    "\n",
    "    def complexity(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, **kwargs) -> float:\n",
    "        if not kwargs.get(\"bypass_check\", False) and not isinstance(explainer, ExplainerWrapper):\n",
    "            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "\n",
    "        explanation = explainer.explain_instance(instance_data_row)\n",
    "\n",
    "        def frac_contribution(explanation: pd.DataFrame, i: int) -> float:\n",
    "            abs_score_sum = explanation['score'].abs().sum()\n",
    "            return explanation['score'].abs()[i] / abs_score_sum\n",
    "\n",
    "        sum = 0\n",
    "        for i in range(explanation.shape[0]):\n",
    "            fc = frac_contribution(explanation, i)\n",
    "            sum += fc * np.log(fc) if fc > 0 else 0\n",
    "            \n",
    "        return -sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2429 - val_loss: 1.2328\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2228 - val_loss: 1.2142\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2202 - val_loss: 1.1969\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1996 - val_loss: 1.1802\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1479 - val_loss: 1.1634\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1609 - val_loss: 1.1471\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1449 - val_loss: 1.1313\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1408 - val_loss: 1.1156\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1408 - val_loss: 1.1002\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1346 - val_loss: 1.0850\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step\n"
     ]
    }
   ],
   "source": [
    "xai_eval = XaiEvaluator(clf, X_train, categorical_features, noise_gen_args={'encoding_dim': 5, 'epochs': 10})\n",
    "xai_eval.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness:  0.11133162798773424\n",
      "Sensitivity:  0.9327924952426636\n",
      "Complexity:  2.516308873975537\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "faithfullness =  xai_eval.faithfullness_correlation(AnchorWrapper,\n",
    "                                                    X_test.iloc[sample_idx], len_subset=10, iterations=100, baseline_strategy=\"mean\")\n",
    "print(\"Faithfulness: \", faithfullness)\n",
    "sensitivity = xai_eval.sensitivity(AnchorWrapper, X_test.iloc[sample_idx], iterations=10)\n",
    "print(\"Sensitivity: \", sensitivity)\n",
    "complexity = xai_eval.complexity(ShapTabularTreeWrapper, X_test.iloc[sample_idx])\n",
    "print(\"Complexity: \", complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19144408195771734\n"
     ]
    }
   ],
   "source": [
    "# Testing edge complexity cases:\n",
    "class TestWrapper(ExplainerWrapper):\n",
    "    def __init__(self, test_explanation):\n",
    "        self.test_explanation = test_explanation\n",
    "\n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        return self.test_explanation\n",
    "\n",
    "test_explanation = pd.DataFrame({'feature': ['a', 'b', 'c'], 'score': [1, 0.05, 0]})\n",
    "test_explainer = TestWrapper(test_explanation)\n",
    "\n",
    "complexity = xai_eval.complexity(test_explainer, X_test.iloc[sample_idx], bypass_check=True)\n",
    "print(complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xai Evaluation Metrics Trends for Selected Explanation Models\n",
    "(Takes a long time to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     23\u001b[0m shap_sensitivities\u001b[38;5;241m.\u001b[39mappend(xai_eval\u001b[38;5;241m.\u001b[39msensitivity(ShapTabularTreeWrapper, X_test\u001b[38;5;241m.\u001b[39miloc[sample_idx], iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m lime_sensitivities\u001b[38;5;241m.\u001b[39mappend(\u001b[43mxai_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLimeWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m anchor_sensitivities\u001b[38;5;241m.\u001b[39mappend(xai_eval\u001b[38;5;241m.\u001b[39msensitivity(AnchorWrapper, X_test\u001b[38;5;241m.\u001b[39miloc[sample_idx], iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     27\u001b[0m shap_faithfullnesses\u001b[38;5;241m.\u001b[39mappend(xai_eval\u001b[38;5;241m.\u001b[39mfaithfullness_correlation(shap_exp, X_test\u001b[38;5;241m.\u001b[39miloc[sample_idx], len_subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, baseline_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[42], line 78\u001b[0m, in \u001b[0;36mXaiEvaluator.sensitivity\u001b[0;34m(self, ExplainerType, instance_data_row, iterations, method, custom_method)\u001b[0m\n\u001b[1;32m     75\u001b[0m results: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Obtain the original explanation:\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     original_explanation \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_explainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_data_row\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Obtain the noisy explanation:\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     noisy_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoisy_data_generator\u001b[38;5;241m.\u001b[39mgenerate_noisy_data(num_features_to_replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 28\u001b[0m, in \u001b[0;36mLimeWrapper.explain_instance\u001b[0;34m(self, instance_data_row)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplain_instance\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance_data_row: pd\u001b[38;5;241m.\u001b[39mSeries \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m---> 28\u001b[0m     lime_exp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_data_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     ranking \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(lime_exp\u001b[38;5;241m.\u001b[39mas_list(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ranking\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lime/lime_tabular.py:452\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    448\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m    450\u001b[0m     (ret_exp\u001b[38;5;241m.\u001b[39mintercept[label],\n\u001b[1;32m    451\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mlocal_exp[label],\n\u001b[0;32m--> 452\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mscore, ret_exp\u001b[38;5;241m.\u001b[39mlocal_pred) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance_with_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43myss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_regressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_regressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    462\u001b[0m     ret_exp\u001b[38;5;241m.\u001b[39mintercept[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ret_exp\u001b[38;5;241m.\u001b[39mintercept[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lime/lime_base.py:183\u001b[0m, in \u001b[0;36mLimeBase.explain_instance_with_data\u001b[0;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[1;32m    181\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_fn(distances)\n\u001b[1;32m    182\u001b[0m labels_column \u001b[38;5;241m=\u001b[39m neighborhood_labels[:, label]\n\u001b[0;32m--> 183\u001b[0m used_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighborhood_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mlabels_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_regressor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     model_regressor \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m                             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lime/lime_base.py:134\u001b[0m, in \u001b[0;36mLimeBase.feature_selection\u001b[0;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     n_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighest_weights\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lime/lime_base.py:80\u001b[0m, in \u001b[0;36mLimeBase.feature_selection\u001b[0;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighest_weights\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     clf \u001b[38;5;241m=\u001b[39m Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m                 random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     coef \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39missparse(data):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:1161\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1152\u001b[0m _accept_sparse \u001b[38;5;241m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[38;5;241m.\u001b[39missparse(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[1;32m   1153\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1154\u001b[0m     X,\n\u001b[1;32m   1155\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[0;32m-> 1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:913\u001b[0m, in \u001b[0;36m_BaseRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# for dense matrices or when intercept is set to 0\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 913\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_ridge_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_intercept(X_offset, y_offset, X_scale)\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:723\u001b[0m, in \u001b[0;36m_ridge_regression\u001b[0;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, positive, random_state, return_n_iter, return_intercept, X_scale, X_offset, check_input, fit_intercept)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m         coef \u001b[38;5;241m=\u001b[39m \u001b[43m_solve_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m linalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;66;03m# use SVD solver if matrix is singular\u001b[39;00m\n\u001b[1;32m    726\u001b[0m         solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:198\u001b[0m, in \u001b[0;36m_solve_cholesky\u001b[0;34m(X, y, alpha)\u001b[0m\n\u001b[1;32m    195\u001b[0m n_targets \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    197\u001b[0m A \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X\u001b[38;5;241m.\u001b[39mT, X, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 198\u001b[0m Xy \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m one_alpha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_equal(alpha, \u001b[38;5;28mlen\u001b[39m(alpha) \u001b[38;5;241m*\u001b[39m [alpha[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m one_alpha:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/extmath.py:192\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 192\u001b[0m     \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m ):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/sparse/_base.py:1461\u001b[0m, in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m-> 1461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array type?\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \n\u001b[1;32m   1464\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: take a look at paralelization:\n",
    "\n",
    "shap_sensitivities = []\n",
    "shap_faithfullnesses = []\n",
    "shap_complexities = []\n",
    "\n",
    "lime_sensitivities = []\n",
    "lime_faithfullnesses = []\n",
    "lime_complexities = []\n",
    "\n",
    "anchor_sensitivities = []\n",
    "anchor_faithfullnesses = []\n",
    "anchor_complexities = []\n",
    "\n",
    "shap_exp = ShapTabularTreeWrapper(clf, X_train, categorical_features)\n",
    "lime_exp = LimeWrapper(clf, X_train, categorical_features)\n",
    "anchor_exp = AnchorWrapper(clf, X_train, categorical_features)\n",
    "\n",
    "INSTANCES_TO_CHECK = 20\n",
    "\n",
    "for i in range(INSTANCES_TO_CHECK):\n",
    "    sample_idx = i\n",
    "    shap_sensitivities.append(xai_eval.sensitivity(ShapTabularTreeWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "    lime_sensitivities.append(xai_eval.sensitivity(LimeWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "    anchor_sensitivities.append(xai_eval.sensitivity(AnchorWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "\n",
    "    shap_faithfullnesses.append(xai_eval.faithfullness_correlation(shap_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "    lime_faithfullnesses.append(xai_eval.faithfullness_correlation(lime_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "    anchor_faithfullnesses.append(xai_eval.faithfullness_correlation(anchor_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "\n",
    "    shap_complexities.append(xai_eval.complexity(shap_exp, X_test.iloc[sample_idx]))\n",
    "    lime_complexities.append(xai_eval.complexity(lime_exp, X_test.iloc[sample_idx]))\n",
    "    anchor_complexities.append(xai_eval.complexity(anchor_exp, X_test.iloc[sample_idx]))\n",
    "\n",
    "shap_metrics = pd.DataFrame({\n",
    "    'faithfullness': shap_faithfullnesses,\n",
    "    'complexity': shap_complexities\n",
    "})\n",
    "\n",
    "lime_metrics = pd.DataFrame({\n",
    "    'faithfullness': lime_faithfullnesses,\n",
    "    'complexity': lime_complexities\n",
    "})\n",
    "\n",
    "anchor_metrics = pd.DataFrame({\n",
    "    'faithfullness': anchor_faithfullnesses,\n",
    "    'complexity': anchor_complexities\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shap trends:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.100756</td>\n",
       "      <td>2.454617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.579722</td>\n",
       "      <td>0.138450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.958061</td>\n",
       "      <td>2.071901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.624039</td>\n",
       "      <td>2.379469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.089470</td>\n",
       "      <td>2.464648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.356174</td>\n",
       "      <td>2.528257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.921765</td>\n",
       "      <td>2.731101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      20.000000   20.000000\n",
       "mean       -0.100756    2.454617\n",
       "std         0.579722    0.138450\n",
       "min        -0.958061    2.071901\n",
       "25%        -0.624039    2.379469\n",
       "50%        -0.089470    2.464648\n",
       "75%         0.356174    2.528257\n",
       "max         0.921765    2.731101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lime trends:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.099843</td>\n",
       "      <td>2.580166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.313569</td>\n",
       "      <td>0.046391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.438614</td>\n",
       "      <td>2.489802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.159231</td>\n",
       "      <td>2.556702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.158247</td>\n",
       "      <td>2.588070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.306877</td>\n",
       "      <td>2.607834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.600445</td>\n",
       "      <td>2.664484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      20.000000   20.000000\n",
       "mean        0.099843    2.580166\n",
       "std         0.313569    0.046391\n",
       "min        -0.438614    2.489802\n",
       "25%        -0.159231    2.556702\n",
       "50%         0.158247    2.588070\n",
       "75%         0.306877    2.607834\n",
       "max         0.600445    2.664484"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor trends:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.299502</td>\n",
       "      <td>1.536169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.447762</td>\n",
       "      <td>0.614974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.691829</td>\n",
       "      <td>0.679615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000327</td>\n",
       "      <td>1.214978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.451800</td>\n",
       "      <td>1.418186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.706762</td>\n",
       "      <td>1.902132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.835922</td>\n",
       "      <td>2.688599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      20.000000   20.000000\n",
       "mean        0.299502    1.536169\n",
       "std         0.447762    0.614974\n",
       "min        -0.691829    0.679615\n",
       "25%        -0.000327    1.214978\n",
       "50%         0.451800    1.418186\n",
       "75%         0.706762    1.902132\n",
       "max         0.835922    2.688599"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Shap trends:\")\n",
    "display(shap_metrics.describe())\n",
    "\n",
    "print(\"Lime trends:\")\n",
    "display(lime_metrics.describe())\n",
    "\n",
    "print(\"Anchor trends:\")\n",
    "display(anchor_metrics.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2939 - val_loss: 1.2636\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2286 - val_loss: 1.2457\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2582 - val_loss: 1.2292\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2290 - val_loss: 1.2137\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1965 - val_loss: 1.1983\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2144 - val_loss: 1.1831\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1856 - val_loss: 1.1678\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1716 - val_loss: 1.1529\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1537 - val_loss: 1.1373\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1656 - val_loss: 1.1213\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step\n"
     ]
    }
   ],
   "source": [
    "xai_eval = XaiEvaluator(clf, X_train, categorical_features, noise_gen_args={'encoding_dim': 5, 'epochs': 10})\n",
    "xai_eval.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating weights with MCDM algorithm: TOPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 10\n",
    "\n",
    "shap_metrics = []\n",
    "lime_metrics = []\n",
    "anchor_metrics = []\n",
    "\n",
    "shap_exp = ShapTabularTreeWrapper(clf, X_train, categorical_features)\n",
    "lime_exp = LimeWrapper(clf, X_train, categorical_features)\n",
    "anchor_exp = AnchorWrapper(clf, X_train, categorical_features)\n",
    "\n",
    "shap_metrics.append(xai_eval.sensitivity(ShapTabularTreeWrapper, X_test.iloc[sample_idx], iterations=1, method=\"pearson\"))\n",
    "shap_metrics.append(xai_eval.faithfullness_correlation(shap_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "shap_metrics.append(xai_eval.complexity(shap_exp, X_test.iloc[sample_idx]))\n",
    "\n",
    "lime_metrics.append(xai_eval.sensitivity(LimeWrapper, X_test.iloc[sample_idx], iterations=1, method=\"pearson\"))\n",
    "lime_metrics.append(xai_eval.faithfullness_correlation(lime_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "lime_metrics.append(xai_eval.complexity(lime_exp, X_test.iloc[sample_idx]))\n",
    "\n",
    "anchor_metrics.append(xai_eval.sensitivity(AnchorWrapper, X_test.iloc[sample_idx], iterations=1, method=\"pearson\"))\n",
    "anchor_metrics.append(xai_eval.faithfullness_correlation(anchor_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "anchor_metrics.append(xai_eval.complexity(lime_exp, X_test.iloc[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99894299, 0.66011371, 2.46439113],\n",
       "       [0.94922866, 0.52194693, 2.64367566],\n",
       "       [0.99715431, 0.58993356, 2.66161282]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Evaluation Matrix:\n",
      " [[0.99894299 0.66011371 2.46439113]\n",
      " [0.94922866 0.52194693 2.64367566]\n",
      " [0.99715431 0.58993356 2.66161282]]\n",
      "\n",
      "Step 2 - Normalized Evaluation Matrix:\n",
      " [[0.58728453 0.64231136 0.54904838]\n",
      " [0.55805717 0.50787075 0.58899167]\n",
      " [0.58623295 0.57402388 0.59298793]]\n",
      "\n",
      "Step 3 - Weighted Normalized Evaluation Matrix\n",
      " [[0.19576151 0.21410379 0.18301613]\n",
      " [0.18601906 0.16929025 0.19633056]\n",
      " [0.19541098 0.19134129 0.19766264]]\n",
      "\n",
      "Step 4 - worst_alternatives | best_alternatives \n",
      " [0.18601906 0.16929025 0.19766264]  |  [0.19576151 0.21410379 0.18301613]\n",
      "\n",
      "Step 5 - Distances to Worst Alternative | Distances to Best Alternative\n",
      " [0.04814238 0.00133209 0.02396783] [0.         0.04775398 0.02706981]\n",
      "\n",
      "Step 6 - Similarites to Worst Alternative | Similarities to Best Alternative\n",
      " [1.         0.02713782 0.46961084] [0.         0.97286218 0.53038916]\n",
      "\n",
      "Shap weight: 1.0\n",
      "Lime weight: 0.0271378212664916\n",
      "Anchor weight: 0.469610843122715\n"
     ]
    }
   ],
   "source": [
    "evaluation_matrix = np.array([\n",
    "    shap_metrics,\n",
    "    lime_metrics,\n",
    "    anchor_metrics\n",
    "])\n",
    "\n",
    "display(evaluation_matrix)\n",
    "\n",
    "\n",
    "# Maybe look into entropy weighting\n",
    "robustness_metrics_weights = [\n",
    "    1/3, # sensitivity\n",
    "    1/3, # faithfullness\n",
    "    1/3, # complexity\n",
    "]\n",
    "# if higher value is preferred - True\n",
    "# if lower value is preferred - False\n",
    "criterias = np.array([\n",
    "    True,  # For sensitivity(method=\"pearson\"), higher is better\n",
    "    True,   # For faithfulness, higher is better\n",
    "    False,   # For complexity, lower is better\n",
    "])\n",
    "\n",
    "t = Topsis(evaluation_matrix, robustness_metrics_weights, criterias, debug=True)\n",
    "display(t.criteria)\n",
    "t.calc()\n",
    "\n",
    "shap_weight, lime_weight, anchor_weight = t.worst_similarity # I know this is confusing; it was suposed to be by best_similarity, but there seems to be a mistake in the implementation, or the naming is wrong\n",
    "\n",
    "print(f\"Shap weight: {shap_weight}\")\n",
    "print(f\"Lime weight: {lime_weight}\")\n",
    "print(f\"Anchor weight: {anchor_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregating rankings with ranx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gettings ranks:\n",
    "shap_explanation = shap_exp.explain_instance(X_test.iloc[sample_idx])\n",
    "lime_explanation = lime_exp.explain_instance(X_test.iloc[sample_idx])\n",
    "anchor_explanation = anchor_exp.explain_instance(X_test.iloc[sample_idx])\n",
    "\n",
    "shap_explanation[\"query\"] = \"1\"\n",
    "lime_explanation[\"query\"] = \"1\"\n",
    "anchor_explanation[\"query\"] = \"1\"\n",
    "\n",
    "shap_run = Run.from_df(shap_explanation, q_id_col=\"query\", doc_id_col=\"feature\", score_col=\"score\")\n",
    "lime_run = Run.from_df(lime_explanation, q_id_col=\"query\", doc_id_col=\"feature\", score_col=\"score\")\n",
    "anchor_run = Run.from_df(anchor_explanation, q_id_col=\"query\", doc_id_col=\"feature\", score_col=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_run = fuse([shap_run, lime_run, anchor_run], method=\"wsum\", params={\"weights\": [shap_weight, lime_weight, anchor_weight]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Checking_account_none</td>\n",
       "      <td>1.392905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Credit_amount</td>\n",
       "      <td>0.596176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Purpose_radio_TV</td>\n",
       "      <td>0.559627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Checking_account_little</td>\n",
       "      <td>0.484291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Checking_account_moderate</td>\n",
       "      <td>0.353738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Age</td>\n",
       "      <td>0.332842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Sex_male</td>\n",
       "      <td>0.283322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Housing_own</td>\n",
       "      <td>0.185941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Duration</td>\n",
       "      <td>0.169730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Saving_accounts_little</td>\n",
       "      <td>0.163430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Housing_free</td>\n",
       "      <td>0.122896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Saving_accounts_none</td>\n",
       "      <td>0.097481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Saving_accounts_moderate</td>\n",
       "      <td>0.096453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>Job_skilled</td>\n",
       "      <td>0.091685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Sex_female</td>\n",
       "      <td>0.077958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Job_unskilled_resident</td>\n",
       "      <td>0.072785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>Purpose_repairs</td>\n",
       "      <td>0.069193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Job_highlyskilled</td>\n",
       "      <td>0.067553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>Purpose_education</td>\n",
       "      <td>0.060729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>Purpose_car</td>\n",
       "      <td>0.052139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>Purpose_business</td>\n",
       "      <td>0.048983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>Saving_accounts_rich</td>\n",
       "      <td>0.044542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>Checking_account_rich</td>\n",
       "      <td>0.042147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Purpose_vacation_others</td>\n",
       "      <td>0.039179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>Housing_rent</td>\n",
       "      <td>0.035680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Job_unskilled_nonresident</td>\n",
       "      <td>0.033672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>Purpose_domestic_appliances</td>\n",
       "      <td>0.032473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>Saving_accounts_quite_rich</td>\n",
       "      <td>0.024090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Purpose_furniture_equipment</td>\n",
       "      <td>0.014276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q_id                       doc_id     score\n",
       "0     1        Checking_account_none  1.392905\n",
       "1     1                Credit_amount  0.596176\n",
       "2     1             Purpose_radio_TV  0.559627\n",
       "3     1      Checking_account_little  0.484291\n",
       "4     1    Checking_account_moderate  0.353738\n",
       "5     1                          Age  0.332842\n",
       "6     1                     Sex_male  0.283322\n",
       "7     1                  Housing_own  0.185941\n",
       "8     1                     Duration  0.169730\n",
       "9     1       Saving_accounts_little  0.163430\n",
       "10    1                 Housing_free  0.122896\n",
       "11    1         Saving_accounts_none  0.097481\n",
       "12    1     Saving_accounts_moderate  0.096453\n",
       "13    1                  Job_skilled  0.091685\n",
       "14    1                   Sex_female  0.077958\n",
       "15    1       Job_unskilled_resident  0.072785\n",
       "16    1              Purpose_repairs  0.069193\n",
       "17    1            Job_highlyskilled  0.067553\n",
       "18    1            Purpose_education  0.060729\n",
       "19    1                  Purpose_car  0.052139\n",
       "20    1             Purpose_business  0.048983\n",
       "21    1         Saving_accounts_rich  0.044542\n",
       "22    1        Checking_account_rich  0.042147\n",
       "23    1      Purpose_vacation_others  0.039179\n",
       "24    1                 Housing_rent  0.035680\n",
       "25    1    Job_unskilled_nonresident  0.033672\n",
       "26    1  Purpose_domestic_appliances  0.032473\n",
       "27    1   Saving_accounts_quite_rich  0.024090\n",
       "28    1  Purpose_furniture_equipment  0.014276"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_run.to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
