{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:05:06.117997: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 15:05:06.246565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732212306.296998   27216 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732212306.312606   27216 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-21 15:05:06.447485: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Explainable AI tools:\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from alibi.explainers import AnchorTabular # why not used the original anchor package?\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam  # Import the Adam optimizer\n",
    "\n",
    "from tools.topsis import Topsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Job</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Saving accounts</th>\n",
       "      <th>Checking account</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Credit Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>NaN</td>\n",
       "      <td>little</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>moderate</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>education</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>car</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
       "0           0   67    male    2     own             NaN           little   \n",
       "1           1   22  female    2     own          little         moderate   \n",
       "2           2   49    male    1     own          little              NaN   \n",
       "3           3   45    male    2    free          little           little   \n",
       "4           4   53    male    2    free          little           little   \n",
       "\n",
       "   Credit amount  Duration              Purpose  Credit Risk  \n",
       "0           1169         6             radio/TV            1  \n",
       "1           5951        48             radio/TV            2  \n",
       "2           2096        12            education            1  \n",
       "3           7882        42  furniture/equipment            1  \n",
       "4           4870        24                  car            2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Job</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Credit Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>476.500000</td>\n",
       "      <td>35.501048</td>\n",
       "      <td>1.909853</td>\n",
       "      <td>3279.112159</td>\n",
       "      <td>20.780922</td>\n",
       "      <td>1.302935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>275.540378</td>\n",
       "      <td>11.379668</td>\n",
       "      <td>0.649681</td>\n",
       "      <td>2853.315158</td>\n",
       "      <td>12.046483</td>\n",
       "      <td>0.459768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>238.250000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1360.250000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>476.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2302.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>714.750000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3975.250000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>953.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18424.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         Age         Job  Credit amount    Duration  \\\n",
       "count  954.000000  954.000000  954.000000     954.000000  954.000000   \n",
       "mean   476.500000   35.501048    1.909853    3279.112159   20.780922   \n",
       "std    275.540378   11.379668    0.649681    2853.315158   12.046483   \n",
       "min      0.000000   19.000000    0.000000     250.000000    4.000000   \n",
       "25%    238.250000   27.000000    2.000000    1360.250000   12.000000   \n",
       "50%    476.500000   33.000000    2.000000    2302.500000   18.000000   \n",
       "75%    714.750000   42.000000    2.000000    3975.250000   24.000000   \n",
       "max    953.000000   75.000000    3.000000   18424.000000   72.000000   \n",
       "\n",
       "       Credit Risk  \n",
       "count   954.000000  \n",
       "mean      1.302935  \n",
       "std       0.459768  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       2.000000  \n",
       "max       2.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        954 non-null    int64 \n",
      " 1   Age               954 non-null    int64 \n",
      " 2   Sex               954 non-null    object\n",
      " 3   Job               954 non-null    int64 \n",
      " 4   Housing           954 non-null    object\n",
      " 5   Saving accounts   779 non-null    object\n",
      " 6   Checking account  576 non-null    object\n",
      " 7   Credit amount     954 non-null    int64 \n",
      " 8   Duration          954 non-null    int64 \n",
      " 9   Purpose           954 non-null    object\n",
      " 10  Credit Risk       954 non-null    int64 \n",
      "dtypes: int64(6), object(5)\n",
      "memory usage: 82.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of the categorical features:\n",
      "\t- Sex: ['male' 'female']\n",
      "\t- Housing: ['own' 'free' 'rent']\n",
      "\t- Saving accounts: [nan 'little' 'quite rich' 'rich' 'moderate']\n",
      "\t- Checking account: ['little' 'moderate' nan 'rich']\n",
      "\t- Purpose: ['radio/TV' 'education' 'furniture/equipment' 'car' 'business'\n",
      " 'domestic appliances' 'repairs' 'vacation/others']\n"
     ]
    }
   ],
   "source": [
    "original_data = pd.read_csv('german_credit_data_updated.csv')\n",
    "\n",
    "# Dataset overview - German Credit Risk (from Kaggle):\n",
    "# 1. Age (numeric)\n",
    "# 2. Sex (text: male, female)\n",
    "# 3. Job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\n",
    "# 4. Housing (text: own, rent, or free)\n",
    "# 5. Saving accounts (text - little, moderate, quite rich, rich)\n",
    "# 6. Checking account (numeric, in DM - Deutsch Mark)\n",
    "# 7. Credit amount (numeric, in DM)\n",
    "# 8. Duration (numeric, in month)\n",
    "# 9. Purpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)\n",
    "\n",
    "display(original_data.head())\n",
    "display(original_data.describe())\n",
    "display(original_data.info())\n",
    "\n",
    "# Display the unique values of thprecision=3, e categorical features:\n",
    "print('Unique values of the categorical features:')\n",
    "for col in original_data.select_dtypes(include='object'):\n",
    "    print(f'\\t- {col}: {original_data[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: Index(['Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account',\n",
      "       'Purpose'],\n",
      "      dtype='object')\n",
      "Numerical features: Index(['Age', 'Credit amount', 'Duration'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Credit_amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Credit_Risk</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Job_highlyskilled</th>\n",
       "      <th>Job_skilled</th>\n",
       "      <th>Job_unskilled_nonresident</th>\n",
       "      <th>Job_unskilled_resident</th>\n",
       "      <th>Housing_free</th>\n",
       "      <th>Housing_own</th>\n",
       "      <th>Housing_rent</th>\n",
       "      <th>Saving_accounts_little</th>\n",
       "      <th>Saving_accounts_moderate</th>\n",
       "      <th>Saving_accounts_none</th>\n",
       "      <th>Saving_accounts_quite_rich</th>\n",
       "      <th>Saving_accounts_rich</th>\n",
       "      <th>Checking_account_little</th>\n",
       "      <th>Checking_account_moderate</th>\n",
       "      <th>Checking_account_none</th>\n",
       "      <th>Checking_account_rich</th>\n",
       "      <th>Purpose_business</th>\n",
       "      <th>Purpose_car</th>\n",
       "      <th>Purpose_domestic_appliances</th>\n",
       "      <th>Purpose_education</th>\n",
       "      <th>Purpose_furniture_equipment</th>\n",
       "      <th>Purpose_radio_TV</th>\n",
       "      <th>Purpose_repairs</th>\n",
       "      <th>Purpose_vacation_others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Credit_amount  Duration  Credit_Risk  Sex_female  Sex_male  \\\n",
       "0   67           1169         6            0           0         1   \n",
       "1   22           5951        48            1           1         0   \n",
       "2   49           2096        12            0           0         1   \n",
       "3   45           7882        42            0           0         1   \n",
       "4   53           4870        24            1           0         1   \n",
       "\n",
       "   Job_highlyskilled  Job_skilled  Job_unskilled_nonresident  \\\n",
       "0                  0            1                          0   \n",
       "1                  0            1                          0   \n",
       "2                  0            0                          0   \n",
       "3                  0            1                          0   \n",
       "4                  0            1                          0   \n",
       "\n",
       "   Job_unskilled_resident  Housing_free  Housing_own  Housing_rent  \\\n",
       "0                       0             0            1             0   \n",
       "1                       0             0            1             0   \n",
       "2                       1             0            1             0   \n",
       "3                       0             1            0             0   \n",
       "4                       0             1            0             0   \n",
       "\n",
       "   Saving_accounts_little  Saving_accounts_moderate  Saving_accounts_none  \\\n",
       "0                       0                         0                     1   \n",
       "1                       1                         0                     0   \n",
       "2                       1                         0                     0   \n",
       "3                       1                         0                     0   \n",
       "4                       1                         0                     0   \n",
       "\n",
       "   Saving_accounts_quite_rich  Saving_accounts_rich  Checking_account_little  \\\n",
       "0                           0                     0                        1   \n",
       "1                           0                     0                        0   \n",
       "2                           0                     0                        0   \n",
       "3                           0                     0                        1   \n",
       "4                           0                     0                        1   \n",
       "\n",
       "   Checking_account_moderate  Checking_account_none  Checking_account_rich  \\\n",
       "0                          0                      0                      0   \n",
       "1                          1                      0                      0   \n",
       "2                          0                      1                      0   \n",
       "3                          0                      0                      0   \n",
       "4                          0                      0                      0   \n",
       "\n",
       "   Purpose_business  Purpose_car  Purpose_domestic_appliances  \\\n",
       "0                 0            0                            0   \n",
       "1                 0            0                            0   \n",
       "2                 0            0                            0   \n",
       "3                 0            0                            0   \n",
       "4                 0            1                            0   \n",
       "\n",
       "   Purpose_education  Purpose_furniture_equipment  Purpose_radio_TV  \\\n",
       "0                  0                            0                 1   \n",
       "1                  0                            0                 1   \n",
       "2                  1                            0                 0   \n",
       "3                  0                            1                 0   \n",
       "4                  0                            0                 0   \n",
       "\n",
       "   Purpose_repairs  Purpose_vacation_others  \n",
       "0                0                        0  \n",
       "1                0                        0  \n",
       "2                0                        0  \n",
       "3                0                        0  \n",
       "4                0                        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 30 columns):\n",
      " #   Column                       Non-Null Count  Dtype\n",
      "---  ------                       --------------  -----\n",
      " 0   Age                          954 non-null    int64\n",
      " 1   Credit_amount                954 non-null    int64\n",
      " 2   Duration                     954 non-null    int64\n",
      " 3   Credit_Risk                  954 non-null    int64\n",
      " 4   Sex_female                   954 non-null    int64\n",
      " 5   Sex_male                     954 non-null    int64\n",
      " 6   Job_highlyskilled            954 non-null    int64\n",
      " 7   Job_skilled                  954 non-null    int64\n",
      " 8   Job_unskilled_nonresident    954 non-null    int64\n",
      " 9   Job_unskilled_resident       954 non-null    int64\n",
      " 10  Housing_free                 954 non-null    int64\n",
      " 11  Housing_own                  954 non-null    int64\n",
      " 12  Housing_rent                 954 non-null    int64\n",
      " 13  Saving_accounts_little       954 non-null    int64\n",
      " 14  Saving_accounts_moderate     954 non-null    int64\n",
      " 15  Saving_accounts_none         954 non-null    int64\n",
      " 16  Saving_accounts_quite_rich   954 non-null    int64\n",
      " 17  Saving_accounts_rich         954 non-null    int64\n",
      " 18  Checking_account_little      954 non-null    int64\n",
      " 19  Checking_account_moderate    954 non-null    int64\n",
      " 20  Checking_account_none        954 non-null    int64\n",
      " 21  Checking_account_rich        954 non-null    int64\n",
      " 22  Purpose_business             954 non-null    int64\n",
      " 23  Purpose_car                  954 non-null    int64\n",
      " 24  Purpose_domestic_appliances  954 non-null    int64\n",
      " 25  Purpose_education            954 non-null    int64\n",
      " 26  Purpose_furniture_equipment  954 non-null    int64\n",
      " 27  Purpose_radio_TV             954 non-null    int64\n",
      " 28  Purpose_repairs              954 non-null    int64\n",
      " 29  Purpose_vacation_others      954 non-null    int64\n",
      "dtypes: int64(30)\n",
      "memory usage: 223.7 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.7694545 , -0.7399179 , -1.22763429, ...,  1.62518349,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-1.18704073,  0.93690642,  2.26068929, ...,  1.62518349,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [ 1.18685641, -0.41486224, -0.72930235, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       ...,\n",
       "       [-1.0111965 , -0.39768023,  1.26402541, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-0.65950803,  0.29240557,  0.26736153, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-0.83535227,  2.69823821,  1.26402541, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_data = original_data.copy()\n",
    "\n",
    "# For savings and checking accounts, we will replace the missing values with 'none':\n",
    "preprocessed_data['Saving accounts'].fillna('none', inplace=True)\n",
    "preprocessed_data['Checking account'].fillna('none', inplace=True)\n",
    "\n",
    "# Dropping index column:\n",
    "preprocessed_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Using pd.dummies to one-hot-encode the categorical features\n",
    "preprocessed_data[\"Job\"] = preprocessed_data[\"Job\"].map({0: 'unskilled_nonresident', 1: 'unskilled_resident',\n",
    "                                                         2: 'skilled', 3: 'highlyskilled'})\n",
    "\n",
    "categorical_features = preprocessed_data.select_dtypes(include='object').columns\n",
    "numerical_features = preprocessed_data.select_dtypes(include='number').columns.drop('Credit Risk')\n",
    "print(f'Categorical features: {categorical_features}')\n",
    "print(f'Numerical features: {numerical_features}')\n",
    "\n",
    "preprocessed_data = pd.get_dummies(preprocessed_data, columns=categorical_features, dtype='int64')\n",
    "\n",
    "# Remapping the target variable to 0 and 1:\n",
    "preprocessed_data['Credit Risk'] = preprocessed_data['Credit Risk'].map({1: 0, 2: 1})\n",
    "\n",
    "# Make sure all column names are valid python identifiers (important for pd.query() calls):\n",
    "preprocessed_data.columns = preprocessed_data.columns.str.replace(' ', '_')\n",
    "preprocessed_data.columns = preprocessed_data.columns.str.replace('/', '_')\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "scaled_preprocessed_data = scaler.fit_transform(preprocessed_data)\n",
    "\n",
    "display(preprocessed_data.head())\n",
    "display(preprocessed_data.info())\n",
    "\n",
    "display(scaled_preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = preprocessed_data['Credit_Risk']\n",
    "X = preprocessed_data.drop(columns='Credit_Risk')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7696335078534031\n",
      "ROC AUC: 0.6830357142857143\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining explanation model wrapper (for standard behaviour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainerWrapper:\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        self.clf = clf\n",
    "\n",
    "        if hasattr(clf, 'predict_proba') and predict_proba is None:\n",
    "            self.predict_proba = clf.predict_proba\n",
    "        elif predict_proba is not None:\n",
    "            self.predict_proba = predict_proba\n",
    "        else:\n",
    "            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.categorical_feature_names = categorical_feature_names\n",
    "\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "class LimeWrapper(ExplainerWrapper):\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "        \n",
    "        self.explainer = LimeTabularExplainer(self.X_train.values, feature_names=self.X_train.columns, discretize_continuous=False)\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        lime_exp = self.explainer.explain_instance(instance_data_row, self.predict_proba, num_features=len(self.X_train.columns))\n",
    "        \n",
    "        ranking = pd.DataFrame(lime_exp.as_list(), columns=['feature', 'score'])\n",
    "        return ranking\n",
    "\n",
    "class ShapTabularTreeWrapper(ExplainerWrapper):\n",
    "    \n",
    "        def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None, **additional_explainer_args):\n",
    "            super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "            \n",
    "            self.explainer = shap.TreeExplainer(clf, self.X_train, **additional_explainer_args)\n",
    "        \n",
    "        def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "            shap_values = self.explainer.shap_values(instance_data_row)\n",
    "    \n",
    "            ranking = pd.DataFrame(list(zip(self.X_train.columns, shap_values[:, 0])), columns=['feature', 'score'])\n",
    "            ranking = ranking.sort_values(by='score', ascending=False, key=lambda x: abs(x)).reset_index(drop=True)\n",
    "            \n",
    "            return ranking\n",
    "\n",
    "class AnchorWrapper(ExplainerWrapper):\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "        \n",
    "        self.explainer = AnchorTabular(predictor=self.predict_proba, feature_names=self.X_train.columns) # TODO: fix parameters\n",
    "        self.explainer.fit(self.X_train.values)\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        if isinstance(instance_data_row, pd.Series):\n",
    "            instance_data_row = instance_data_row.to_numpy()\n",
    "\n",
    "        feature_importances = {feature: 0 for feature in self.X_train.columns}\n",
    "        explanation = self.explainer.explain(instance_data_row)\n",
    "        \n",
    "        for rule in explanation.anchor:\n",
    "            # Extract the feature name from the rule string\n",
    "            # This method won't work for column names that have spaces in them or that don't contain any letters\n",
    "            for expression_element in rule.split():\n",
    "                if any(c.isalpha() for c in expression_element):\n",
    "                    referenced_feature = expression_element\n",
    "                    break\n",
    "\n",
    "            rule_coverage = self.X_train.query(rule).shape[0] / self.X_train.shape[0]\n",
    "            feature_importances[referenced_feature] = 1 - rule_coverage\n",
    "        \n",
    "        return pd.DataFrame(list(feature_importances.items()), columns=['feature', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining autoencoder noisy data generator\n",
    "(to be used on calculating sensitivity metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderNoisyDataGenerator():\n",
    "    def __init__(self, X: pd.DataFrame, ohe_categorical_features_names: list[str], encoding_dim: int = 5, epochs=500):\n",
    "        self.X = X\n",
    "        self.categorical_features_names = ohe_categorical_features_names\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.epochs = epochs\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        self.X_scaled = scaler.fit_transform(self.X)\n",
    "        \n",
    "        input_dim = self.X_scaled.shape[1]\n",
    "\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(self.encoding_dim, activation='relu')(input_layer)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        self.encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "        self.was_fit = False\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        self.autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        self.autoencoder.fit(self.X_scaled, self.X_scaled, epochs=self.epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "        # Extract hidden layer representation:\n",
    "        self.hidden_representation = self.encoder.predict(self.X_scaled)\n",
    "        self.was_fit = True\n",
    "\n",
    "\n",
    "    def generate_noisy_data(self, num_features_to_replace: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame containing a noisy variation of the data.\n",
    "\n",
    "        The noise is generated by swapping the values of a small number of features between a sample and a random close neighbor.\n",
    "        To determine the neighbors, we use an autoencoder to reduce the dimensionality of the data and then calculate the use the NearestNeightbors algorithm in the reduced space.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.was_fit:\n",
    "            raise ValueError('The autoencoder has not been fitted yet. Call the fit() method before generating noisy data.')\n",
    "\n",
    "        # Compute Nearest Neighbors using hidden_representation\n",
    "        nbrs = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(self.hidden_representation)\n",
    "        distances, indices = nbrs.kneighbors(self.hidden_representation)\n",
    "\n",
    "        X_noisy = self.X.copy()\n",
    "\n",
    "        # Get id's of columns that belong to the same categorical feature (after being one-hot-encodeded);\n",
    "        # Columns that belong to the same categorical feature start with the same name, and will be treated as a single feature when adding noise.\n",
    "        categorical_features_indices = [\n",
    "            [self.X.columns.get_loc(col_name) for col_name in self.X.columns if col_name.startswith(feature)]\n",
    "            for feature in self.categorical_features_names\n",
    "        ]\n",
    "\n",
    "        # Replace features with random neighbor's features\n",
    "        for i in range(self.X.shape[0]):  # Iterate over each sample\n",
    "            available_features_to_replace = list(range(self.X.shape[1]))\n",
    "            for j in range(num_features_to_replace):\n",
    "                # Select features to replace; if the feture selected belong to one of the lists in categorical_features_indices, we will replace all the features in that list\n",
    "                features_to_replace = np.random.choice(available_features_to_replace, 1)\n",
    "                for feature_indices in categorical_features_indices:\n",
    "                    if features_to_replace in feature_indices:\n",
    "                        features_to_replace = feature_indices\n",
    "                        break\n",
    "                \n",
    "                # Remove the selected features from the list of available features to replace\n",
    "                available_features_to_replace = [f for f in available_features_to_replace if f not in features_to_replace]\n",
    "\n",
    "                # Choose a random neighbor from the nearest neighbors\n",
    "                neighbor_idx = np.random.choice(indices[i][1:])\n",
    "\n",
    "                # Replace the selected features with the neighbor's features\n",
    "                X_noisy.iloc[i, features_to_replace] = self.X.iloc[neighbor_idx, features_to_replace]\n",
    "\n",
    "        return X_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732213418.793056   27216 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2439 - val_loss: 1.2222\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2149 - val_loss: 1.2038\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1942 - val_loss: 1.1866\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2050 - val_loss: 1.1695\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1801 - val_loss: 1.1520\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1651 - val_loss: 1.1344\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1390 - val_loss: 1.1174\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1145 - val_loss: 1.1007\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0939 - val_loss: 1.0848\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0887 - val_loss: 1.0699\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mean Absolute Difference: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.548108645546165"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage Example:\n",
    "autoencoder_noisy_data_generator = AutoencoderNoisyDataGenerator(X_train, categorical_features, encoding_dim=5, epochs=10)\n",
    "autoencoder_noisy_data_generator.fit()\n",
    "noisy_data = autoencoder_noisy_data_generator.generate_noisy_data(num_features_to_replace=2)\n",
    "display(\"Mean Absolute Difference: \", np.mean(np.abs(X_train - noisy_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining evaluator class to hold evaluation metric methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XaiEvaluator:\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, ohe_categorical_feature_names: list[str], predict_proba: callable = None, noise_gen_args: dict = {}):\n",
    "        self.clf = clf\n",
    "        if hasattr(clf, 'predict_proba') and predict_proba is None:\n",
    "            self.predict_proba = clf.predict_proba\n",
    "        elif predict_proba is not None:\n",
    "            self.predict_proba = predict_proba\n",
    "        else:\n",
    "            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.ohe_categorical_feature_names = ohe_categorical_feature_names\n",
    "\n",
    "        self.categorical_features_indices = [\n",
    "            [self.X_train.columns.get_loc(col_name) for col_name in self.X_train.columns if col_name.startswith(feature)]\n",
    "            for feature in self.ohe_categorical_feature_names\n",
    "        ]\n",
    "\n",
    "        self.noisy_data_generator = AutoencoderNoisyDataGenerator(X_train, ohe_categorical_feature_names, **noise_gen_args)\n",
    "\n",
    "        self.was_initialized = False\n",
    "    \n",
    "    # Initialization opeations that take a long time to run\n",
    "    def init(self):\n",
    "        self.noisy_data_generator.fit()\n",
    "        self.was_initialized = True\n",
    "            \n",
    "    def faithfullness_correlation(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, len_subset: int = None,\n",
    "                                  iterations: int = 100, baseline_strategy: Literal[\"zeros\", \"mean\"] = \"zeros\") -> float:\n",
    "        if not isinstance(explainer, ExplainerWrapper):\n",
    "            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "        \n",
    "        dimension = len(instance_data_row)\n",
    "\n",
    "        importance_sums = []\n",
    "        delta_fs = []\n",
    "\n",
    "        f_x = self.predict_proba(instance_data_row.to_numpy().reshape(1, -1))[0][1]\n",
    "        g_x = explainer.explain_instance(instance_data_row)\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            # Select a subset of features to perturb\n",
    "            subset = np.random.choice(instance_data_row.index.values, len_subset if len_subset else dimension/4, replace=False)\n",
    "\n",
    "            perturbed_instance = instance_data_row.copy()\n",
    "\n",
    "            if baseline_strategy == \"zeros\":\n",
    "                baseline = np.zeros(dimension)  # either mean on all zeros\n",
    "            elif baseline_strategy == \"mean\":\n",
    "                baseline = np.mean(self.X_train, axis=0)\n",
    "                for feature_index in self.categorical_features_indices:\n",
    "                    baseline[feature_index] = 0\n",
    "                \n",
    "            perturbed_instance[subset] = baseline[instance_data_row.index.get_indexer(subset)]\n",
    "\n",
    "            importance_sum = 0\n",
    "            for feature in subset:\n",
    "                importance_sum += g_x[g_x['feature'] == feature]['score'].values[0] # should I take the abs value here?\n",
    "            importance_sums.append(importance_sum)\n",
    "\n",
    "            f_x_perturbed = self.predict_proba(perturbed_instance.to_numpy().reshape(1, -1))[0][1]\n",
    "            delta_f = np.abs(f_x - f_x_perturbed)\n",
    "            delta_fs.append(delta_f)\n",
    "        \n",
    "        return abs(pearsonr(importance_sums, delta_fs).statistic)\n",
    "    \n",
    "    def sensitivity(self, ExplainerType: Type[ExplainerWrapper], instance_data_row: pd.Series, iterations: int = 10, method: Literal['mean_squared', 'spearman', 'pearson'] = 'pearson',\n",
    "                    custom_method: Callable[[pd.DataFrame, pd.DataFrame], float]=None) -> float:\n",
    "        if not self.was_initialized:\n",
    "            raise ValueError('The XaiEvaluator has not been initialized yet. Call the init() method before evaluating sensitivity.')\n",
    "        \n",
    "        original_explainer = ExplainerType(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "\n",
    "        results: list[float] = []\n",
    "        for _ in range(iterations):\n",
    "            # Obtain the original explanation:\n",
    "            original_explanation = original_explainer.explain_instance(instance_data_row)\n",
    "\n",
    "            # Obtain the noisy explanation:\n",
    "            noisy_data = self.noisy_data_generator.generate_noisy_data(num_features_to_replace=2)\n",
    "            noisy_explainer = ExplainerType(self.clf, noisy_data, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "            noisy_explanation = noisy_explainer.explain_instance(instance_data_row)\n",
    "\n",
    "            if custom_method is not None:\n",
    "                results.append(custom_method(original_explanation, noisy_explanation))\n",
    "            elif method == 'mean_squared':\n",
    "                mean_squared_difference = ((original_explanation['score'] - noisy_explanation['score']) ** 2).mean()\n",
    "                results.append(mean_squared_difference)\n",
    "            elif method == 'spearman':\n",
    "                spearman_correlation = spearmanr(original_explanation['score'], noisy_explanation['score']).correlation\n",
    "                results.append(abs(spearman_correlation))\n",
    "            elif method == 'pearson':\n",
    "                pearson_correlation = pearsonr(original_explanation['score'], noisy_explanation['score']).correlation\n",
    "                results.append(abs(pearson_correlation))\n",
    "        \n",
    "        return np.mean(results)\n",
    "\n",
    "    def complexity(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, **kwargs) -> float:\n",
    "        if not kwargs.get(\"bypass_check\", False) and not isinstance(explainer, ExplainerWrapper):\n",
    "            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "\n",
    "        explanation = explainer.explain_instance(instance_data_row)\n",
    "\n",
    "        def frac_contribution(explanation: pd.DataFrame, i: int) -> float:\n",
    "            abs_score_sum = explanation['score'].abs().sum()\n",
    "            return explanation['score'].abs()[i] / abs_score_sum\n",
    "\n",
    "        sum = 0\n",
    "        for i in range(explanation.shape[0]):\n",
    "            fc = frac_contribution(explanation, i)\n",
    "            sum += fc * np.log(fc) if fc > 0 else 0\n",
    "            \n",
    "        return -sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2670 - val_loss: 1.2361\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2204 - val_loss: 1.2187\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2376 - val_loss: 1.2020\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2238 - val_loss: 1.1853\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1991 - val_loss: 1.1688\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1949 - val_loss: 1.1527\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1810 - val_loss: 1.1364\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1576 - val_loss: 1.1203\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1280 - val_loss: 1.1044\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0939 - val_loss: 1.0893\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step\n"
     ]
    }
   ],
   "source": [
    "xai_eval = XaiEvaluator(clf, X_train, categorical_features, noise_gen_args={'encoding_dim': 5, 'epochs': 10})\n",
    "xai_eval.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness:  0.3679202721684219\n",
      "Sensitivity:  0.0033128607459109076\n",
      "Complexity:  2.516308873975537\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "faithfullness =  xai_eval.faithfullness_correlation(AnchorWrapper,\n",
    "                                                    X_test.iloc[sample_idx], len_subset=10, iterations=100, baseline_strategy=\"mean\")\n",
    "print(\"Faithfulness: \", faithfullness)\n",
    "sensitivity = xai_eval.sensitivity(AnchorWrapper, X_test.iloc[sample_idx], iterations=10)\n",
    "print(\"Sensitivity: \", sensitivity)\n",
    "complexity = xai_eval.complexity(ShapTabularTreeWrapper, X_test.iloc[sample_idx])\n",
    "print(\"Complexity: \", complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19144408195771734\n"
     ]
    }
   ],
   "source": [
    "# Testing edge complexity cases:\n",
    "class TestWrapper(ExplainerWrapper):\n",
    "    def __init__(self, test_explanation):\n",
    "        self.test_explanation = test_explanation\n",
    "\n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        return self.test_explanation\n",
    "\n",
    "test_explanation = pd.DataFrame({'feature': ['a', 'b', 'c'], 'score': [1, 0.05, 0]})\n",
    "test_explainer = TestWrapper(test_explanation)\n",
    "\n",
    "complexity = xai_eval.complexity(test_explainer, X_test.iloc[sample_idx], bypass_check=True)\n",
    "print(complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xai Evaluation Metrics Trends for Selected Explanation Models\n",
    "(Takes a long time to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     23\u001b[0m shap_sensitivities\u001b[38;5;241m.\u001b[39mappend(xai_eval\u001b[38;5;241m.\u001b[39msensitivity(ShapTabularTreeWrapper, X_test\u001b[38;5;241m.\u001b[39miloc[sample_idx], iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m lime_sensitivities\u001b[38;5;241m.\u001b[39mappend(\u001b[43mxai_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLimeWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m anchor_sensitivities\u001b[38;5;241m.\u001b[39mappend(xai_eval\u001b[38;5;241m.\u001b[39msensitivity(AnchorWrapper, X_test\u001b[38;5;241m.\u001b[39miloc[sample_idx], iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     27\u001b[0m shap_faithfullnesses\u001b[38;5;241m.\u001b[39mappend(xai_eval\u001b[38;5;241m.\u001b[39mfaithfullness_correlation(shap_exp, X_test\u001b[38;5;241m.\u001b[39miloc[sample_idx], len_subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, baseline_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[48], line 79\u001b[0m, in \u001b[0;36mXaiEvaluator.sensitivity\u001b[0;34m(self, ExplainerType, instance_data_row, iterations)\u001b[0m\n\u001b[1;32m     76\u001b[0m original_explanation \u001b[38;5;241m=\u001b[39m original_explainer\u001b[38;5;241m.\u001b[39mexplain_instance(instance_data_row)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Obtain the noisy explanation:\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m noisy_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoisy_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_noisy_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_features_to_replace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m noisy_explainer \u001b[38;5;241m=\u001b[39m ExplainerType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclf, noisy_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mohe_categorical_feature_names, predict_proba\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba)\n\u001b[1;32m     81\u001b[0m noisy_explanation \u001b[38;5;241m=\u001b[39m noisy_explainer\u001b[38;5;241m.\u001b[39mexplain_instance(instance_data_row)\n",
      "Cell \u001b[0;32mIn[10], line 66\u001b[0m, in \u001b[0;36mAutoencoderNoisyDataGenerator.generate_noisy_data\u001b[0;34m(self, num_features_to_replace)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Remove the selected features from the list of available features to replace\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m available_features_to_replace \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m available_features_to_replace \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m features_to_replace]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Choose a random neighbor from the nearest neighbors\u001b[39;00m\n\u001b[1;32m     69\u001b[0m neighbor_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(indices[i][\u001b[38;5;241m1\u001b[39m:])\n",
      "Cell \u001b[0;32mIn[10], line 66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Remove the selected features from the list of available features to replace\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m available_features_to_replace \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m available_features_to_replace \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m features_to_replace]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Choose a random neighbor from the nearest neighbors\u001b[39;00m\n\u001b[1;32m     69\u001b[0m neighbor_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(indices[i][\u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: take a look at paralelization:\n",
    "\n",
    "shap_sensitivities = []\n",
    "shap_faithfullnesses = []\n",
    "shap_complexities = []\n",
    "\n",
    "lime_sensitivities = []\n",
    "lime_faithfullnesses = []\n",
    "lime_complexities = []\n",
    "\n",
    "anchor_sensitivities = []\n",
    "anchor_faithfullnesses = []\n",
    "anchor_complexities = []\n",
    "\n",
    "shap_exp = ShapTabularTreeWrapper(clf, X_train, categorical_features)\n",
    "lime_exp = LimeWrapper(clf, X_train, categorical_features)\n",
    "anchor_exp = AnchorWrapper(clf, X_train, categorical_features)\n",
    "\n",
    "INSTANCES_TO_CHECK = 20\n",
    "\n",
    "for i in range(INSTANCES_TO_CHECK):\n",
    "    sample_idx = i\n",
    "    shap_sensitivities.append(xai_eval.sensitivity(ShapTabularTreeWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "    lime_sensitivities.append(xai_eval.sensitivity(LimeWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "    anchor_sensitivities.append(xai_eval.sensitivity(AnchorWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "\n",
    "    shap_faithfullnesses.append(xai_eval.faithfullness_correlation(shap_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "    lime_faithfullnesses.append(xai_eval.faithfullness_correlation(lime_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "    anchor_faithfullnesses.append(xai_eval.faithfullness_correlation(anchor_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "\n",
    "    shap_complexities.append(xai_eval.complexity(shap_exp, X_test.iloc[sample_idx]))\n",
    "    lime_complexities.append(xai_eval.complexity(lime_exp, X_test.iloc[sample_idx]))\n",
    "    anchor_complexities.append(xai_eval.complexity(anchor_exp, X_test.iloc[sample_idx]))\n",
    "\n",
    "shap_metrics = pd.DataFrame({\n",
    "    'faithfullness': shap_faithfullnesses,\n",
    "    'complexity': shap_complexities\n",
    "})\n",
    "\n",
    "lime_metrics = pd.DataFrame({\n",
    "    'faithfullness': lime_faithfullnesses,\n",
    "    'complexity': lime_complexities\n",
    "})\n",
    "\n",
    "anchor_metrics = pd.DataFrame({\n",
    "    'faithfullness': anchor_faithfullnesses,\n",
    "    'complexity': anchor_complexities\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shap trends:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.100756</td>\n",
       "      <td>2.454617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.579722</td>\n",
       "      <td>0.138450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.958061</td>\n",
       "      <td>2.071901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.624039</td>\n",
       "      <td>2.379469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.089470</td>\n",
       "      <td>2.464648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.356174</td>\n",
       "      <td>2.528257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.921765</td>\n",
       "      <td>2.731101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      20.000000   20.000000\n",
       "mean       -0.100756    2.454617\n",
       "std         0.579722    0.138450\n",
       "min        -0.958061    2.071901\n",
       "25%        -0.624039    2.379469\n",
       "50%        -0.089470    2.464648\n",
       "75%         0.356174    2.528257\n",
       "max         0.921765    2.731101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lime trends:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.099843</td>\n",
       "      <td>2.580166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.313569</td>\n",
       "      <td>0.046391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.438614</td>\n",
       "      <td>2.489802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.159231</td>\n",
       "      <td>2.556702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.158247</td>\n",
       "      <td>2.588070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.306877</td>\n",
       "      <td>2.607834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.600445</td>\n",
       "      <td>2.664484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      20.000000   20.000000\n",
       "mean        0.099843    2.580166\n",
       "std         0.313569    0.046391\n",
       "min        -0.438614    2.489802\n",
       "25%        -0.159231    2.556702\n",
       "50%         0.158247    2.588070\n",
       "75%         0.306877    2.607834\n",
       "max         0.600445    2.664484"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor trends:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.299502</td>\n",
       "      <td>1.536169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.447762</td>\n",
       "      <td>0.614974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.691829</td>\n",
       "      <td>0.679615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000327</td>\n",
       "      <td>1.214978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.451800</td>\n",
       "      <td>1.418186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.706762</td>\n",
       "      <td>1.902132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.835922</td>\n",
       "      <td>2.688599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      20.000000   20.000000\n",
       "mean        0.299502    1.536169\n",
       "std         0.447762    0.614974\n",
       "min        -0.691829    0.679615\n",
       "25%        -0.000327    1.214978\n",
       "50%         0.451800    1.418186\n",
       "75%         0.706762    1.902132\n",
       "max         0.835922    2.688599"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Shap trends:\")\n",
    "display(shap_metrics.describe())\n",
    "\n",
    "print(\"Lime trends:\")\n",
    "display(lime_metrics.describe())\n",
    "\n",
    "print(\"Anchor trends:\")\n",
    "display(anchor_metrics.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using along with TOPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2384 - val_loss: 1.2351\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2301 - val_loss: 1.2180\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2267 - val_loss: 1.2016\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1926 - val_loss: 1.1854\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1950 - val_loss: 1.1689\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1569 - val_loss: 1.1518\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1520 - val_loss: 1.1347\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1383 - val_loss: 1.1181\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1207 - val_loss: 1.1020\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1118 - val_loss: 1.0864\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step\n"
     ]
    }
   ],
   "source": [
    "xai_eval = XaiEvaluator(clf, X_train, categorical_features, noise_gen_args={'encoding_dim': 5, 'epochs': 10})\n",
    "xai_eval.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 100\n",
    "\n",
    "shap_metrics = []\n",
    "lime_metrics = []\n",
    "anchor_metrics = []\n",
    "\n",
    "shap_exp = ShapTabularTreeWrapper(clf, X_train, categorical_features)\n",
    "lime_exp = LimeWrapper(clf, X_train, categorical_features)\n",
    "anchor_exp = AnchorWrapper(clf, X_train, categorical_features)\n",
    "\n",
    "shap_metrics.append(xai_eval.sensitivity(ShapTabularTreeWrapper, X_test.iloc[sample_idx], iterations=1))\n",
    "shap_metrics.append(xai_eval.faithfullness_correlation(shap_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "shap_metrics.append(xai_eval.complexity(shap_exp, X_test.iloc[sample_idx]))\n",
    "\n",
    "lime_metrics.append(xai_eval.sensitivity(LimeWrapper, X_test.iloc[sample_idx], iterations=1))\n",
    "lime_metrics.append(xai_eval.faithfullness_correlation(lime_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "lime_metrics.append(xai_eval.complexity(lime_exp, X_test.iloc[sample_idx]))\n",
    "\n",
    "anchor_metrics.append(xai_eval.sensitivity(AnchorWrapper, X_test.iloc[sample_idx], iterations=1))\n",
    "anchor_metrics.append(xai_eval.faithfullness_correlation(anchor_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "anchor_metrics.append(xai_eval.complexity(lime_exp, X_test.iloc[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000131, 0.57237171, 2.45330307],\n",
       "       [0.00005537, 0.34218822, 2.58338039],\n",
       "       [0.00475955, 0.39082461, 2.54674096]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Evaluation Matrix:\n",
      " [[0.00000131 0.57237171 2.45330307]\n",
      " [0.00005537 0.34218822 2.58338039]\n",
      " [0.00475955 0.39082461 2.54674096]]\n",
      "\n",
      "Step 2 - Normalized Evaluation Matrix:\n",
      " [[0.00027566 0.74050641 0.56020189]\n",
      " [0.01163358 0.44270631 0.58990452]\n",
      " [0.99993229 0.50562968 0.58153805]]\n",
      "\n",
      "Step 3 - Weighted Normalized Evaluation Matrix\n",
      " [[0.00009189 0.24683547 0.18673396]\n",
      " [0.00387786 0.14756877 0.19663484]\n",
      " [0.33331076 0.16854323 0.19384602]]\n",
      "\n",
      "Step 4 - worst_alternatives | best_alternatives \n",
      " [0.33331076 0.14756877 0.19663484]  |  [0.00009189 0.24683547 0.18673396]\n",
      "\n",
      "Step 5 - Distances to Worst Alternative | Distances to Best Alternative\n",
      " [0.34783146 0.3294329  0.02115905] [0.         0.09983105 0.34236687]\n",
      "\n",
      "Step 6 - Similarites to Worst Alternative | Similarities to Best Alternative\n",
      " [1.         0.76743668 0.05820506] [0.         0.23256332 0.94179494]\n",
      "\n",
      "Shap weight: 0.0\n",
      "Lime weight: 0.23256332116877532\n",
      "Anchor weight: 0.9417949350654228\n"
     ]
    }
   ],
   "source": [
    "evaluation_matrix = np.array([\n",
    "    shap_metrics,\n",
    "    lime_metrics,\n",
    "    anchor_metrics\n",
    "])\n",
    "\n",
    "display(evaluation_matrix)\n",
    "\n",
    "robustness_metrics_weights = [\n",
    "    1/3, # sensitivity\n",
    "    1/3, # faithfullness\n",
    "    1/3, # complexity\n",
    "]\n",
    "# if higher value is preferred - True\n",
    "# if lower value is preferred - False\n",
    "criterias = np.array([\n",
    "    False,  # For sensitivity, lower is better\n",
    "    True,   # For faithfulness, higher is better\n",
    "    False,   # For complexity, lower is better\n",
    "])\n",
    "\n",
    "t = Topsis(evaluation_matrix, robustness_metrics_weights, criterias, debug=True)\n",
    "t.calc()\n",
    "\n",
    "shap_weight, lime_weight, anchor_weight = t.best_similarity\n",
    "\n",
    "print(f\"Shap weight: {shap_weight}\")\n",
    "print(f\"Lime weight: {lime_weight}\")\n",
    "print(f\"Anchor weight: {anchor_weight}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
