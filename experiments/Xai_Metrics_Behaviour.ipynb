{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from xai_agg.agg_exp import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Job</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Saving accounts</th>\n",
       "      <th>Checking account</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Credit Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>NaN</td>\n",
       "      <td>little</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>moderate</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>education</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>car</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
       "0           0   67    male    2     own             NaN           little   \n",
       "1           1   22  female    2     own          little         moderate   \n",
       "2           2   49    male    1     own          little              NaN   \n",
       "3           3   45    male    2    free          little           little   \n",
       "4           4   53    male    2    free          little           little   \n",
       "\n",
       "   Credit amount  Duration              Purpose  Credit Risk  \n",
       "0           1169         6             radio/TV            1  \n",
       "1           5951        48             radio/TV            2  \n",
       "2           2096        12            education            1  \n",
       "3           7882        42  furniture/equipment            1  \n",
       "4           4870        24                  car            2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Job</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Credit Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>476.500000</td>\n",
       "      <td>35.501048</td>\n",
       "      <td>1.909853</td>\n",
       "      <td>3279.112159</td>\n",
       "      <td>20.780922</td>\n",
       "      <td>1.302935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>275.540378</td>\n",
       "      <td>11.379668</td>\n",
       "      <td>0.649681</td>\n",
       "      <td>2853.315158</td>\n",
       "      <td>12.046483</td>\n",
       "      <td>0.459768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>238.250000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1360.250000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>476.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2302.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>714.750000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3975.250000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>953.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18424.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         Age         Job  Credit amount    Duration  \\\n",
       "count  954.000000  954.000000  954.000000     954.000000  954.000000   \n",
       "mean   476.500000   35.501048    1.909853    3279.112159   20.780922   \n",
       "std    275.540378   11.379668    0.649681    2853.315158   12.046483   \n",
       "min      0.000000   19.000000    0.000000     250.000000    4.000000   \n",
       "25%    238.250000   27.000000    2.000000    1360.250000   12.000000   \n",
       "50%    476.500000   33.000000    2.000000    2302.500000   18.000000   \n",
       "75%    714.750000   42.000000    2.000000    3975.250000   24.000000   \n",
       "max    953.000000   75.000000    3.000000   18424.000000   72.000000   \n",
       "\n",
       "       Credit Risk  \n",
       "count   954.000000  \n",
       "mean      1.302935  \n",
       "std       0.459768  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       2.000000  \n",
       "max       2.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        954 non-null    int64 \n",
      " 1   Age               954 non-null    int64 \n",
      " 2   Sex               954 non-null    object\n",
      " 3   Job               954 non-null    int64 \n",
      " 4   Housing           954 non-null    object\n",
      " 5   Saving accounts   779 non-null    object\n",
      " 6   Checking account  576 non-null    object\n",
      " 7   Credit amount     954 non-null    int64 \n",
      " 8   Duration          954 non-null    int64 \n",
      " 9   Purpose           954 non-null    object\n",
      " 10  Credit Risk       954 non-null    int64 \n",
      "dtypes: int64(6), object(5)\n",
      "memory usage: 82.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of the categorical features:\n",
      "\t- Sex: ['male' 'female']\n",
      "\t- Housing: ['own' 'free' 'rent']\n",
      "\t- Saving accounts: [nan 'little' 'quite rich' 'rich' 'moderate']\n",
      "\t- Checking account: ['little' 'moderate' nan 'rich']\n",
      "\t- Purpose: ['radio/TV' 'education' 'furniture/equipment' 'car' 'business'\n",
      " 'domestic appliances' 'repairs' 'vacation/others']\n"
     ]
    }
   ],
   "source": [
    "original_data = pd.read_csv('data/german_credit_data_updated.csv')\n",
    "\n",
    "# Dataset overview - German Credit Risk (from Kaggle):\n",
    "# 1. Age (numeric)\n",
    "# 2. Sex (text: male, female)\n",
    "# 3. Job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\n",
    "# 4. Housing (text: own, rent, or free)\n",
    "# 5. Saving accounts (text - little, moderate, quite rich, rich)\n",
    "# 6. Checking account (numeric, in DM - Deutsch Mark)\n",
    "# 7. Credit amount (numeric, in DM)\n",
    "# 8. Duration (numeric, in month)\n",
    "# 9. Purpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)\n",
    "\n",
    "display(original_data.head())\n",
    "display(original_data.describe())\n",
    "display(original_data.info())\n",
    "\n",
    "# Display the unique values of the categorical features:\n",
    "print('Unique values of the categorical features:')\n",
    "for col in original_data.select_dtypes(include='object'):\n",
    "    print(f'\\t- {col}: {original_data[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: Index(['Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account',\n",
      "       'Purpose'],\n",
      "      dtype='object')\n",
      "Numerical features: Index(['Age', 'Credit amount', 'Duration'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Credit_amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Credit_Risk</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Job_highlyskilled</th>\n",
       "      <th>Job_skilled</th>\n",
       "      <th>Job_unskilled_nonresident</th>\n",
       "      <th>Job_unskilled_resident</th>\n",
       "      <th>...</th>\n",
       "      <th>Checking_account_none</th>\n",
       "      <th>Checking_account_rich</th>\n",
       "      <th>Purpose_business</th>\n",
       "      <th>Purpose_car</th>\n",
       "      <th>Purpose_domestic_appliances</th>\n",
       "      <th>Purpose_education</th>\n",
       "      <th>Purpose_furniture_equipment</th>\n",
       "      <th>Purpose_radio_TV</th>\n",
       "      <th>Purpose_repairs</th>\n",
       "      <th>Purpose_vacation_others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Credit_amount  Duration  Credit_Risk  Sex_female  Sex_male  \\\n",
       "0   67           1169         6            0           0         1   \n",
       "1   22           5951        48            1           1         0   \n",
       "2   49           2096        12            0           0         1   \n",
       "3   45           7882        42            0           0         1   \n",
       "4   53           4870        24            1           0         1   \n",
       "\n",
       "   Job_highlyskilled  Job_skilled  Job_unskilled_nonresident  \\\n",
       "0                  0            1                          0   \n",
       "1                  0            1                          0   \n",
       "2                  0            0                          0   \n",
       "3                  0            1                          0   \n",
       "4                  0            1                          0   \n",
       "\n",
       "   Job_unskilled_resident  ...  Checking_account_none  Checking_account_rich  \\\n",
       "0                       0  ...                      0                      0   \n",
       "1                       0  ...                      0                      0   \n",
       "2                       1  ...                      1                      0   \n",
       "3                       0  ...                      0                      0   \n",
       "4                       0  ...                      0                      0   \n",
       "\n",
       "   Purpose_business  Purpose_car  Purpose_domestic_appliances  \\\n",
       "0                 0            0                            0   \n",
       "1                 0            0                            0   \n",
       "2                 0            0                            0   \n",
       "3                 0            0                            0   \n",
       "4                 0            1                            0   \n",
       "\n",
       "   Purpose_education  Purpose_furniture_equipment  Purpose_radio_TV  \\\n",
       "0                  0                            0                 1   \n",
       "1                  0                            0                 1   \n",
       "2                  1                            0                 0   \n",
       "3                  0                            1                 0   \n",
       "4                  0                            0                 0   \n",
       "\n",
       "   Purpose_repairs  Purpose_vacation_others  \n",
       "0                0                        0  \n",
       "1                0                        0  \n",
       "2                0                        0  \n",
       "3                0                        0  \n",
       "4                0                        0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 30 columns):\n",
      " #   Column                       Non-Null Count  Dtype\n",
      "---  ------                       --------------  -----\n",
      " 0   Age                          954 non-null    int64\n",
      " 1   Credit_amount                954 non-null    int64\n",
      " 2   Duration                     954 non-null    int64\n",
      " 3   Credit_Risk                  954 non-null    int64\n",
      " 4   Sex_female                   954 non-null    int64\n",
      " 5   Sex_male                     954 non-null    int64\n",
      " 6   Job_highlyskilled            954 non-null    int64\n",
      " 7   Job_skilled                  954 non-null    int64\n",
      " 8   Job_unskilled_nonresident    954 non-null    int64\n",
      " 9   Job_unskilled_resident       954 non-null    int64\n",
      " 10  Housing_free                 954 non-null    int64\n",
      " 11  Housing_own                  954 non-null    int64\n",
      " 12  Housing_rent                 954 non-null    int64\n",
      " 13  Saving_accounts_little       954 non-null    int64\n",
      " 14  Saving_accounts_moderate     954 non-null    int64\n",
      " 15  Saving_accounts_none         954 non-null    int64\n",
      " 16  Saving_accounts_quite_rich   954 non-null    int64\n",
      " 17  Saving_accounts_rich         954 non-null    int64\n",
      " 18  Checking_account_little      954 non-null    int64\n",
      " 19  Checking_account_moderate    954 non-null    int64\n",
      " 20  Checking_account_none        954 non-null    int64\n",
      " 21  Checking_account_rich        954 non-null    int64\n",
      " 22  Purpose_business             954 non-null    int64\n",
      " 23  Purpose_car                  954 non-null    int64\n",
      " 24  Purpose_domestic_appliances  954 non-null    int64\n",
      " 25  Purpose_education            954 non-null    int64\n",
      " 26  Purpose_furniture_equipment  954 non-null    int64\n",
      " 27  Purpose_radio_TV             954 non-null    int64\n",
      " 28  Purpose_repairs              954 non-null    int64\n",
      " 29  Purpose_vacation_others      954 non-null    int64\n",
      "dtypes: int64(30)\n",
      "memory usage: 223.7 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.7694545 , -0.7399179 , -1.22763429, ...,  1.62518349,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-1.18704073,  0.93690642,  2.26068929, ...,  1.62518349,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [ 1.18685641, -0.41486224, -0.72930235, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       ...,\n",
       "       [-1.0111965 , -0.39768023,  1.26402541, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-0.65950803,  0.29240557,  0.26736153, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-0.83535227,  2.69823821,  1.26402541, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_data = original_data.copy()\n",
    "\n",
    "# For savings and checking accounts, we will replace the missing values with 'none':\n",
    "preprocessed_data['Saving accounts'].fillna('none', inplace=True)\n",
    "preprocessed_data['Checking account'].fillna('none', inplace=True)\n",
    "\n",
    "# Dropping index column:\n",
    "preprocessed_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Using pd.dummies to one-hot-encode the categorical features\n",
    "preprocessed_data[\"Job\"] = preprocessed_data[\"Job\"].map({0: 'unskilled_nonresident', 1: 'unskilled_resident',\n",
    "                                                         2: 'skilled', 3: 'highlyskilled'})\n",
    "\n",
    "categorical_features = preprocessed_data.select_dtypes(include='object').columns\n",
    "numerical_features = preprocessed_data.select_dtypes(include='number').columns.drop('Credit Risk')\n",
    "print(f'Categorical features: {categorical_features}')\n",
    "print(f'Numerical features: {numerical_features}')\n",
    "\n",
    "preprocessed_data = pd.get_dummies(preprocessed_data, columns=categorical_features, dtype='int64')\n",
    "\n",
    "# Remapping the target variable to 0 and 1:\n",
    "preprocessed_data['Credit Risk'] = preprocessed_data['Credit Risk'].map({1: 0, 2: 1})\n",
    "\n",
    "# Make sure all column names are valid python identifiers (important for pd.query() calls):\n",
    "preprocessed_data.columns = preprocessed_data.columns.str.replace(' ', '_')\n",
    "preprocessed_data.columns = preprocessed_data.columns.str.replace('/', '_')\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "scaled_preprocessed_data = scaler.fit_transform(preprocessed_data)\n",
    "\n",
    "display(preprocessed_data.head())\n",
    "display(preprocessed_data.info())\n",
    "\n",
    "display(scaled_preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = preprocessed_data['Credit_Risk']\n",
    "X = preprocessed_data.drop(columns='Credit_Risk')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7696335078534031\n",
      "ROC AUC: 0.6830357142857143\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking metrics behaviour for LIME, SHAP and Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2682 - val_loss: 1.2560\n",
      "Epoch 2/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2667 - val_loss: 1.2388\n",
      "Epoch 3/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2304 - val_loss: 1.2231\n",
      "Epoch 4/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2241 - val_loss: 1.2082\n",
      "Epoch 5/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2245 - val_loss: 1.1937\n",
      "Epoch 6/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1595 - val_loss: 1.1795\n",
      "Epoch 7/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1863 - val_loss: 1.1654\n",
      "Epoch 8/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1718 - val_loss: 1.1516\n",
      "Epoch 9/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1345 - val_loss: 1.1379\n",
      "Epoch 10/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1375 - val_loss: 1.1244\n",
      "Epoch 11/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1065 - val_loss: 1.1111\n",
      "Epoch 12/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0908 - val_loss: 1.0980\n",
      "Epoch 13/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1113 - val_loss: 1.0854\n",
      "Epoch 14/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0631 - val_loss: 1.0725\n",
      "Epoch 15/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0329 - val_loss: 1.0599\n",
      "Epoch 16/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0436 - val_loss: 1.0480\n",
      "Epoch 17/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0502 - val_loss: 1.0365\n",
      "Epoch 18/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0614 - val_loss: 1.0248\n",
      "Epoch 19/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0637 - val_loss: 1.0142\n",
      "Epoch 20/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0308 - val_loss: 1.0038\n",
      "Epoch 21/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0060 - val_loss: 0.9940\n",
      "Epoch 22/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9904 - val_loss: 0.9852\n",
      "Epoch 23/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9837 - val_loss: 0.9769\n",
      "Epoch 24/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9701 - val_loss: 0.9690\n",
      "Epoch 25/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9759 - val_loss: 0.9616\n",
      "Epoch 26/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9552 - val_loss: 0.9547\n",
      "Epoch 27/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9464 - val_loss: 0.9483\n",
      "Epoch 28/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9336 - val_loss: 0.9427\n",
      "Epoch 29/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9384 - val_loss: 0.9376\n",
      "Epoch 30/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9672 - val_loss: 0.9327\n",
      "Epoch 31/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9538 - val_loss: 0.9284\n",
      "Epoch 32/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9098 - val_loss: 0.9241\n",
      "Epoch 33/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9488 - val_loss: 0.9201\n",
      "Epoch 34/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9228 - val_loss: 0.9164\n",
      "Epoch 35/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9064 - val_loss: 0.9127\n",
      "Epoch 36/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9029 - val_loss: 0.9093\n",
      "Epoch 37/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9116 - val_loss: 0.9061\n",
      "Epoch 38/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8917 - val_loss: 0.9033\n",
      "Epoch 39/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8981 - val_loss: 0.9006\n",
      "Epoch 40/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8953 - val_loss: 0.8979\n",
      "Epoch 41/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9048 - val_loss: 0.8955\n",
      "Epoch 42/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8817 - val_loss: 0.8933\n",
      "Epoch 43/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9247 - val_loss: 0.8910\n",
      "Epoch 44/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9257 - val_loss: 0.8886\n",
      "Epoch 45/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9103 - val_loss: 0.8864\n",
      "Epoch 46/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9166 - val_loss: 0.8839\n",
      "Epoch 47/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8964 - val_loss: 0.8819\n",
      "Epoch 48/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8493 - val_loss: 0.8800\n",
      "Epoch 49/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8972 - val_loss: 0.8783\n",
      "Epoch 50/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8881 - val_loss: 0.8765\n",
      "Epoch 51/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8711 - val_loss: 0.8750\n",
      "Epoch 52/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8718 - val_loss: 0.8731\n",
      "Epoch 53/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8928 - val_loss: 0.8715\n",
      "Epoch 54/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8745 - val_loss: 0.8699\n",
      "Epoch 55/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8595 - val_loss: 0.8688\n",
      "Epoch 56/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8715 - val_loss: 0.8673\n",
      "Epoch 57/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8743 - val_loss: 0.8659\n",
      "Epoch 58/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8820 - val_loss: 0.8646\n",
      "Epoch 59/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8942 - val_loss: 0.8633\n",
      "Epoch 60/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8745 - val_loss: 0.8624\n",
      "Epoch 61/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8358 - val_loss: 0.8610\n",
      "Epoch 62/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8315 - val_loss: 0.8601\n",
      "Epoch 63/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8798 - val_loss: 0.8590\n",
      "Epoch 64/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8451 - val_loss: 0.8579\n",
      "Epoch 65/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8525 - val_loss: 0.8568\n",
      "Epoch 66/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8713 - val_loss: 0.8557\n",
      "Epoch 67/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8710 - val_loss: 0.8545\n",
      "Epoch 68/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8335 - val_loss: 0.8536\n",
      "Epoch 69/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8743 - val_loss: 0.8525\n",
      "Epoch 70/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8583 - val_loss: 0.8514\n",
      "Epoch 71/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8846 - val_loss: 0.8503\n",
      "Epoch 72/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8423 - val_loss: 0.8496\n",
      "Epoch 73/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8403 - val_loss: 0.8487\n",
      "Epoch 74/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8409 - val_loss: 0.8479\n",
      "Epoch 75/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8629 - val_loss: 0.8469\n",
      "Epoch 76/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8458 - val_loss: 0.8460\n",
      "Epoch 77/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8416 - val_loss: 0.8451\n",
      "Epoch 78/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8566 - val_loss: 0.8445\n",
      "Epoch 79/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8528 - val_loss: 0.8436\n",
      "Epoch 80/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8723 - val_loss: 0.8428\n",
      "Epoch 81/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8113 - val_loss: 0.8420\n",
      "Epoch 82/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8334 - val_loss: 0.8412\n",
      "Epoch 83/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8651 - val_loss: 0.8403\n",
      "Epoch 84/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8286 - val_loss: 0.8398\n",
      "Epoch 85/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8321 - val_loss: 0.8391\n",
      "Epoch 86/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9114 - val_loss: 0.8384\n",
      "Epoch 87/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8903 - val_loss: 0.8378\n",
      "Epoch 88/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8520 - val_loss: 0.8371\n",
      "Epoch 89/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8287 - val_loss: 0.8365\n",
      "Epoch 90/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8351 - val_loss: 0.8357\n",
      "Epoch 91/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8696 - val_loss: 0.8351\n",
      "Epoch 92/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8413 - val_loss: 0.8347\n",
      "Epoch 93/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8388 - val_loss: 0.8342\n",
      "Epoch 94/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8263 - val_loss: 0.8335\n",
      "Epoch 95/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8497 - val_loss: 0.8331\n",
      "Epoch 96/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8586 - val_loss: 0.8327\n",
      "Epoch 97/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8482 - val_loss: 0.8320\n",
      "Epoch 98/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8487 - val_loss: 0.8310\n",
      "Epoch 99/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8097 - val_loss: 0.8302\n",
      "Epoch 100/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8176 - val_loss: 0.8298\n",
      "Epoch 101/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8157 - val_loss: 0.8293\n",
      "Epoch 102/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8033 - val_loss: 0.8287\n",
      "Epoch 103/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7970 - val_loss: 0.8281\n",
      "Epoch 104/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8367 - val_loss: 0.8275\n",
      "Epoch 105/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8497 - val_loss: 0.8269\n",
      "Epoch 106/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8390 - val_loss: 0.8265\n",
      "Epoch 107/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7882 - val_loss: 0.8259\n",
      "Epoch 108/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8287 - val_loss: 0.8253\n",
      "Epoch 109/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8274 - val_loss: 0.8248\n",
      "Epoch 110/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8417 - val_loss: 0.8244\n",
      "Epoch 111/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8360 - val_loss: 0.8239\n",
      "Epoch 112/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8342 - val_loss: 0.8234\n",
      "Epoch 113/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8383 - val_loss: 0.8232\n",
      "Epoch 114/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7981 - val_loss: 0.8227\n",
      "Epoch 115/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8118 - val_loss: 0.8220\n",
      "Epoch 116/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8374 - val_loss: 0.8214\n",
      "Epoch 117/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8157 - val_loss: 0.8208\n",
      "Epoch 118/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8101 - val_loss: 0.8203\n",
      "Epoch 119/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8080 - val_loss: 0.8200\n",
      "Epoch 120/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8021 - val_loss: 0.8197\n",
      "Epoch 121/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8332 - val_loss: 0.8191\n",
      "Epoch 122/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8334 - val_loss: 0.8183\n",
      "Epoch 123/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8362 - val_loss: 0.8182\n",
      "Epoch 124/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7974 - val_loss: 0.8179\n",
      "Epoch 125/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8116 - val_loss: 0.8172\n",
      "Epoch 126/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8348 - val_loss: 0.8168\n",
      "Epoch 127/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8368 - val_loss: 0.8163\n",
      "Epoch 128/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8052 - val_loss: 0.8157\n",
      "Epoch 129/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8427 - val_loss: 0.8155\n",
      "Epoch 130/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8207 - val_loss: 0.8150\n",
      "Epoch 131/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7905 - val_loss: 0.8145\n",
      "Epoch 132/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8216 - val_loss: 0.8142\n",
      "Epoch 133/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7982 - val_loss: 0.8138\n",
      "Epoch 134/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8000 - val_loss: 0.8134\n",
      "Epoch 135/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8077 - val_loss: 0.8131\n",
      "Epoch 136/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8116 - val_loss: 0.8125\n",
      "Epoch 137/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8440 - val_loss: 0.8120\n",
      "Epoch 138/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8146 - val_loss: 0.8117\n",
      "Epoch 139/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8364 - val_loss: 0.8113\n",
      "Epoch 140/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8226 - val_loss: 0.8109\n",
      "Epoch 141/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7648 - val_loss: 0.8106\n",
      "Epoch 142/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8102 - val_loss: 0.8102\n",
      "Epoch 143/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7910 - val_loss: 0.8094\n",
      "Epoch 144/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8029 - val_loss: 0.8090\n",
      "Epoch 145/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8082 - val_loss: 0.8087\n",
      "Epoch 146/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8326 - val_loss: 0.8083\n",
      "Epoch 147/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8105 - val_loss: 0.8077\n",
      "Epoch 148/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7800 - val_loss: 0.8073\n",
      "Epoch 149/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7893 - val_loss: 0.8067\n",
      "Epoch 150/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8339 - val_loss: 0.8061\n",
      "Epoch 151/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8257 - val_loss: 0.8056\n",
      "Epoch 152/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7943 - val_loss: 0.8051\n",
      "Epoch 153/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8011 - val_loss: 0.8048\n",
      "Epoch 154/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7908 - val_loss: 0.8043\n",
      "Epoch 155/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7865 - val_loss: 0.8040\n",
      "Epoch 156/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7859 - val_loss: 0.8035\n",
      "Epoch 157/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7693 - val_loss: 0.8030\n",
      "Epoch 158/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7803 - val_loss: 0.8029\n",
      "Epoch 159/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8110 - val_loss: 0.8023\n",
      "Epoch 160/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7617 - val_loss: 0.8021\n",
      "Epoch 161/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7852 - val_loss: 0.8017\n",
      "Epoch 162/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7800 - val_loss: 0.8013\n",
      "Epoch 163/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7696 - val_loss: 0.8006\n",
      "Epoch 164/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7945 - val_loss: 0.8001\n",
      "Epoch 165/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8021 - val_loss: 0.7997\n",
      "Epoch 166/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8016 - val_loss: 0.7996\n",
      "Epoch 167/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7980 - val_loss: 0.7990\n",
      "Epoch 168/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7954 - val_loss: 0.7988\n",
      "Epoch 169/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7969 - val_loss: 0.7986\n",
      "Epoch 170/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7768 - val_loss: 0.7982\n",
      "Epoch 171/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8121 - val_loss: 0.7977\n",
      "Epoch 172/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7726 - val_loss: 0.7973\n",
      "Epoch 173/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8118 - val_loss: 0.7970\n",
      "Epoch 174/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7792 - val_loss: 0.7966\n",
      "Epoch 175/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7600 - val_loss: 0.7966\n",
      "Epoch 176/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7751 - val_loss: 0.7964\n",
      "Epoch 177/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8002 - val_loss: 0.7962\n",
      "Epoch 178/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7815 - val_loss: 0.7957\n",
      "Epoch 179/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8008 - val_loss: 0.7953\n",
      "Epoch 180/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8146 - val_loss: 0.7947\n",
      "Epoch 181/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7667 - val_loss: 0.7946\n",
      "Epoch 182/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7529 - val_loss: 0.7944\n",
      "Epoch 183/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7939 - val_loss: 0.7939\n",
      "Epoch 184/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7928 - val_loss: 0.7934\n",
      "Epoch 185/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7718 - val_loss: 0.7930\n",
      "Epoch 186/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7613 - val_loss: 0.7927\n",
      "Epoch 187/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7998 - val_loss: 0.7928\n",
      "Epoch 188/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7942 - val_loss: 0.7926\n",
      "Epoch 189/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7872 - val_loss: 0.7923\n",
      "Epoch 190/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7593 - val_loss: 0.7920\n",
      "Epoch 191/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8078 - val_loss: 0.7916\n",
      "Epoch 192/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7894 - val_loss: 0.7916\n",
      "Epoch 193/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7626 - val_loss: 0.7912\n",
      "Epoch 194/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7951 - val_loss: 0.7907\n",
      "Epoch 195/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7724 - val_loss: 0.7905\n",
      "Epoch 196/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7812 - val_loss: 0.7903\n",
      "Epoch 197/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7924 - val_loss: 0.7899\n",
      "Epoch 198/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7881 - val_loss: 0.7898\n",
      "Epoch 199/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7557 - val_loss: 0.7893\n",
      "Epoch 200/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7666 - val_loss: 0.7890\n",
      "Epoch 201/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7981 - val_loss: 0.7885\n",
      "Epoch 202/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7728 - val_loss: 0.7882\n",
      "Epoch 203/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7508 - val_loss: 0.7881\n",
      "Epoch 204/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7868 - val_loss: 0.7877\n",
      "Epoch 205/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7728 - val_loss: 0.7873\n",
      "Epoch 206/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7586 - val_loss: 0.7869\n",
      "Epoch 207/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7631 - val_loss: 0.7866\n",
      "Epoch 208/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7819 - val_loss: 0.7865\n",
      "Epoch 209/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7566 - val_loss: 0.7863\n",
      "Epoch 210/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8117 - val_loss: 0.7862\n",
      "Epoch 211/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7655 - val_loss: 0.7861\n",
      "Epoch 212/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7736 - val_loss: 0.7861\n",
      "Epoch 213/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7673 - val_loss: 0.7856\n",
      "Epoch 214/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7790 - val_loss: 0.7853\n",
      "Epoch 215/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7718 - val_loss: 0.7850\n",
      "Epoch 216/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7599 - val_loss: 0.7847\n",
      "Epoch 217/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7652 - val_loss: 0.7844\n",
      "Epoch 218/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7982 - val_loss: 0.7841\n",
      "Epoch 219/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7501 - val_loss: 0.7835\n",
      "Epoch 220/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7632 - val_loss: 0.7834\n",
      "Epoch 221/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7572 - val_loss: 0.7833\n",
      "Epoch 222/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7796 - val_loss: 0.7829\n",
      "Epoch 223/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7839 - val_loss: 0.7825\n",
      "Epoch 224/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7365 - val_loss: 0.7822\n",
      "Epoch 225/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7394 - val_loss: 0.7819\n",
      "Epoch 226/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7674 - val_loss: 0.7818\n",
      "Epoch 227/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8139 - val_loss: 0.7819\n",
      "Epoch 228/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7249 - val_loss: 0.7820\n",
      "Epoch 229/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7492 - val_loss: 0.7817\n",
      "Epoch 230/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7700 - val_loss: 0.7814\n",
      "Epoch 231/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7824 - val_loss: 0.7807\n",
      "Epoch 232/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7649 - val_loss: 0.7807\n",
      "Epoch 233/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7550 - val_loss: 0.7807\n",
      "Epoch 234/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7698 - val_loss: 0.7806\n",
      "Epoch 235/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7345 - val_loss: 0.7804\n",
      "Epoch 236/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7766 - val_loss: 0.7802\n",
      "Epoch 237/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7765 - val_loss: 0.7801\n",
      "Epoch 238/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7648 - val_loss: 0.7798\n",
      "Epoch 239/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7832 - val_loss: 0.7796\n",
      "Epoch 240/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7556 - val_loss: 0.7793\n",
      "Epoch 241/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7448 - val_loss: 0.7789\n",
      "Epoch 242/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7582 - val_loss: 0.7791\n",
      "Epoch 243/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7867 - val_loss: 0.7790\n",
      "Epoch 244/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7635 - val_loss: 0.7788\n",
      "Epoch 245/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7587 - val_loss: 0.7789\n",
      "Epoch 246/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7556 - val_loss: 0.7783\n",
      "Epoch 247/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7664 - val_loss: 0.7783\n",
      "Epoch 248/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7812 - val_loss: 0.7782\n",
      "Epoch 249/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7777 - val_loss: 0.7777\n",
      "Epoch 250/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7839 - val_loss: 0.7773\n",
      "Epoch 251/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8116 - val_loss: 0.7770\n",
      "Epoch 252/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7413 - val_loss: 0.7766\n",
      "Epoch 253/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7738 - val_loss: 0.7764\n",
      "Epoch 254/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7689 - val_loss: 0.7761\n",
      "Epoch 255/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7856 - val_loss: 0.7757\n",
      "Epoch 256/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7829 - val_loss: 0.7757\n",
      "Epoch 257/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7920 - val_loss: 0.7757\n",
      "Epoch 258/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7672 - val_loss: 0.7755\n",
      "Epoch 259/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7614 - val_loss: 0.7757\n",
      "Epoch 260/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7356 - val_loss: 0.7755\n",
      "Epoch 261/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7351 - val_loss: 0.7756\n",
      "Epoch 262/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7541 - val_loss: 0.7757\n",
      "Epoch 263/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7768 - val_loss: 0.7753\n",
      "Epoch 264/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7541 - val_loss: 0.7749\n",
      "Epoch 265/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7674 - val_loss: 0.7749\n",
      "Epoch 266/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7544 - val_loss: 0.7742\n",
      "Epoch 267/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7807 - val_loss: 0.7740\n",
      "Epoch 268/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7815 - val_loss: 0.7740\n",
      "Epoch 269/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7716 - val_loss: 0.7741\n",
      "Epoch 270/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7975 - val_loss: 0.7741\n",
      "Epoch 271/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7535 - val_loss: 0.7736\n",
      "Epoch 272/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7304 - val_loss: 0.7735\n",
      "Epoch 273/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7427 - val_loss: 0.7732\n",
      "Epoch 274/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7507 - val_loss: 0.7732\n",
      "Epoch 275/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7755 - val_loss: 0.7732\n",
      "Epoch 276/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7785 - val_loss: 0.7732\n",
      "Epoch 277/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7413 - val_loss: 0.7734\n",
      "Epoch 278/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7552 - val_loss: 0.7731\n",
      "Epoch 279/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8127 - val_loss: 0.7729\n",
      "Epoch 280/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7364 - val_loss: 0.7728\n",
      "Epoch 281/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7454 - val_loss: 0.7727\n",
      "Epoch 282/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7578 - val_loss: 0.7725\n",
      "Epoch 283/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7308 - val_loss: 0.7722\n",
      "Epoch 284/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7611 - val_loss: 0.7724\n",
      "Epoch 285/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7783 - val_loss: 0.7721\n",
      "Epoch 286/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7309 - val_loss: 0.7721\n",
      "Epoch 287/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7668 - val_loss: 0.7721\n",
      "Epoch 288/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7454 - val_loss: 0.7718\n",
      "Epoch 289/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7641 - val_loss: 0.7716\n",
      "Epoch 290/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7806 - val_loss: 0.7717\n",
      "Epoch 291/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7183 - val_loss: 0.7715\n",
      "Epoch 292/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7489 - val_loss: 0.7717\n",
      "Epoch 293/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7182 - val_loss: 0.7716\n",
      "Epoch 294/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7764 - val_loss: 0.7716\n",
      "Epoch 295/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7836 - val_loss: 0.7715\n",
      "Epoch 296/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7495 - val_loss: 0.7713\n",
      "Epoch 297/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7920 - val_loss: 0.7711\n",
      "Epoch 298/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7833 - val_loss: 0.7713\n",
      "Epoch 299/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7628 - val_loss: 0.7712\n",
      "Epoch 300/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7286 - val_loss: 0.7709\n",
      "Epoch 301/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7537 - val_loss: 0.7708\n",
      "Epoch 302/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7483 - val_loss: 0.7703\n",
      "Epoch 303/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7345 - val_loss: 0.7703\n",
      "Epoch 304/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7587 - val_loss: 0.7703\n",
      "Epoch 305/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7585 - val_loss: 0.7700\n",
      "Epoch 306/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7666 - val_loss: 0.7700\n",
      "Epoch 307/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7548 - val_loss: 0.7702\n",
      "Epoch 308/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7590 - val_loss: 0.7702\n",
      "Epoch 309/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7179 - val_loss: 0.7699\n",
      "Epoch 310/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7590 - val_loss: 0.7697\n",
      "Epoch 311/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7471 - val_loss: 0.7693\n",
      "Epoch 312/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7560 - val_loss: 0.7692\n",
      "Epoch 313/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7783 - val_loss: 0.7695\n",
      "Epoch 314/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7639 - val_loss: 0.7694\n",
      "Epoch 315/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7731 - val_loss: 0.7691\n",
      "Epoch 316/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7548 - val_loss: 0.7687\n",
      "Epoch 317/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7403 - val_loss: 0.7683\n",
      "Epoch 318/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7146 - val_loss: 0.7684\n",
      "Epoch 319/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7736 - val_loss: 0.7683\n",
      "Epoch 320/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7861 - val_loss: 0.7684\n",
      "Epoch 321/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7256 - val_loss: 0.7683\n",
      "Epoch 322/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7761 - val_loss: 0.7682\n",
      "Epoch 323/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7450 - val_loss: 0.7686\n",
      "Epoch 324/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7214 - val_loss: 0.7683\n",
      "Epoch 325/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7793 - val_loss: 0.7682\n",
      "Epoch 326/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7504 - val_loss: 0.7682\n",
      "Epoch 327/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7515 - val_loss: 0.7682\n",
      "Epoch 328/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7319 - val_loss: 0.7682\n",
      "Epoch 329/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7692 - val_loss: 0.7678\n",
      "Epoch 330/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7477 - val_loss: 0.7677\n",
      "Epoch 331/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7384 - val_loss: 0.7674\n",
      "Epoch 332/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7510 - val_loss: 0.7672\n",
      "Epoch 333/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7808 - val_loss: 0.7671\n",
      "Epoch 334/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7677 - val_loss: 0.7672\n",
      "Epoch 335/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7829 - val_loss: 0.7672\n",
      "Epoch 336/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7882 - val_loss: 0.7672\n",
      "Epoch 337/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7651 - val_loss: 0.7671\n",
      "Epoch 338/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7492 - val_loss: 0.7670\n",
      "Epoch 339/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7848 - val_loss: 0.7670\n",
      "Epoch 340/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7651 - val_loss: 0.7667\n",
      "Epoch 341/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7614 - val_loss: 0.7664\n",
      "Epoch 342/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7707 - val_loss: 0.7663\n",
      "Epoch 343/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7592 - val_loss: 0.7664\n",
      "Epoch 344/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7365 - val_loss: 0.7663\n",
      "Epoch 345/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7640 - val_loss: 0.7661\n",
      "Epoch 346/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7640 - val_loss: 0.7660\n",
      "Epoch 347/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7483 - val_loss: 0.7659\n",
      "Epoch 348/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7479 - val_loss: 0.7658\n",
      "Epoch 349/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7556 - val_loss: 0.7656\n",
      "Epoch 350/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7496 - val_loss: 0.7658\n",
      "Epoch 351/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7671 - val_loss: 0.7652\n",
      "Epoch 352/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7670 - val_loss: 0.7650\n",
      "Epoch 353/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7611 - val_loss: 0.7650\n",
      "Epoch 354/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7877 - val_loss: 0.7651\n",
      "Epoch 355/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7415 - val_loss: 0.7653\n",
      "Epoch 356/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7534 - val_loss: 0.7655\n",
      "Epoch 357/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7614 - val_loss: 0.7655\n",
      "Epoch 358/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7321 - val_loss: 0.7653\n",
      "Epoch 359/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7633 - val_loss: 0.7654\n",
      "Epoch 360/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7549 - val_loss: 0.7655\n",
      "Epoch 361/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7724 - val_loss: 0.7655\n",
      "Epoch 362/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7411 - val_loss: 0.7651\n",
      "Epoch 363/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7401 - val_loss: 0.7648\n",
      "Epoch 364/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7411 - val_loss: 0.7645\n",
      "Epoch 365/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7591 - val_loss: 0.7643\n",
      "Epoch 366/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7695 - val_loss: 0.7644\n",
      "Epoch 367/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7278 - val_loss: 0.7642\n",
      "Epoch 368/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7291 - val_loss: 0.7639\n",
      "Epoch 369/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7165 - val_loss: 0.7638\n",
      "Epoch 370/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7594 - val_loss: 0.7638\n",
      "Epoch 371/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7445 - val_loss: 0.7638\n",
      "Epoch 372/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7475 - val_loss: 0.7638\n",
      "Epoch 373/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7599 - val_loss: 0.7635\n",
      "Epoch 374/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7671 - val_loss: 0.7635\n",
      "Epoch 375/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7511 - val_loss: 0.7632\n",
      "Epoch 376/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7589 - val_loss: 0.7633\n",
      "Epoch 377/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7516 - val_loss: 0.7635\n",
      "Epoch 378/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7449 - val_loss: 0.7637\n",
      "Epoch 379/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7527 - val_loss: 0.7633\n",
      "Epoch 380/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7467 - val_loss: 0.7629\n",
      "Epoch 381/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7379 - val_loss: 0.7630\n",
      "Epoch 382/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7325 - val_loss: 0.7626\n",
      "Epoch 383/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7358 - val_loss: 0.7624\n",
      "Epoch 384/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7583 - val_loss: 0.7624\n",
      "Epoch 385/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7517 - val_loss: 0.7625\n",
      "Epoch 386/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7397 - val_loss: 0.7627\n",
      "Epoch 387/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7924 - val_loss: 0.7623\n",
      "Epoch 388/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7420 - val_loss: 0.7622\n",
      "Epoch 389/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7078 - val_loss: 0.7621\n",
      "Epoch 390/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7453 - val_loss: 0.7622\n",
      "Epoch 391/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7466 - val_loss: 0.7621\n",
      "Epoch 392/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7514 - val_loss: 0.7618\n",
      "Epoch 393/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7456 - val_loss: 0.7615\n",
      "Epoch 394/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7345 - val_loss: 0.7614\n",
      "Epoch 395/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7485 - val_loss: 0.7613\n",
      "Epoch 396/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7614 - val_loss: 0.7615\n",
      "Epoch 397/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7330 - val_loss: 0.7615\n",
      "Epoch 398/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7527 - val_loss: 0.7615\n",
      "Epoch 399/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7436 - val_loss: 0.7612\n",
      "Epoch 400/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7202 - val_loss: 0.7611\n",
      "Epoch 401/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7492 - val_loss: 0.7614\n",
      "Epoch 402/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7834 - val_loss: 0.7614\n",
      "Epoch 403/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7450 - val_loss: 0.7614\n",
      "Epoch 404/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7382 - val_loss: 0.7612\n",
      "Epoch 405/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7315 - val_loss: 0.7615\n",
      "Epoch 406/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7160 - val_loss: 0.7613\n",
      "Epoch 407/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7584 - val_loss: 0.7611\n",
      "Epoch 408/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7732 - val_loss: 0.7612\n",
      "Epoch 409/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7325 - val_loss: 0.7613\n",
      "Epoch 410/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7397 - val_loss: 0.7612\n",
      "Epoch 411/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7472 - val_loss: 0.7613\n",
      "Epoch 412/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7790 - val_loss: 0.7611\n",
      "Epoch 413/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7545 - val_loss: 0.7612\n",
      "Epoch 414/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7215 - val_loss: 0.7609\n",
      "Epoch 415/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7927 - val_loss: 0.7610\n",
      "Epoch 416/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7568 - val_loss: 0.7608\n",
      "Epoch 417/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7722 - val_loss: 0.7608\n",
      "Epoch 418/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7388 - val_loss: 0.7610\n",
      "Epoch 419/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7484 - val_loss: 0.7609\n",
      "Epoch 420/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7397 - val_loss: 0.7609\n",
      "Epoch 421/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7652 - val_loss: 0.7610\n",
      "Epoch 422/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7435 - val_loss: 0.7609\n",
      "Epoch 423/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7678 - val_loss: 0.7609\n",
      "Epoch 424/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7299 - val_loss: 0.7609\n",
      "Epoch 425/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7621 - val_loss: 0.7612\n",
      "Epoch 426/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7392 - val_loss: 0.7610\n",
      "Epoch 427/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7285 - val_loss: 0.7607\n",
      "Epoch 428/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7384 - val_loss: 0.7605\n",
      "Epoch 429/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7496 - val_loss: 0.7603\n",
      "Epoch 430/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7627 - val_loss: 0.7600\n",
      "Epoch 431/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7351 - val_loss: 0.7602\n",
      "Epoch 432/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7462 - val_loss: 0.7602\n",
      "Epoch 433/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7548 - val_loss: 0.7601\n",
      "Epoch 434/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7525 - val_loss: 0.7599\n",
      "Epoch 435/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7389 - val_loss: 0.7597\n",
      "Epoch 436/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6980 - val_loss: 0.7595\n",
      "Epoch 437/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7264 - val_loss: 0.7594\n",
      "Epoch 438/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7567 - val_loss: 0.7596\n",
      "Epoch 439/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7217 - val_loss: 0.7596\n",
      "Epoch 440/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7699 - val_loss: 0.7596\n",
      "Epoch 441/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7265 - val_loss: 0.7594\n",
      "Epoch 442/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7343 - val_loss: 0.7596\n",
      "Epoch 443/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7174 - val_loss: 0.7594\n",
      "Epoch 444/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7224 - val_loss: 0.7592\n",
      "Epoch 445/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7437 - val_loss: 0.7587\n",
      "Epoch 446/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7620 - val_loss: 0.7588\n",
      "Epoch 447/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7420 - val_loss: 0.7589\n",
      "Epoch 448/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7114 - val_loss: 0.7590\n",
      "Epoch 449/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7213 - val_loss: 0.7587\n",
      "Epoch 450/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7292 - val_loss: 0.7591\n",
      "Epoch 451/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7544 - val_loss: 0.7591\n",
      "Epoch 452/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7265 - val_loss: 0.7590\n",
      "Epoch 453/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7736 - val_loss: 0.7588\n",
      "Epoch 454/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7107 - val_loss: 0.7587\n",
      "Epoch 455/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7083 - val_loss: 0.7585\n",
      "Epoch 456/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7323 - val_loss: 0.7589\n",
      "Epoch 457/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7476 - val_loss: 0.7591\n",
      "Epoch 458/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7840 - val_loss: 0.7590\n",
      "Epoch 459/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7301 - val_loss: 0.7589\n",
      "Epoch 460/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7430 - val_loss: 0.7587\n",
      "Epoch 461/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7463 - val_loss: 0.7585\n",
      "Epoch 462/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7706 - val_loss: 0.7586\n",
      "Epoch 463/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7484 - val_loss: 0.7586\n",
      "Epoch 464/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7413 - val_loss: 0.7587\n",
      "Epoch 465/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7050 - val_loss: 0.7586\n",
      "Epoch 466/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7486 - val_loss: 0.7584\n",
      "Epoch 467/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7528 - val_loss: 0.7582\n",
      "Epoch 468/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7378 - val_loss: 0.7581\n",
      "Epoch 469/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7439 - val_loss: 0.7580\n",
      "Epoch 470/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7361 - val_loss: 0.7578\n",
      "Epoch 471/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7460 - val_loss: 0.7576\n",
      "Epoch 472/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7482 - val_loss: 0.7579\n",
      "Epoch 473/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7415 - val_loss: 0.7579\n",
      "Epoch 474/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7252 - val_loss: 0.7577\n",
      "Epoch 475/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7282 - val_loss: 0.7577\n",
      "Epoch 476/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7593 - val_loss: 0.7578\n",
      "Epoch 477/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7476 - val_loss: 0.7575\n",
      "Epoch 478/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7414 - val_loss: 0.7575\n",
      "Epoch 479/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7393 - val_loss: 0.7577\n",
      "Epoch 480/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7859 - val_loss: 0.7575\n",
      "Epoch 481/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7577 - val_loss: 0.7573\n",
      "Epoch 482/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7773 - val_loss: 0.7572\n",
      "Epoch 483/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7159 - val_loss: 0.7572\n",
      "Epoch 484/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7393 - val_loss: 0.7571\n",
      "Epoch 485/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7445 - val_loss: 0.7567\n",
      "Epoch 486/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7424 - val_loss: 0.7567\n",
      "Epoch 487/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7263 - val_loss: 0.7566\n",
      "Epoch 488/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7953 - val_loss: 0.7567\n",
      "Epoch 489/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7377 - val_loss: 0.7570\n",
      "Epoch 490/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7280 - val_loss: 0.7568\n",
      "Epoch 491/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7246 - val_loss: 0.7569\n",
      "Epoch 492/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7292 - val_loss: 0.7565\n",
      "Epoch 493/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6936 - val_loss: 0.7561\n",
      "Epoch 494/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7575 - val_loss: 0.7563\n",
      "Epoch 495/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7606 - val_loss: 0.7563\n",
      "Epoch 496/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7327 - val_loss: 0.7563\n",
      "Epoch 497/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7464 - val_loss: 0.7566\n",
      "Epoch 498/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7121 - val_loss: 0.7566\n",
      "Epoch 499/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7362 - val_loss: 0.7562\n",
      "Epoch 500/500\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7206 - val_loss: 0.7561\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step\n"
     ]
    }
   ],
   "source": [
    "shap_exp = ShapTabularTreeWrapper(clf, X_train, categorical_features)\n",
    "lime_exp = LimeWrapper(clf, X_train, categorical_features)\n",
    "anchor_exp = AnchorWrapper(clf, X_train, categorical_features)\n",
    "\n",
    "evaluator = ExplanationModelEvaluator(clf, X_train, categorical_features, noise_gen_args={'encoding_dim': 5, 'epochs': 500})\n",
    "evaluator.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking instance 527\n",
      "Checking instance 235\n",
      "Checking instance 398\n",
      "Checking instance 948\n",
      "Checking instance 633\n",
      "Checking instance 692\n",
      "Checking instance 918\n",
      "Checking instance 695\n",
      "Checking instance 296\n",
      "Checking instance 689\n",
      "Checking instance 890\n",
      "Checking instance 424\n",
      "Checking instance 412\n",
      "Checking instance 63\n",
      "Checking instance 634\n",
      "Checking instance 66\n",
      "Checking instance 826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking instance 346\n",
      "Checking instance 67\n",
      "Checking instance 788\n",
      "Checking instance 481\n",
      "Checking instance 660\n",
      "Checking instance 751\n",
      "Checking instance 342\n",
      "Checking instance 209\n",
      "Checking instance 789\n",
      "Checking instance 306\n",
      "Checking instance 468\n",
      "Checking instance 513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking instance 870\n",
      "Checking instance 786\n",
      "Checking instance 580\n",
      "Checking instance 745\n",
      "Checking instance 30\n",
      "Checking instance 357\n",
      "Checking instance 707\n",
      "Checking instance 33\n",
      "Checking instance 666\n",
      "Checking instance 547\n",
      "Checking instance 928\n",
      "Checking instance 827\n",
      "Checking instance 910\n",
      "Checking instance 858\n",
      "Checking instance 485\n",
      "Checking instance 881\n",
      "Checking instance 714\n",
      "Checking instance 684\n",
      "Checking instance 70\n",
      "Checking instance 567\n",
      "Checking instance 732\n",
      "Checking instance 39\n",
      "Checking instance 841\n",
      "Checking instance 568\n",
      "Checking instance 370\n",
      "Checking instance 174\n",
      "Checking instance 139\n",
      "Checking instance 218\n",
      "Checking instance 522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking instance 625\n",
      "Checking instance 917\n",
      "Checking instance 882\n",
      "Checking instance 493\n",
      "Checking instance 158\n",
      "Checking instance 78\n",
      "Checking instance 800\n",
      "Checking instance 903\n",
      "Checking instance 949\n",
      "Checking instance 49\n",
      "Checking instance 530\n",
      "Checking instance 694\n",
      "Checking instance 621\n",
      "Checking instance 321\n",
      "Checking instance 617\n",
      "Checking instance 601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking instance 819\n",
      "Checking instance 199\n",
      "Checking instance 137\n",
      "Checking instance 596\n",
      "Checking instance 603\n",
      "Checking instance 286\n",
      "Checking instance 529\n",
      "Checking instance 381\n",
      "Checking instance 318\n",
      "Checking instance 260\n",
      "Checking instance 314\n",
      "Checking instance 593\n",
      "Checking instance 213\n",
      "Checking instance 422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking instance 302\n",
      "Checking instance 679\n",
      "Checking instance 120\n",
      "Checking instance 500\n",
      "Checking instance 436\n",
      "Checking instance 850\n",
      "Checking instance 31\n",
      "Checking instance 88\n",
      "Checking instance 280\n",
      "Checking instance 728\n",
      "Checking instance 265\n",
      "Checking instance 382\n"
     ]
    }
   ],
   "source": [
    "metric_runs = {shap_exp: [], lime_exp: [], anchor_exp: []} # Arrays of the format [[faithfulness1, sensitivity1, complexity1], [faithfulness2, sensitivity2, complexity2], ...]\n",
    "\n",
    "num_instances_to_check = 100\n",
    "# Chosse num_isntances_to_check unique indexes from X_test size\n",
    "indexes = np.random.choice(X_test.index, num_instances_to_check, replace=False)\n",
    "\n",
    "for idx in indexes:\n",
    "    print (f'Checking instance {idx}')\n",
    "    instance_data_row = X_test.loc[idx]\n",
    "    for exp in [shap_exp, lime_exp, anchor_exp]:\n",
    "        row = [\n",
    "            idx,\n",
    "            evaluator.faithfullness_correlation(exp, instance_data_row),\n",
    "            evaluator.sensitivity(exp, instance_data_row),\n",
    "            evaluator.complexity(exp, instance_data_row)\n",
    "        ]\n",
    "        metric_runs[exp].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change metric_runs keys to [\"SHAP\", \"LIME\", \"Anchor\"]\n",
    "metric_runs1 = {\"shap\": metric_runs[shap_exp], \"lime\": metric_runs[lime_exp], \"anchor\": metric_runs[anchor_exp]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shap': [[0.5723255128372227, 0.9681694998768169, 2.3930018139207707],\n",
       "  [0.4455079719457512, 0.7932906283588386, 2.5643626036115066],\n",
       "  [0.11866524032382038, 0.5698036691722417, 2.4865124394765457],\n",
       "  [0.3321909828950127, 0.5973491832952431, 2.267543704339637],\n",
       "  [0.3880845549517916, 0.651052978785058, 2.1921237565267475],\n",
       "  [0.045062567054379186, 0.7173494575917096, 2.4287078477545365],\n",
       "  [0.3533635648657668, 0.8600620161683944, 2.55966172035827],\n",
       "  [0.6466996145829886, 0.6259177137225918, 2.3808836586188544],\n",
       "  [0.38288184555107013, 0.7660893228293795, 2.5972896124354863],\n",
       "  [0.28946033910655344, 0.6925758450380424, 2.4304895479349486],\n",
       "  [0.4108290471891962, 0.6841527247710804, 2.539228234839168],\n",
       "  [0.4595011656489133, 0.7233812655654359, 2.7126702640784655],\n",
       "  [0.11344736925449553, 0.6884734997750285, 2.4962205489144815],\n",
       "  [0.912693063962245, 0.7772242998504544, 2.2466135681909547],\n",
       "  [0.5627141313984826, 0.8864549119769549, 2.4792100295543626],\n",
       "  [0.47097624826839524, 0.8887352070535895, 2.4806667580651087],\n",
       "  [0.40492115686985175, 0.6275107476621999, 2.232263280977403],\n",
       "  [0.19804168436318043, 0.686712107322799, 2.638976259940046],\n",
       "  [0.3344691192321751, 0.7495825877985847, 2.463792663296052],\n",
       "  [0.7791289956773, 0.763844783379364, 2.364329017043897],\n",
       "  [0.6744615388027504, 0.7537324464153733, 2.379877255303748],\n",
       "  [0.13313691980877174, 0.7277098659056038, 2.6371344650691193],\n",
       "  [0.3676134197723151, 0.5871659419278774, 2.473786476941067],\n",
       "  [0.003457053167568211, 0.6807710174184235, 2.477746106554715],\n",
       "  [0.47634422717347635, 0.7024149111886389, 2.4120545807098988],\n",
       "  [0.8025168285567015, 0.7037092624535269, 2.1304446714082164],\n",
       "  [0.33592918665015215, 0.8807468496004673, 2.6025628244747585],\n",
       "  [0.6564603581388444, 0.5380019580331821, 2.438157315209812],\n",
       "  [0.0729244898098935, 0.7043239958622941, 2.4754379439739416],\n",
       "  [0.4133921191335984, 0.715989159891599, 2.507704547761129],\n",
       "  [0.6269815573288033, 0.9413903407405048, 2.538343206671231],\n",
       "  [0.32531784709608963, 0.7553388553383305, 2.369303739598663],\n",
       "  [0.45489208021571514, 0.4299350938579022, 2.7068310714794475],\n",
       "  [0.5037577814824614, 0.5406690201236075, 2.6797192363276325],\n",
       "  [0.734766910405864, 0.7122885799502358, 2.2925039962887332],\n",
       "  [0.4609838016240337, 0.967001070074387, 2.5783889129468007],\n",
       "  [0.3105032770756511, 0.3915672004689593, 2.636286700443381],\n",
       "  [0.12996190522122517, 0.8468536315163637, 2.4533030745223012],\n",
       "  [0.6903184929633949, 0.6728989234032585, 2.109852249280503],\n",
       "  [0.676306255705017, 0.7316677382885054, 2.498902716977565],\n",
       "  [0.6298633671861751, 0.6830547017548645, 2.3554802300654742],\n",
       "  [0.6704536355347406, 0.8456447156882578, 2.3667058553850233],\n",
       "  [0.457941183245048, 0.731876673854553, 2.480081257124662],\n",
       "  [0.3575586931935374, 0.48696322506164497, 2.284778202453444],\n",
       "  [0.48826820496121115, 0.5632526703199716, 2.183579265424431],\n",
       "  [0.49758778740679155, 0.75594063552498, 2.341580791212653],\n",
       "  [0.7206497185813034, 0.7388317137177514, 2.4152336583454797],\n",
       "  [0.2752468092834439, 0.6297357768710777, 2.3168982119905066],\n",
       "  [0.6484516990893687, 0.9386211578610709, 2.34744725416488],\n",
       "  [0.39757838633529874, 0.685914472230995, 2.5128424864830543],\n",
       "  [0.02127655414905654, 0.6577814540775728, 2.576905425585115],\n",
       "  [0.7218617745321266, 0.7402029195542368, 2.250006822676447],\n",
       "  [0.27336998152895076, 0.782886129327562, 2.212005916823145],\n",
       "  [0.7136033081401512, 0.7908742258351888, 2.5019602920503137],\n",
       "  [0.4790826041215973, 0.6965755112096575, 2.3752245730578507],\n",
       "  [0.35716067210812624, 0.779197522545255, 2.5502841626759487],\n",
       "  [0.6936279432985348, 0.7096329145109633, 2.398950735723036],\n",
       "  [0.6843423439352136, 0.6109720528540021, 2.191377959402322],\n",
       "  [0.6910442651816128, 0.9822254694786926, 2.460428027354297],\n",
       "  [0.3337154916140206, 0.6238165366162254, 2.003556170751073],\n",
       "  [0.48094248700055936, 0.8852121551349441, 2.555484684269827],\n",
       "  [0.048172515510966754, 0.8280553959158514, 2.5245911604420006],\n",
       "  [0.5817243365596659, 0.6053830272219102, 2.5931819810668477],\n",
       "  [0.2861454407136643, 0.8952451342695245, 2.6283404244489605],\n",
       "  [0.4241222555867611, 0.5742572685622227, 2.266378282266019],\n",
       "  [0.8251967187907199, 0.7891810524730205, 2.3925990679666542],\n",
       "  [0.7633469151520246, 0.9176294432050472, 2.247273930391836],\n",
       "  [0.5862870759962564, 0.482414890252318, 2.4565680349396013],\n",
       "  [0.527877388623962, 0.7217258443681562, 2.525052421350687],\n",
       "  [0.7060201866381923, 0.682424699989425, 2.0859507010487484],\n",
       "  [0.7632281790025199, 0.8215715632683434, 2.124081385806579],\n",
       "  [0.3249141751426, 0.4473300273031809, 2.5133382027875766],\n",
       "  [0.43822982780090813, 0.8033401320302828, 2.1829711557777043],\n",
       "  [0.3092093316448113, 0.8329216626953537, 2.5165855982621195],\n",
       "  [0.5087617764461833, 0.5726335947446974, 2.4931557753451985],\n",
       "  [0.3790675399783836, 0.6402069475240207, 2.516308873975537],\n",
       "  [0.41338684026068817, 0.5246023123337082, 2.622232480934472],\n",
       "  [0.46868705359742896, 0.8837303052952425, 2.4326708338010086],\n",
       "  [0.522201609027732, 0.7252201133713694, 2.480856573770952],\n",
       "  [0.8294364743238601, 0.7982454740601334, 2.207322531417973],\n",
       "  [0.23152145980932198, 0.8204991705794191, 2.3247459817577965],\n",
       "  [0.7627328623055558, 0.7817890621005773, 2.2410616907413936],\n",
       "  [0.5627968512095609, 0.6544208347769696, 2.3210952213930645],\n",
       "  [0.32443153970211264, 0.7768666009719822, 2.544593068781881],\n",
       "  [0.10807671830803928, 0.6268298948656524, 2.7246248931480417],\n",
       "  [0.3358700942298447, 0.8069475240206948, 2.684958346476917],\n",
       "  [0.5007773614796679, 0.6652733623719845, 2.5134290304925346],\n",
       "  [0.13321598482768854, 0.7868651625847549, 2.3928318870350447],\n",
       "  [0.049440117618469746, 0.5700675398727519, 2.7586018226861704],\n",
       "  [0.5006892386041564, 0.7639450229320573, 2.3767542052451818],\n",
       "  [0.5653113606756155, 0.881760389935254, 2.4348154075347956],\n",
       "  [0.3422570501261355, 0.5624386624039098, 2.4567012056478363],\n",
       "  [0.1755876045135944, 0.7332141691501618, 2.632249174860061],\n",
       "  [0.6403976908938009, 0.6605885408343144, 2.2833209860708794],\n",
       "  [0.5203655415495172, 0.624388232193283, 2.3871745673433398],\n",
       "  [0.23480904558208726, 0.6286542844602299, 2.5361818938716096],\n",
       "  [0.6553184444304438, 0.3941753768286608, 2.474240720062548],\n",
       "  [0.8871620815198342, 0.7262717760498035, 2.2195678068443865],\n",
       "  [0.2837406764211883, 0.6953491727056864, 2.4716564360380926],\n",
       "  [0.7638502580903173, 0.6651065725677097, 2.465889631180544]],\n",
       " 'lime': [[0.24131976435692098, 0.5182266009852217, 2.566785591274473],\n",
       "  [0.26026085551148126, 0.5392610837438422, 2.544957765005519],\n",
       "  [0.24438205214495062, 0.6099999999999999, 2.584273307184186],\n",
       "  [0.5860974090281467, 0.5084236453201969, 2.5918265165971164],\n",
       "  [0.32612498226029896, 0.39300492610837434, 2.583616717946241],\n",
       "  [0.07813816010400308, 0.5342857142857143, 2.5917965083596908],\n",
       "  [0.2273757333434126, 0.5089162561576354, 2.553944208142918],\n",
       "  [0.6952501625362772, 0.550246305418719, 2.5604985417799964],\n",
       "  [0.4604490593852064, 0.5235467980295565, 2.611501619953084],\n",
       "  [0.22825014252861603, 0.48610837438423626, 2.52948082939624],\n",
       "  [0.5290609536929081, 0.6218719211822659, 2.602920727591655],\n",
       "  [0.04513877351139291, 0.41394088669950724, 2.573136884784493],\n",
       "  [0.19248681705161827, 0.4655665024630541, 2.5846030391359656],\n",
       "  [0.5197035644307867, 0.13448275862068965, 2.517576186179361],\n",
       "  [0.3667986340159387, 0.47394088669950724, 2.627486417769765],\n",
       "  [0.536583363095758, 0.551231527093596, 2.6024830682134907],\n",
       "  [0.5089333089511172, 0.5083743842364531, 2.544977601332756],\n",
       "  [0.017479215384092098, 0.5945812807881773, 2.6211909692089996],\n",
       "  [0.33377273114180905, 0.5753694581280786, 2.52473045966569],\n",
       "  [0.413380704622255, 0.38660098522167474, 2.724200915886211],\n",
       "  [0.5261687662199818, 0.5237931034482758, 2.569557578622148],\n",
       "  [0.27623053909506023, 0.6045812807881773, 2.5785465510838232],\n",
       "  [0.5704065894963983, 0.15822660098522162, 2.584549012413987],\n",
       "  [0.022631706679080682, 0.6164039408866994, 2.591498668045793],\n",
       "  [0.46934573506281085, 0.4715763546798028, 2.457439385290073],\n",
       "  [0.6624387321868219, 0.4128571428571428, 2.544240365890946],\n",
       "  [0.08463055872347827, 0.5220689655172411, 2.551979266223303],\n",
       "  [0.6583989113645021, 0.43871921182266, 2.5980317343074355],\n",
       "  [0.4418039039934236, 0.6325615763546796, 2.578895334028552],\n",
       "  [0.4074578999542149, 0.5061576354679802, 2.588106278125181],\n",
       "  [0.6744703391635913, 0.53871921182266, 2.562693372680844],\n",
       "  [0.5285248359575301, 0.5995566502463053, 2.535496801341763],\n",
       "  [0.2807439666664554, 0.42073891625615756, 2.548963825426891],\n",
       "  [0.09457506592777778, 0.5633497536945812, 2.66165376202883],\n",
       "  [0.5945907542601798, 0.5392610837438424, 2.5246047137247047],\n",
       "  [0.047023373541345406, 0.4255172413793103, 2.540965009285334],\n",
       "  [0.41542518995096184, 0.44743842364532005, 2.526370723851395],\n",
       "  [0.3514568640137844, 0.4873891625615762, 2.5715454273924125],\n",
       "  [0.3654353458485865, 0.5748275862068964, 2.6781621827695057],\n",
       "  [0.7863964244861835, 0.556206896551724, 2.5255838662946437],\n",
       "  [0.4462613566682698, 0.4436453201970442, 2.5968825807718208],\n",
       "  [0.734380174708519, 0.5120689655172412, 2.6366516158086983],\n",
       "  [0.0823474228271365, 0.5144827586206896, 2.5509222055947567],\n",
       "  [0.13137687790678, 0.47201970443349744, 2.641269719533963],\n",
       "  [0.5427582164030036, 0.5230049261083743, 2.6347052152247348],\n",
       "  [0.16748853567045707, 0.5279802955665024, 2.60826162189079],\n",
       "  [0.614041906600596, 0.3600492610837438, 2.556396729926544],\n",
       "  [0.2550766590599739, 0.5485714285714284, 2.591279088796598],\n",
       "  [0.6116122385324211, 0.5761083743842363, 2.5993588062534294],\n",
       "  [0.358391861088898, 0.5361083743842363, 2.5420446872676252],\n",
       "  [0.23753396805746066, 0.36403940886699504, 2.627265194720239],\n",
       "  [0.5294238563652236, 0.4260098522167487, 2.5923553735294282],\n",
       "  [0.09065852850836154, 0.5566009852216748, 2.632003396601232],\n",
       "  [0.7138765160037495, 0.5200492610837437, 2.6490962539309932],\n",
       "  [0.16874614370010818, 0.4650246305418719, 2.582621875666692],\n",
       "  [0.30565510684682545, 0.4955665024630541, 2.576240257347122],\n",
       "  [0.47897647175728164, 0.4327586206896551, 2.640923219915053],\n",
       "  [0.426365786658777, 0.4246798029556649, 2.5885259636342024],\n",
       "  [0.6342542304628362, 0.4854187192118225, 2.5655627679283155],\n",
       "  [0.06615463583165129, 0.41044334975369445, 2.654481814094768],\n",
       "  [0.09933731537025041, 0.5711330049261082, 2.610579412525098],\n",
       "  [0.22208380471026687, 0.5979310344827585, 2.567931481444532],\n",
       "  [0.3325305965497863, 0.5425123152709358, 2.554964785914766],\n",
       "  [0.3163421923531584, 0.4138423645320196, 2.562649783680965],\n",
       "  [0.2804561656683784, 0.4925123152709358, 2.5884759545286853],\n",
       "  [0.7036294535428249, 0.5646798029556649, 2.609733616497516],\n",
       "  [0.6970953131377211, 0.5578817733990146, 2.559074878001898],\n",
       "  [0.668630317704526, 0.48965517241379286, 2.6857191142444576],\n",
       "  [0.28042401897513547, 0.5443842364532019, 2.615629353951555],\n",
       "  [0.6846898499049949, 0.6721674876847289, 2.5460973131431253],\n",
       "  [0.6949606062547072, 0.4573399014778324, 2.60547985252403],\n",
       "  [0.25312116221709624, 0.5896551724137928, 2.485454941605927],\n",
       "  [0.31610788265447753, 0.23615763546798024, 2.525518969779263],\n",
       "  [0.4443889426532645, 0.5202463054187192, 2.5782920045881257],\n",
       "  [0.47540817292997084, 0.5185714285714285, 2.5549238174014057],\n",
       "  [0.30496870539795506, 0.4310837438423644, 2.635140820965854],\n",
       "  [0.428836981647517, 0.45059113300492604, 2.5536495797097185],\n",
       "  [0.7166705806053077, 0.39600985221674867, 2.5841630350055516],\n",
       "  [0.5867248867499135, 0.440591133004926, 2.6199295226092567],\n",
       "  [0.6241186950310623, 0.4350738916256156, 2.553622083378206],\n",
       "  [0.40566549751032055, 0.45197044334975356, 2.4952886506964025],\n",
       "  [0.3324859912872089, 0.5670443349753693, 2.6345526550005376],\n",
       "  [0.6974704096926347, 0.4350246305418719, 2.5685638936881605],\n",
       "  [0.19243650251353228, 0.5054679802955664, 2.5192403002151655],\n",
       "  [0.01715688298376614, 0.2930049261083743, 2.561847876405538],\n",
       "  [0.34523044576287787, 0.4858128078817733, 2.6386514719746326],\n",
       "  [0.353461445778, 0.25133004926108365, 2.61973746877003],\n",
       "  [0.5585707715291786, 0.46157635467980285, 2.626966451092358],\n",
       "  [0.04637173731983765, 0.5743349753694581, 2.5820117800690645],\n",
       "  [0.6932697687328568, 0.5659605911330049, 2.5903292598419685],\n",
       "  [0.3391973956148836, 0.43719211822660087, 2.509603148768307],\n",
       "  [0.41142439594443136, 0.231231527093596, 2.621801523319824],\n",
       "  [0.3565838291866501, 0.4416748768472905, 2.5849550911799226],\n",
       "  [0.5286922753186254, 0.49443349753694577, 2.612696216463402],\n",
       "  [0.4021817789166331, 0.5058620689655172, 2.5958301092722382],\n",
       "  [0.18027069344812996, 0.46418719211822645, 2.5456244034691062],\n",
       "  [0.33195269419050366, 0.4961576354679802, 2.572279778674221],\n",
       "  [0.4124257269819188, 0.3844334975369457, 2.64889567649675],\n",
       "  [0.24939675027193436, 0.5247783251231526, 2.582312376061029],\n",
       "  [0.39927449955750705, 0.601822660098522, 2.6386336110653446]],\n",
       " 'anchor': [[0.07948311823472401, 0.8368527301718803, 0.6891874292301077],\n",
       "  [0.5311618021875795, 0.8828293989579933, 1.3958654319329693],\n",
       "  [0.37892450723647375, 0.9674196547821152, 1.2803986533404172],\n",
       "  [0.6829759160478723, 0.932445219229494, 0.6886267366357613],\n",
       "  [0.45814281605501145, 0.8706301205571334, 1.0066067178702325],\n",
       "  [0.05958167468853705, 0.9495343641158, 1.2036944116171655],\n",
       "  [0.03535270625219366, 0.8966815465572109, 1.2747360168939836],\n",
       "  [0.6061145029534495, 0.8415879332634877, 1.0935839846319686],\n",
       "  [0.5718127906823323, 0.8986678288442409, 0.692943322694429],\n",
       "  [0.3288684413837403, 0.9431803455140473, 1.5004647626812235],\n",
       "  [0.010946448386167089, 0.8924055197232466, 1.646193528092209],\n",
       "  [0.710644604829571, 0.8726928409523733, 1.0929019595891294],\n",
       "  [0.006043678539688818, 0.9155565240368674, 0.692943322694429],\n",
       "  [0.6084135801572639, 0.9343569663463779, 2.263649014417868],\n",
       "  [0.7411358267214254, 0.9268244469818955, 2.2703900101776875],\n",
       "  [0.23380330541936548, 0.9324452192294939, 0.6891874292301077],\n",
       "  [0.4663969633034909, 0.9219475680236613, 2.276168940998744],\n",
       "  [0.42674890343587946, 0.8839475174655451, 1.543777173465033],\n",
       "  [0.3205192358072234, 0.821490571590967, 1.4613331852459297],\n",
       "  [0.643031557600583, 0.9446806438458084, 2.1926335565056188],\n",
       "  [0.6262473589458786, 0.9505683362560007, 2.321897008554156],\n",
       "  [0.08465151265025399, 0.8627581215591263, 1.2887112417728446],\n",
       "  [0.5502123810604522, 0.9437450629105602, 2.0530751108523644],\n",
       "  [0.03401870762602731, 0.8454652056613915, 1.2271818927471387],\n",
       "  [0.4412551510845305, 0.8753653236487405, 1.0941031588821553],\n",
       "  [0.7957619348490226, 0.9377003369049353, 1.5462143563324509],\n",
       "  [0.016273313424444415, 0.8217664897235111, 1.0912070555476816],\n",
       "  [0.7093243198754532, 0.8770060076143871, 0.6872693456882963],\n",
       "  [0.22996952155149475, 0.9581277642144563, 2.6815113161793853],\n",
       "  [0.16958822298354637, 0.9155035024654057, 1.0589025542473018],\n",
       "  [0.32658516653355874, 0.9155565240368674, 0.6047673800580007],\n",
       "  [0.5764211578674031, 0.9634491515959658, 2.257559894186621],\n",
       "  [0.44815553233897265, 0.9435670641765688, 1.3007042184053834],\n",
       "  [0.27034499863186146, 0.9160065637985404, 1.5853473744736368],\n",
       "  [0.6682846340308217, 0.8550584828142442, 0.6891874292301077],\n",
       "  [0.1686608912441645, 0.8511025117789405, 1.1897789996530936],\n",
       "  [0.4645447033443013, 0.8986678288442409, 1.0935839846319686],\n",
       "  [0.1593701197184162, 0.9251200372815307, 1.6363751701954088],\n",
       "  [0.6191879745673623, 0.9324452192294939, 1.0452579296704747],\n",
       "  [0.5044542030627579, 0.932445219229494, 1.0119121974144343],\n",
       "  [0.12737604505393824, 0.8706301205571332, 0.679614826067882],\n",
       "  [0.7956332101377186, 0.9044075109423864, 1.087849723251832],\n",
       "  [0.1058279637245294, 0.8790685342851761, 1.0259024623586959],\n",
       "  [0.7020063021073473, 0.9703289063195614, 1.2963190058497147],\n",
       "  [0.6194856386327604, 0.8724669496393153, 1.0402151851520964],\n",
       "  [0.4566979656909438, 0.9795942489302358, 1.983735982105626],\n",
       "  [0.444486313533891, 0.9357782290977618, 1.8380379703195586],\n",
       "  [0.04493110859135125, 0.8654618455995658, 0.9536864103368818],\n",
       "  [0.1779869014262488, 0.8986678288442409, 1.0240441901083037],\n",
       "  [0.27109960646725834, 0.9042667939002529, 1.3295670238968116],\n",
       "  [0.38771956798282237, 0.834009472540133, 1.5338696916742969],\n",
       "  [0.6993720492415809, 0.8409138053587594, 0.6047673800580007],\n",
       "  [0.24488512084626696, 0.9240293562633208, 1.2037070903809164],\n",
       "  [0.712372677250547, 0.9155565240368674, 1.0589025542473018],\n",
       "  [0.09152530446077381, 0.827359596251396, 1.508940260440746],\n",
       "  [0.49199458262470935, 0.8935312097340077, 1.3233361568561597],\n",
       "  [0.6519846017804509, 0.9154637679573538, 2.085418640300234],\n",
       "  [0.5855603324660683, 0.9119534248185464, 2.3620395815641713],\n",
       "  [0.4521775697522284, 0.8986678288442409, 1.087849723251832],\n",
       "  [0.17005171305033143, 0.9220360515528976, 1.5644005911076073],\n",
       "  [0.4954109130462071, 0.9290208064893385, 1.272555793970693],\n",
       "  [0.27489734630231843, 0.8865695208590504, 1.3348455456193877],\n",
       "  [0.8111262069260685, 0.8689437398742689, 1.2744764587082023],\n",
       "  [0.4998935690966228, 0.846646762871004, 0.6827548931655778],\n",
       "  [0.2416203582832001, 0.8753653236487405, 1.0119121974144343],\n",
       "  [0.5680634899173094, 0.9155565240368674, 1.0452579296704747],\n",
       "  [0.7883147612917398, 0.8986678288442409, 0.6891874292301077],\n",
       "  [0.33195412173058764, 1.0, 0.6891874292301077],\n",
       "  [0.23064690727163742, 0.9040403460556066, 0.9865270820444966],\n",
       "  [0.4649658534112002, 0.8986678288442409, 1.0324989575779382],\n",
       "  [0.6357600966931848, 0.8480017432663611, 1.0410122513107338],\n",
       "  [0.22559188688045265, 0.9037899795509088, 1.50532914190818],\n",
       "  [0.47674055933197707, 0.9052319948646353, 1.5641779707569046],\n",
       "  [0.3578271132625276, 0.9811097846713125, 2.3027534352303887],\n",
       "  [0.21504698415934012, 0.8786630121569688, 1.5370711185807318],\n",
       "  [0.19868919529710927, 0.9525296071397935, 1.5898848825964758],\n",
       "  [0.14808233468497517, 0.9080081819027794, 1.3295670238968116],\n",
       "  [0.561994054562621, 0.8804575588122192, 2.0573671711487425],\n",
       "  [0.11681539473869605, 0.8013967328753611, 0.6872693456882963],\n",
       "  [0.7703370079175058, 0.9505600206019975, 2.0736287050003073],\n",
       "  [0.7459372997462685, 0.8816879181862666, 1.805727674096604],\n",
       "  [0.17381299519046817, 0.9746005849776781, 2.7641066183627254],\n",
       "  [0.46138532664550735, 0.8986678288442409, 0.692943322694429],\n",
       "  [0.4641964083602999, 0.8992040794321354, 1.33650595060364],\n",
       "  [0.03592505290284313, 0.8711119388959476, 1.4932753680849635],\n",
       "  [0.5314270029785544, 0.9270528305701227, 2.678450442252556],\n",
       "  [0.01078089963411058, 0.8319185232764683, 0.6876697193919352],\n",
       "  [0.3324555415021826, 0.9659230876861749, 2.732359845844766],\n",
       "  [0.17483800187356557, 0.9259186955165664, 1.2841271171276296],\n",
       "  [0.4051138547650149, 0.9044075109423864, 0.968331259465891],\n",
       "  [0.2647925918160291, 0.8994231595693238, 1.5207278027050484],\n",
       "  [0.523910299353533, 0.926637527601663, 2.343401020225543],\n",
       "  [0.19512405962509083, 0.8550584828142442, 1.0774177125151527],\n",
       "  [0.5446887210184452, 0.9052098509386761, 1.9011331556537576],\n",
       "  [0.5530194197832364, 0.8423788433681793, 0.9539044168050732],\n",
       "  [0.0829906528038275, 0.9362870570725486, 1.7135852268218879],\n",
       "  [0.6932271524925129, 0.8986678288442409, 1.01533819088369],\n",
       "  [0.2681810890092081, 0.9544307412151909, 2.2711201378903545],\n",
       "  [0.17022072410858413, 0.8894375810908739, 1.3925444776566902],\n",
       "  [0.5044129264819466, 0.8550584828142442, 1.0944384803858087]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_runs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle metric_runs\n",
    "with open('./pickles/experiments_org_metric_runs.pkl', 'wb') as f:\n",
    "    dill.dump(metric_runs1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shap': [[0.5723255128372227, 0.9681694998768169, 2.3930018139207707],\n",
       "  [0.4455079719457512, 0.7932906283588386, 2.5643626036115066],\n",
       "  [0.11866524032382038, 0.5698036691722417, 2.4865124394765457],\n",
       "  [0.3321909828950127, 0.5973491832952431, 2.267543704339637],\n",
       "  [0.3880845549517916, 0.651052978785058, 2.1921237565267475],\n",
       "  [0.045062567054379186, 0.7173494575917096, 2.4287078477545365],\n",
       "  [0.3533635648657668, 0.8600620161683944, 2.55966172035827],\n",
       "  [0.6466996145829886, 0.6259177137225918, 2.3808836586188544],\n",
       "  [0.38288184555107013, 0.7660893228293795, 2.5972896124354863],\n",
       "  [0.28946033910655344, 0.6925758450380424, 2.4304895479349486],\n",
       "  [0.4108290471891962, 0.6841527247710804, 2.539228234839168],\n",
       "  [0.4595011656489133, 0.7233812655654359, 2.7126702640784655],\n",
       "  [0.11344736925449553, 0.6884734997750285, 2.4962205489144815],\n",
       "  [0.912693063962245, 0.7772242998504544, 2.2466135681909547],\n",
       "  [0.5627141313984826, 0.8864549119769549, 2.4792100295543626],\n",
       "  [0.47097624826839524, 0.8887352070535895, 2.4806667580651087],\n",
       "  [0.40492115686985175, 0.6275107476621999, 2.232263280977403],\n",
       "  [0.19804168436318043, 0.686712107322799, 2.638976259940046],\n",
       "  [0.3344691192321751, 0.7495825877985847, 2.463792663296052],\n",
       "  [0.7791289956773, 0.763844783379364, 2.364329017043897],\n",
       "  [0.6744615388027504, 0.7537324464153733, 2.379877255303748],\n",
       "  [0.13313691980877174, 0.7277098659056038, 2.6371344650691193],\n",
       "  [0.3676134197723151, 0.5871659419278774, 2.473786476941067],\n",
       "  [0.003457053167568211, 0.6807710174184235, 2.477746106554715],\n",
       "  [0.47634422717347635, 0.7024149111886389, 2.4120545807098988],\n",
       "  [0.8025168285567015, 0.7037092624535269, 2.1304446714082164],\n",
       "  [0.33592918665015215, 0.8807468496004673, 2.6025628244747585],\n",
       "  [0.6564603581388444, 0.5380019580331821, 2.438157315209812],\n",
       "  [0.0729244898098935, 0.7043239958622941, 2.4754379439739416],\n",
       "  [0.4133921191335984, 0.715989159891599, 2.507704547761129],\n",
       "  [0.6269815573288033, 0.9413903407405048, 2.538343206671231],\n",
       "  [0.32531784709608963, 0.7553388553383305, 2.369303739598663],\n",
       "  [0.45489208021571514, 0.4299350938579022, 2.7068310714794475],\n",
       "  [0.5037577814824614, 0.5406690201236075, 2.6797192363276325],\n",
       "  [0.734766910405864, 0.7122885799502358, 2.2925039962887332],\n",
       "  [0.4609838016240337, 0.967001070074387, 2.5783889129468007],\n",
       "  [0.3105032770756511, 0.3915672004689593, 2.636286700443381],\n",
       "  [0.12996190522122517, 0.8468536315163637, 2.4533030745223012],\n",
       "  [0.6903184929633949, 0.6728989234032585, 2.109852249280503],\n",
       "  [0.676306255705017, 0.7316677382885054, 2.498902716977565],\n",
       "  [0.6298633671861751, 0.6830547017548645, 2.3554802300654742],\n",
       "  [0.6704536355347406, 0.8456447156882578, 2.3667058553850233],\n",
       "  [0.457941183245048, 0.731876673854553, 2.480081257124662],\n",
       "  [0.3575586931935374, 0.48696322506164497, 2.284778202453444],\n",
       "  [0.48826820496121115, 0.5632526703199716, 2.183579265424431],\n",
       "  [0.49758778740679155, 0.75594063552498, 2.341580791212653],\n",
       "  [0.7206497185813034, 0.7388317137177514, 2.4152336583454797],\n",
       "  [0.2752468092834439, 0.6297357768710777, 2.3168982119905066],\n",
       "  [0.6484516990893687, 0.9386211578610709, 2.34744725416488],\n",
       "  [0.39757838633529874, 0.685914472230995, 2.5128424864830543],\n",
       "  [0.02127655414905654, 0.6577814540775728, 2.576905425585115],\n",
       "  [0.7218617745321266, 0.7402029195542368, 2.250006822676447],\n",
       "  [0.27336998152895076, 0.782886129327562, 2.212005916823145],\n",
       "  [0.7136033081401512, 0.7908742258351888, 2.5019602920503137],\n",
       "  [0.4790826041215973, 0.6965755112096575, 2.3752245730578507],\n",
       "  [0.35716067210812624, 0.779197522545255, 2.5502841626759487],\n",
       "  [0.6936279432985348, 0.7096329145109633, 2.398950735723036],\n",
       "  [0.6843423439352136, 0.6109720528540021, 2.191377959402322],\n",
       "  [0.6910442651816128, 0.9822254694786926, 2.460428027354297],\n",
       "  [0.3337154916140206, 0.6238165366162254, 2.003556170751073],\n",
       "  [0.48094248700055936, 0.8852121551349441, 2.555484684269827],\n",
       "  [0.048172515510966754, 0.8280553959158514, 2.5245911604420006],\n",
       "  [0.5817243365596659, 0.6053830272219102, 2.5931819810668477],\n",
       "  [0.2861454407136643, 0.8952451342695245, 2.6283404244489605],\n",
       "  [0.4241222555867611, 0.5742572685622227, 2.266378282266019],\n",
       "  [0.8251967187907199, 0.7891810524730205, 2.3925990679666542],\n",
       "  [0.7633469151520246, 0.9176294432050472, 2.247273930391836],\n",
       "  [0.5862870759962564, 0.482414890252318, 2.4565680349396013],\n",
       "  [0.527877388623962, 0.7217258443681562, 2.525052421350687],\n",
       "  [0.7060201866381923, 0.682424699989425, 2.0859507010487484],\n",
       "  [0.7632281790025199, 0.8215715632683434, 2.124081385806579],\n",
       "  [0.3249141751426, 0.4473300273031809, 2.5133382027875766],\n",
       "  [0.43822982780090813, 0.8033401320302828, 2.1829711557777043],\n",
       "  [0.3092093316448113, 0.8329216626953537, 2.5165855982621195],\n",
       "  [0.5087617764461833, 0.5726335947446974, 2.4931557753451985],\n",
       "  [0.3790675399783836, 0.6402069475240207, 2.516308873975537],\n",
       "  [0.41338684026068817, 0.5246023123337082, 2.622232480934472],\n",
       "  [0.46868705359742896, 0.8837303052952425, 2.4326708338010086],\n",
       "  [0.522201609027732, 0.7252201133713694, 2.480856573770952],\n",
       "  [0.8294364743238601, 0.7982454740601334, 2.207322531417973],\n",
       "  [0.23152145980932198, 0.8204991705794191, 2.3247459817577965],\n",
       "  [0.7627328623055558, 0.7817890621005773, 2.2410616907413936],\n",
       "  [0.5627968512095609, 0.6544208347769696, 2.3210952213930645],\n",
       "  [0.32443153970211264, 0.7768666009719822, 2.544593068781881],\n",
       "  [0.10807671830803928, 0.6268298948656524, 2.7246248931480417],\n",
       "  [0.3358700942298447, 0.8069475240206948, 2.684958346476917],\n",
       "  [0.5007773614796679, 0.6652733623719845, 2.5134290304925346],\n",
       "  [0.13321598482768854, 0.7868651625847549, 2.3928318870350447],\n",
       "  [0.049440117618469746, 0.5700675398727519, 2.7586018226861704],\n",
       "  [0.5006892386041564, 0.7639450229320573, 2.3767542052451818],\n",
       "  [0.5653113606756155, 0.881760389935254, 2.4348154075347956],\n",
       "  [0.3422570501261355, 0.5624386624039098, 2.4567012056478363],\n",
       "  [0.1755876045135944, 0.7332141691501618, 2.632249174860061],\n",
       "  [0.6403976908938009, 0.6605885408343144, 2.2833209860708794],\n",
       "  [0.5203655415495172, 0.624388232193283, 2.3871745673433398],\n",
       "  [0.23480904558208726, 0.6286542844602299, 2.5361818938716096],\n",
       "  [0.6553184444304438, 0.3941753768286608, 2.474240720062548],\n",
       "  [0.8871620815198342, 0.7262717760498035, 2.2195678068443865],\n",
       "  [0.2837406764211883, 0.6953491727056864, 2.4716564360380926],\n",
       "  [0.7638502580903173, 0.6651065725677097, 2.465889631180544]],\n",
       " 'lime': [[0.24131976435692098, 0.5182266009852217, 2.566785591274473],\n",
       "  [0.26026085551148126, 0.5392610837438422, 2.544957765005519],\n",
       "  [0.24438205214495062, 0.6099999999999999, 2.584273307184186],\n",
       "  [0.5860974090281467, 0.5084236453201969, 2.5918265165971164],\n",
       "  [0.32612498226029896, 0.39300492610837434, 2.583616717946241],\n",
       "  [0.07813816010400308, 0.5342857142857143, 2.5917965083596908],\n",
       "  [0.2273757333434126, 0.5089162561576354, 2.553944208142918],\n",
       "  [0.6952501625362772, 0.550246305418719, 2.5604985417799964],\n",
       "  [0.4604490593852064, 0.5235467980295565, 2.611501619953084],\n",
       "  [0.22825014252861603, 0.48610837438423626, 2.52948082939624],\n",
       "  [0.5290609536929081, 0.6218719211822659, 2.602920727591655],\n",
       "  [0.04513877351139291, 0.41394088669950724, 2.573136884784493],\n",
       "  [0.19248681705161827, 0.4655665024630541, 2.5846030391359656],\n",
       "  [0.5197035644307867, 0.13448275862068965, 2.517576186179361],\n",
       "  [0.3667986340159387, 0.47394088669950724, 2.627486417769765],\n",
       "  [0.536583363095758, 0.551231527093596, 2.6024830682134907],\n",
       "  [0.5089333089511172, 0.5083743842364531, 2.544977601332756],\n",
       "  [0.017479215384092098, 0.5945812807881773, 2.6211909692089996],\n",
       "  [0.33377273114180905, 0.5753694581280786, 2.52473045966569],\n",
       "  [0.413380704622255, 0.38660098522167474, 2.724200915886211],\n",
       "  [0.5261687662199818, 0.5237931034482758, 2.569557578622148],\n",
       "  [0.27623053909506023, 0.6045812807881773, 2.5785465510838232],\n",
       "  [0.5704065894963983, 0.15822660098522162, 2.584549012413987],\n",
       "  [0.022631706679080682, 0.6164039408866994, 2.591498668045793],\n",
       "  [0.46934573506281085, 0.4715763546798028, 2.457439385290073],\n",
       "  [0.6624387321868219, 0.4128571428571428, 2.544240365890946],\n",
       "  [0.08463055872347827, 0.5220689655172411, 2.551979266223303],\n",
       "  [0.6583989113645021, 0.43871921182266, 2.5980317343074355],\n",
       "  [0.4418039039934236, 0.6325615763546796, 2.578895334028552],\n",
       "  [0.4074578999542149, 0.5061576354679802, 2.588106278125181],\n",
       "  [0.6744703391635913, 0.53871921182266, 2.562693372680844],\n",
       "  [0.5285248359575301, 0.5995566502463053, 2.535496801341763],\n",
       "  [0.2807439666664554, 0.42073891625615756, 2.548963825426891],\n",
       "  [0.09457506592777778, 0.5633497536945812, 2.66165376202883],\n",
       "  [0.5945907542601798, 0.5392610837438424, 2.5246047137247047],\n",
       "  [0.047023373541345406, 0.4255172413793103, 2.540965009285334],\n",
       "  [0.41542518995096184, 0.44743842364532005, 2.526370723851395],\n",
       "  [0.3514568640137844, 0.4873891625615762, 2.5715454273924125],\n",
       "  [0.3654353458485865, 0.5748275862068964, 2.6781621827695057],\n",
       "  [0.7863964244861835, 0.556206896551724, 2.5255838662946437],\n",
       "  [0.4462613566682698, 0.4436453201970442, 2.5968825807718208],\n",
       "  [0.734380174708519, 0.5120689655172412, 2.6366516158086983],\n",
       "  [0.0823474228271365, 0.5144827586206896, 2.5509222055947567],\n",
       "  [0.13137687790678, 0.47201970443349744, 2.641269719533963],\n",
       "  [0.5427582164030036, 0.5230049261083743, 2.6347052152247348],\n",
       "  [0.16748853567045707, 0.5279802955665024, 2.60826162189079],\n",
       "  [0.614041906600596, 0.3600492610837438, 2.556396729926544],\n",
       "  [0.2550766590599739, 0.5485714285714284, 2.591279088796598],\n",
       "  [0.6116122385324211, 0.5761083743842363, 2.5993588062534294],\n",
       "  [0.358391861088898, 0.5361083743842363, 2.5420446872676252],\n",
       "  [0.23753396805746066, 0.36403940886699504, 2.627265194720239],\n",
       "  [0.5294238563652236, 0.4260098522167487, 2.5923553735294282],\n",
       "  [0.09065852850836154, 0.5566009852216748, 2.632003396601232],\n",
       "  [0.7138765160037495, 0.5200492610837437, 2.6490962539309932],\n",
       "  [0.16874614370010818, 0.4650246305418719, 2.582621875666692],\n",
       "  [0.30565510684682545, 0.4955665024630541, 2.576240257347122],\n",
       "  [0.47897647175728164, 0.4327586206896551, 2.640923219915053],\n",
       "  [0.426365786658777, 0.4246798029556649, 2.5885259636342024],\n",
       "  [0.6342542304628362, 0.4854187192118225, 2.5655627679283155],\n",
       "  [0.06615463583165129, 0.41044334975369445, 2.654481814094768],\n",
       "  [0.09933731537025041, 0.5711330049261082, 2.610579412525098],\n",
       "  [0.22208380471026687, 0.5979310344827585, 2.567931481444532],\n",
       "  [0.3325305965497863, 0.5425123152709358, 2.554964785914766],\n",
       "  [0.3163421923531584, 0.4138423645320196, 2.562649783680965],\n",
       "  [0.2804561656683784, 0.4925123152709358, 2.5884759545286853],\n",
       "  [0.7036294535428249, 0.5646798029556649, 2.609733616497516],\n",
       "  [0.6970953131377211, 0.5578817733990146, 2.559074878001898],\n",
       "  [0.668630317704526, 0.48965517241379286, 2.6857191142444576],\n",
       "  [0.28042401897513547, 0.5443842364532019, 2.615629353951555],\n",
       "  [0.6846898499049949, 0.6721674876847289, 2.5460973131431253],\n",
       "  [0.6949606062547072, 0.4573399014778324, 2.60547985252403],\n",
       "  [0.25312116221709624, 0.5896551724137928, 2.485454941605927],\n",
       "  [0.31610788265447753, 0.23615763546798024, 2.525518969779263],\n",
       "  [0.4443889426532645, 0.5202463054187192, 2.5782920045881257],\n",
       "  [0.47540817292997084, 0.5185714285714285, 2.5549238174014057],\n",
       "  [0.30496870539795506, 0.4310837438423644, 2.635140820965854],\n",
       "  [0.428836981647517, 0.45059113300492604, 2.5536495797097185],\n",
       "  [0.7166705806053077, 0.39600985221674867, 2.5841630350055516],\n",
       "  [0.5867248867499135, 0.440591133004926, 2.6199295226092567],\n",
       "  [0.6241186950310623, 0.4350738916256156, 2.553622083378206],\n",
       "  [0.40566549751032055, 0.45197044334975356, 2.4952886506964025],\n",
       "  [0.3324859912872089, 0.5670443349753693, 2.6345526550005376],\n",
       "  [0.6974704096926347, 0.4350246305418719, 2.5685638936881605],\n",
       "  [0.19243650251353228, 0.5054679802955664, 2.5192403002151655],\n",
       "  [0.01715688298376614, 0.2930049261083743, 2.561847876405538],\n",
       "  [0.34523044576287787, 0.4858128078817733, 2.6386514719746326],\n",
       "  [0.353461445778, 0.25133004926108365, 2.61973746877003],\n",
       "  [0.5585707715291786, 0.46157635467980285, 2.626966451092358],\n",
       "  [0.04637173731983765, 0.5743349753694581, 2.5820117800690645],\n",
       "  [0.6932697687328568, 0.5659605911330049, 2.5903292598419685],\n",
       "  [0.3391973956148836, 0.43719211822660087, 2.509603148768307],\n",
       "  [0.41142439594443136, 0.231231527093596, 2.621801523319824],\n",
       "  [0.3565838291866501, 0.4416748768472905, 2.5849550911799226],\n",
       "  [0.5286922753186254, 0.49443349753694577, 2.612696216463402],\n",
       "  [0.4021817789166331, 0.5058620689655172, 2.5958301092722382],\n",
       "  [0.18027069344812996, 0.46418719211822645, 2.5456244034691062],\n",
       "  [0.33195269419050366, 0.4961576354679802, 2.572279778674221],\n",
       "  [0.4124257269819188, 0.3844334975369457, 2.64889567649675],\n",
       "  [0.24939675027193436, 0.5247783251231526, 2.582312376061029],\n",
       "  [0.39927449955750705, 0.601822660098522, 2.6386336110653446]],\n",
       " 'anchor': [[0.07948311823472401, 0.8368527301718803, 0.6891874292301077],\n",
       "  [0.5311618021875795, 0.8828293989579933, 1.3958654319329693],\n",
       "  [0.37892450723647375, 0.9674196547821152, 1.2803986533404172],\n",
       "  [0.6829759160478723, 0.932445219229494, 0.6886267366357613],\n",
       "  [0.45814281605501145, 0.8706301205571334, 1.0066067178702325],\n",
       "  [0.05958167468853705, 0.9495343641158, 1.2036944116171655],\n",
       "  [0.03535270625219366, 0.8966815465572109, 1.2747360168939836],\n",
       "  [0.6061145029534495, 0.8415879332634877, 1.0935839846319686],\n",
       "  [0.5718127906823323, 0.8986678288442409, 0.692943322694429],\n",
       "  [0.3288684413837403, 0.9431803455140473, 1.5004647626812235],\n",
       "  [0.010946448386167089, 0.8924055197232466, 1.646193528092209],\n",
       "  [0.710644604829571, 0.8726928409523733, 1.0929019595891294],\n",
       "  [0.006043678539688818, 0.9155565240368674, 0.692943322694429],\n",
       "  [0.6084135801572639, 0.9343569663463779, 2.263649014417868],\n",
       "  [0.7411358267214254, 0.9268244469818955, 2.2703900101776875],\n",
       "  [0.23380330541936548, 0.9324452192294939, 0.6891874292301077],\n",
       "  [0.4663969633034909, 0.9219475680236613, 2.276168940998744],\n",
       "  [0.42674890343587946, 0.8839475174655451, 1.543777173465033],\n",
       "  [0.3205192358072234, 0.821490571590967, 1.4613331852459297],\n",
       "  [0.643031557600583, 0.9446806438458084, 2.1926335565056188],\n",
       "  [0.6262473589458786, 0.9505683362560007, 2.321897008554156],\n",
       "  [0.08465151265025399, 0.8627581215591263, 1.2887112417728446],\n",
       "  [0.5502123810604522, 0.9437450629105602, 2.0530751108523644],\n",
       "  [0.03401870762602731, 0.8454652056613915, 1.2271818927471387],\n",
       "  [0.4412551510845305, 0.8753653236487405, 1.0941031588821553],\n",
       "  [0.7957619348490226, 0.9377003369049353, 1.5462143563324509],\n",
       "  [0.016273313424444415, 0.8217664897235111, 1.0912070555476816],\n",
       "  [0.7093243198754532, 0.8770060076143871, 0.6872693456882963],\n",
       "  [0.22996952155149475, 0.9581277642144563, 2.6815113161793853],\n",
       "  [0.16958822298354637, 0.9155035024654057, 1.0589025542473018],\n",
       "  [0.32658516653355874, 0.9155565240368674, 0.6047673800580007],\n",
       "  [0.5764211578674031, 0.9634491515959658, 2.257559894186621],\n",
       "  [0.44815553233897265, 0.9435670641765688, 1.3007042184053834],\n",
       "  [0.27034499863186146, 0.9160065637985404, 1.5853473744736368],\n",
       "  [0.6682846340308217, 0.8550584828142442, 0.6891874292301077],\n",
       "  [0.1686608912441645, 0.8511025117789405, 1.1897789996530936],\n",
       "  [0.4645447033443013, 0.8986678288442409, 1.0935839846319686],\n",
       "  [0.1593701197184162, 0.9251200372815307, 1.6363751701954088],\n",
       "  [0.6191879745673623, 0.9324452192294939, 1.0452579296704747],\n",
       "  [0.5044542030627579, 0.932445219229494, 1.0119121974144343],\n",
       "  [0.12737604505393824, 0.8706301205571332, 0.679614826067882],\n",
       "  [0.7956332101377186, 0.9044075109423864, 1.087849723251832],\n",
       "  [0.1058279637245294, 0.8790685342851761, 1.0259024623586959],\n",
       "  [0.7020063021073473, 0.9703289063195614, 1.2963190058497147],\n",
       "  [0.6194856386327604, 0.8724669496393153, 1.0402151851520964],\n",
       "  [0.4566979656909438, 0.9795942489302358, 1.983735982105626],\n",
       "  [0.444486313533891, 0.9357782290977618, 1.8380379703195586],\n",
       "  [0.04493110859135125, 0.8654618455995658, 0.9536864103368818],\n",
       "  [0.1779869014262488, 0.8986678288442409, 1.0240441901083037],\n",
       "  [0.27109960646725834, 0.9042667939002529, 1.3295670238968116],\n",
       "  [0.38771956798282237, 0.834009472540133, 1.5338696916742969],\n",
       "  [0.6993720492415809, 0.8409138053587594, 0.6047673800580007],\n",
       "  [0.24488512084626696, 0.9240293562633208, 1.2037070903809164],\n",
       "  [0.712372677250547, 0.9155565240368674, 1.0589025542473018],\n",
       "  [0.09152530446077381, 0.827359596251396, 1.508940260440746],\n",
       "  [0.49199458262470935, 0.8935312097340077, 1.3233361568561597],\n",
       "  [0.6519846017804509, 0.9154637679573538, 2.085418640300234],\n",
       "  [0.5855603324660683, 0.9119534248185464, 2.3620395815641713],\n",
       "  [0.4521775697522284, 0.8986678288442409, 1.087849723251832],\n",
       "  [0.17005171305033143, 0.9220360515528976, 1.5644005911076073],\n",
       "  [0.4954109130462071, 0.9290208064893385, 1.272555793970693],\n",
       "  [0.27489734630231843, 0.8865695208590504, 1.3348455456193877],\n",
       "  [0.8111262069260685, 0.8689437398742689, 1.2744764587082023],\n",
       "  [0.4998935690966228, 0.846646762871004, 0.6827548931655778],\n",
       "  [0.2416203582832001, 0.8753653236487405, 1.0119121974144343],\n",
       "  [0.5680634899173094, 0.9155565240368674, 1.0452579296704747],\n",
       "  [0.7883147612917398, 0.8986678288442409, 0.6891874292301077],\n",
       "  [0.33195412173058764, 1.0, 0.6891874292301077],\n",
       "  [0.23064690727163742, 0.9040403460556066, 0.9865270820444966],\n",
       "  [0.4649658534112002, 0.8986678288442409, 1.0324989575779382],\n",
       "  [0.6357600966931848, 0.8480017432663611, 1.0410122513107338],\n",
       "  [0.22559188688045265, 0.9037899795509088, 1.50532914190818],\n",
       "  [0.47674055933197707, 0.9052319948646353, 1.5641779707569046],\n",
       "  [0.3578271132625276, 0.9811097846713125, 2.3027534352303887],\n",
       "  [0.21504698415934012, 0.8786630121569688, 1.5370711185807318],\n",
       "  [0.19868919529710927, 0.9525296071397935, 1.5898848825964758],\n",
       "  [0.14808233468497517, 0.9080081819027794, 1.3295670238968116],\n",
       "  [0.561994054562621, 0.8804575588122192, 2.0573671711487425],\n",
       "  [0.11681539473869605, 0.8013967328753611, 0.6872693456882963],\n",
       "  [0.7703370079175058, 0.9505600206019975, 2.0736287050003073],\n",
       "  [0.7459372997462685, 0.8816879181862666, 1.805727674096604],\n",
       "  [0.17381299519046817, 0.9746005849776781, 2.7641066183627254],\n",
       "  [0.46138532664550735, 0.8986678288442409, 0.692943322694429],\n",
       "  [0.4641964083602999, 0.8992040794321354, 1.33650595060364],\n",
       "  [0.03592505290284313, 0.8711119388959476, 1.4932753680849635],\n",
       "  [0.5314270029785544, 0.9270528305701227, 2.678450442252556],\n",
       "  [0.01078089963411058, 0.8319185232764683, 0.6876697193919352],\n",
       "  [0.3324555415021826, 0.9659230876861749, 2.732359845844766],\n",
       "  [0.17483800187356557, 0.9259186955165664, 1.2841271171276296],\n",
       "  [0.4051138547650149, 0.9044075109423864, 0.968331259465891],\n",
       "  [0.2647925918160291, 0.8994231595693238, 1.5207278027050484],\n",
       "  [0.523910299353533, 0.926637527601663, 2.343401020225543],\n",
       "  [0.19512405962509083, 0.8550584828142442, 1.0774177125151527],\n",
       "  [0.5446887210184452, 0.9052098509386761, 1.9011331556537576],\n",
       "  [0.5530194197832364, 0.8423788433681793, 0.9539044168050732],\n",
       "  [0.0829906528038275, 0.9362870570725486, 1.7135852268218879],\n",
       "  [0.6932271524925129, 0.8986678288442409, 1.01533819088369],\n",
       "  [0.2681810890092081, 0.9544307412151909, 2.2711201378903545],\n",
       "  [0.17022072410858413, 0.8894375810908739, 1.3925444776566902],\n",
       "  [0.5044129264819466, 0.8550584828142442, 1.0944384803858087]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read pickle\n",
    "with open('./pickles/experiments_org_metric_runs.pkl', 'rb') as f:\n",
    "    metric_runs1 = dill.load(f)\n",
    "\n",
    "metric_runs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe from the metric_runs dictionary\n",
    "lime_metric_runs = pd.DataFrame(metric_runs1['lime'], columns=['idx', 'faithfulness', 'sensitivity', 'complexity'])\n",
    "shap_metric_runs = pd.DataFrame(metric_runs1['shap'], columns=['idx', 'faithfulness', 'sensitivity', 'complexity'])\n",
    "anchor_metric_runs = pd.DataFrame(metric_runs1['anchor'], columns=['idx', 'faithfulness', 'sensitivity', 'complexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.390486</td>\n",
       "      <td>0.486279</td>\n",
       "      <td>2.583746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.203001</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.044482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>2.457439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.248143</td>\n",
       "      <td>0.438337</td>\n",
       "      <td>2.553871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.383037</td>\n",
       "      <td>0.505665</td>\n",
       "      <td>2.583890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.538127</td>\n",
       "      <td>0.548990</td>\n",
       "      <td>2.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.786396</td>\n",
       "      <td>0.672167</td>\n",
       "      <td>2.724201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  sensitivity  complexity\n",
       "count    100.000000   100.000000  100.000000\n",
       "mean       0.390486     0.486279    2.583746\n",
       "std        0.203001     0.095099    0.044482\n",
       "min        0.017157     0.134483    2.457439\n",
       "25%        0.248143     0.438337    2.553871\n",
       "50%        0.383037     0.505665    2.583890\n",
       "75%        0.538127     0.548990    2.611800\n",
       "max        0.786396     0.672167    2.724201"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_metric_runs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.459490</td>\n",
       "      <td>0.713927</td>\n",
       "      <td>2.430877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.217598</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.156041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.003457</td>\n",
       "      <td>0.391567</td>\n",
       "      <td>2.003556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.325217</td>\n",
       "      <td>0.629465</td>\n",
       "      <td>2.337372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.460242</td>\n",
       "      <td>0.716669</td>\n",
       "      <td>2.458565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.647138</td>\n",
       "      <td>0.789604</td>\n",
       "      <td>2.527835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.912693</td>\n",
       "      <td>0.982225</td>\n",
       "      <td>2.758602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  sensitivity  complexity\n",
       "count    100.000000   100.000000  100.000000\n",
       "mean       0.459490     0.713927    2.430877\n",
       "std        0.217598     0.125869    0.156041\n",
       "min        0.003457     0.391567    2.003556\n",
       "25%        0.325217     0.629465    2.337372\n",
       "50%        0.460242     0.716669    2.458565\n",
       "75%        0.647138     0.789604    2.527835\n",
       "max        0.912693     0.982225    2.758602"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_metric_runs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.395468</td>\n",
       "      <td>0.902281</td>\n",
       "      <td>1.384464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.230227</td>\n",
       "      <td>0.041055</td>\n",
       "      <td>0.550394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.801397</td>\n",
       "      <td>0.604767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.190840</td>\n",
       "      <td>0.874697</td>\n",
       "      <td>1.025438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.442871</td>\n",
       "      <td>0.903915</td>\n",
       "      <td>1.282263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.572965</td>\n",
       "      <td>0.932445</td>\n",
       "      <td>1.601507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.811126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.764107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfulness  sensitivity  complexity\n",
       "count    100.000000   100.000000  100.000000\n",
       "mean       0.395468     0.902281    1.384464\n",
       "std        0.230227     0.041055    0.550394\n",
       "min        0.006044     0.801397    0.604767\n",
       "25%        0.190840     0.874697    1.025438\n",
       "50%        0.442871     0.903915    1.282263\n",
       "75%        0.572965     0.932445    1.601507\n",
       "max        0.811126     1.000000    2.764107"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_metric_runs.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
