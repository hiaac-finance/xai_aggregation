{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 18:50:38.342881: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-18 18:50:38.366852: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from data_loading import *\n",
    "\n",
    "from xai_agg.agg_exp import *\n",
    "from xai_agg.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7466666666666667\n",
      "ROC AUC: 0.5158014399393709\n"
     ]
    }
   ],
   "source": [
    "dataset_name, preprocessed_data, categorical_features, X, y, X_train, X_test, y_train, y_test, clf = load_pakdd2010_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1698 - val_loss: 1.0884\n",
      "Epoch 2/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 1.2200 - val_loss: 1.0398\n",
      "Epoch 3/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 1.1136 - val_loss: 0.9910\n",
      "Epoch 4/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 1.1491 - val_loss: 0.9464\n",
      "Epoch 5/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 1.0467 - val_loss: 0.9088\n",
      "Epoch 6/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.9962 - val_loss: 0.8787\n",
      "Epoch 7/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 1.0078 - val_loss: 0.8549\n",
      "Epoch 8/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 1.0518 - val_loss: 0.8365\n",
      "Epoch 9/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.9414 - val_loss: 0.8224\n",
      "Epoch 10/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 1.0205 - val_loss: 0.8117\n",
      "Epoch 11/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 1.0061 - val_loss: 0.8032\n",
      "Epoch 12/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.9602 - val_loss: 0.7961\n",
      "Epoch 13/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.8984 - val_loss: 0.7899\n",
      "Epoch 14/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.9303 - val_loss: 0.7847\n",
      "Epoch 15/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.8355 - val_loss: 0.7800\n",
      "Epoch 16/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.8679 - val_loss: 0.7755\n",
      "Epoch 17/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 0.9144 - val_loss: 0.7713\n",
      "Epoch 18/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.8640 - val_loss: 0.7671\n",
      "Epoch 19/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 1.0489 - val_loss: 0.7633\n",
      "Epoch 20/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.8177 - val_loss: 0.7595\n",
      "Epoch 21/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.8851 - val_loss: 0.7560\n",
      "Epoch 22/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.9144 - val_loss: 0.7527\n",
      "Epoch 23/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.8026 - val_loss: 0.7497\n",
      "Epoch 24/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.8038 - val_loss: 0.7469\n",
      "Epoch 25/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.9128 - val_loss: 0.7443\n",
      "Epoch 26/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.9482 - val_loss: 0.7419\n",
      "Epoch 27/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.8419 - val_loss: 0.7395\n",
      "Epoch 28/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.9278 - val_loss: 0.7372\n",
      "Epoch 29/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 0.8891 - val_loss: 0.7351\n",
      "Epoch 30/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.8114 - val_loss: 0.7329\n",
      "Epoch 31/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.8656 - val_loss: 0.7310\n",
      "Epoch 32/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.8568 - val_loss: 0.7291\n",
      "Epoch 33/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.8528 - val_loss: 0.7272\n",
      "Epoch 34/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 0.9394 - val_loss: 0.7256\n",
      "Epoch 35/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.8785 - val_loss: 0.7239\n",
      "Epoch 36/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.8172 - val_loss: 0.7223\n",
      "Epoch 37/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.8781 - val_loss: 0.7208\n",
      "Epoch 38/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.9686 - val_loss: 0.7193\n",
      "Epoch 39/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.8848 - val_loss: 0.7179\n",
      "Epoch 40/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.7959 - val_loss: 0.7164\n",
      "Epoch 41/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.8562 - val_loss: 0.7149\n",
      "Epoch 42/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 0.8487 - val_loss: 0.7135\n",
      "Epoch 43/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.8916 - val_loss: 0.7121\n",
      "Epoch 44/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.7832 - val_loss: 0.7107\n",
      "Epoch 45/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.8278 - val_loss: 0.7094\n",
      "Epoch 46/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.8053 - val_loss: 0.7081\n",
      "Epoch 47/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.8580 - val_loss: 0.7070\n",
      "Epoch 48/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.7878 - val_loss: 0.7058\n",
      "Epoch 49/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.8585 - val_loss: 0.7045\n",
      "Epoch 50/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 0.8552 - val_loss: 0.7035\n",
      "Epoch 51/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8414 - val_loss: 0.7024\n",
      "Epoch 52/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.8047 - val_loss: 0.7014\n",
      "Epoch 53/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.8160 - val_loss: 0.7004\n",
      "Epoch 54/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 0.8096 - val_loss: 0.6994\n",
      "Epoch 55/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.7786 - val_loss: 0.6985\n",
      "Epoch 56/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.9210 - val_loss: 0.6974\n",
      "Epoch 57/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.9315 - val_loss: 0.6965\n",
      "Epoch 58/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.7696 - val_loss: 0.6955\n",
      "Epoch 59/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.8373 - val_loss: 0.6945\n",
      "Epoch 60/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.9932 - val_loss: 0.6936\n",
      "Epoch 61/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.8525 - val_loss: 0.6927\n",
      "Epoch 62/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 0.7903 - val_loss: 0.6917\n",
      "Epoch 63/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.8351 - val_loss: 0.6907\n",
      "Epoch 64/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.9201 - val_loss: 0.6898\n",
      "Epoch 65/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.8485 - val_loss: 0.6890\n",
      "Epoch 66/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 0.7827 - val_loss: 0.6880\n",
      "Epoch 67/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.7049 - val_loss: 0.6872\n",
      "Epoch 68/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.7422 - val_loss: 0.6865\n",
      "Epoch 69/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.7739 - val_loss: 0.6858\n",
      "Epoch 70/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.7921 - val_loss: 0.6849\n",
      "Epoch 71/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.9520 - val_loss: 0.6843\n",
      "Epoch 72/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 0.8153 - val_loss: 0.6835\n",
      "Epoch 73/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8888 - val_loss: 0.6830\n",
      "Epoch 74/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.8164 - val_loss: 0.6823\n",
      "Epoch 75/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.7888 - val_loss: 0.6818\n",
      "Epoch 76/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.8748 - val_loss: 0.6813\n",
      "Epoch 77/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - loss: 0.9277 - val_loss: 0.6808\n",
      "Epoch 78/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.8950 - val_loss: 0.6802\n",
      "Epoch 79/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.7384 - val_loss: 0.6798\n",
      "Epoch 80/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.8699 - val_loss: 0.6792\n",
      "Epoch 81/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 0.8211 - val_loss: 0.6787\n",
      "Epoch 82/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.8156 - val_loss: 0.6782\n",
      "Epoch 83/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.9714 - val_loss: 0.6779\n",
      "Epoch 84/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.8883 - val_loss: 0.6774\n",
      "Epoch 85/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.7488 - val_loss: 0.6770\n",
      "Epoch 86/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.8530 - val_loss: 0.6765\n",
      "Epoch 87/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.8026 - val_loss: 0.6761\n",
      "Epoch 88/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.7666 - val_loss: 0.6757\n",
      "Epoch 89/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.8270 - val_loss: 0.6751\n",
      "Epoch 90/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.9239 - val_loss: 0.6748\n",
      "Epoch 91/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.7487 - val_loss: 0.6743\n",
      "Epoch 92/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.7531 - val_loss: 0.6739\n",
      "Epoch 93/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.7526 - val_loss: 0.6735\n",
      "Epoch 94/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8688 - val_loss: 0.6732\n",
      "Epoch 95/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8066 - val_loss: 0.6729\n",
      "Epoch 96/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.7819 - val_loss: 0.6725\n",
      "Epoch 97/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.8193 - val_loss: 0.6722\n",
      "Epoch 98/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.8363 - val_loss: 0.6718\n",
      "Epoch 99/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.7352 - val_loss: 0.6714\n",
      "Epoch 100/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.7224 - val_loss: 0.6711\n",
      "Epoch 101/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 1.0255 - val_loss: 0.6708\n",
      "Epoch 102/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.8140 - val_loss: 0.6705\n",
      "Epoch 103/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.7791 - val_loss: 0.6702\n",
      "Epoch 104/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.8134 - val_loss: 0.6699\n",
      "Epoch 105/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.8027 - val_loss: 0.6696\n",
      "Epoch 106/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 0.8556 - val_loss: 0.6693\n",
      "Epoch 107/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.9002 - val_loss: 0.6690\n",
      "Epoch 108/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.9162 - val_loss: 0.6688\n",
      "Epoch 109/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7418 - val_loss: 0.6684\n",
      "Epoch 110/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.8231 - val_loss: 0.6683\n",
      "Epoch 111/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 0.7660 - val_loss: 0.6680\n",
      "Epoch 112/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 1.0150 - val_loss: 0.6678\n",
      "Epoch 113/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.7454 - val_loss: 0.6676\n",
      "Epoch 114/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.7017 - val_loss: 0.6673\n",
      "Epoch 115/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.7560 - val_loss: 0.6671\n",
      "Epoch 116/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.8475 - val_loss: 0.6669\n",
      "Epoch 117/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.9575 - val_loss: 0.6666\n",
      "Epoch 118/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.7417 - val_loss: 0.6663\n",
      "Epoch 119/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.7170 - val_loss: 0.6663\n",
      "Epoch 120/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.8687 - val_loss: 0.6659\n",
      "Epoch 121/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.6850 - val_loss: 0.6658\n",
      "Epoch 122/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.7692 - val_loss: 0.6655\n",
      "Epoch 123/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.7827 - val_loss: 0.6653\n",
      "Epoch 124/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 0.7404 - val_loss: 0.6650\n",
      "Epoch 125/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.8162 - val_loss: 0.6648\n",
      "Epoch 126/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.7086 - val_loss: 0.6646\n",
      "Epoch 127/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 0.8476 - val_loss: 0.6644\n",
      "Epoch 128/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.8186 - val_loss: 0.6641\n",
      "Epoch 129/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.7518 - val_loss: 0.6641\n",
      "Epoch 130/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.7875 - val_loss: 0.6638\n",
      "Epoch 131/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 0.8286 - val_loss: 0.6636\n",
      "Epoch 132/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.9355 - val_loss: 0.6634\n",
      "Epoch 133/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.8302 - val_loss: 0.6631\n",
      "Epoch 134/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.8929 - val_loss: 0.6629\n",
      "Epoch 135/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.7919 - val_loss: 0.6628\n",
      "Epoch 136/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.8027 - val_loss: 0.6626\n",
      "Epoch 137/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.9869 - val_loss: 0.6623\n",
      "Epoch 138/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.7946 - val_loss: 0.6621\n",
      "Epoch 139/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.9007 - val_loss: 0.6619\n",
      "Epoch 140/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.8107 - val_loss: 0.6618\n",
      "Epoch 141/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.8296 - val_loss: 0.6616\n",
      "Epoch 142/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 0.8463 - val_loss: 0.6614\n",
      "Epoch 143/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 0.8791 - val_loss: 0.6611\n",
      "Epoch 144/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.8630 - val_loss: 0.6610\n",
      "Epoch 145/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.8185 - val_loss: 0.6609\n",
      "Epoch 146/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.8074 - val_loss: 0.6607\n",
      "Epoch 147/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.7228 - val_loss: 0.6605\n",
      "Epoch 148/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.8232 - val_loss: 0.6602\n",
      "Epoch 149/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.8003 - val_loss: 0.6601\n",
      "Epoch 150/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.8008 - val_loss: 0.6598\n",
      "Epoch 151/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.8002 - val_loss: 0.6596\n",
      "Epoch 152/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.7393 - val_loss: 0.6595\n",
      "Epoch 153/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.7856 - val_loss: 0.6594\n",
      "Epoch 154/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 0.8414 - val_loss: 0.6592\n",
      "Epoch 155/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 0.8238 - val_loss: 0.6590\n",
      "Epoch 156/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.7913 - val_loss: 0.6588\n",
      "Epoch 157/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.7497 - val_loss: 0.6587\n",
      "Epoch 158/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.7842 - val_loss: 0.6585\n",
      "Epoch 159/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 0.7163 - val_loss: 0.6582\n",
      "Epoch 160/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.8802 - val_loss: 0.6581\n",
      "Epoch 161/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.9157 - val_loss: 0.6579\n",
      "Epoch 162/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.8914 - val_loss: 0.6578\n",
      "Epoch 163/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7594 - val_loss: 0.6576\n",
      "Epoch 164/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.7735 - val_loss: 0.6575\n",
      "Epoch 165/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.9377 - val_loss: 0.6573\n",
      "Epoch 166/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.6969 - val_loss: 0.6572\n",
      "Epoch 167/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.7596 - val_loss: 0.6571\n",
      "Epoch 168/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.7350 - val_loss: 0.6568\n",
      "Epoch 169/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.7703 - val_loss: 0.6568\n",
      "Epoch 170/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.7673 - val_loss: 0.6566\n",
      "Epoch 171/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.9829 - val_loss: 0.6565\n",
      "Epoch 172/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.8978 - val_loss: 0.6565\n",
      "Epoch 173/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.7595 - val_loss: 0.6563\n",
      "Epoch 174/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.7177 - val_loss: 0.6561\n",
      "Epoch 175/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.7521 - val_loss: 0.6560\n",
      "Epoch 176/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.8920 - val_loss: 0.6559\n",
      "Epoch 177/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.8711 - val_loss: 0.6558\n",
      "Epoch 178/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.7949 - val_loss: 0.6556\n",
      "Epoch 179/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 0.7184 - val_loss: 0.6556\n",
      "Epoch 180/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.8579 - val_loss: 0.6553\n",
      "Epoch 181/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 0.7373 - val_loss: 0.6553\n",
      "Epoch 182/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.7122 - val_loss: 0.6551\n",
      "Epoch 183/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.8089 - val_loss: 0.6551\n",
      "Epoch 184/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.7528 - val_loss: 0.6549\n",
      "Epoch 185/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 0.7665 - val_loss: 0.6546\n",
      "Epoch 186/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 0.8440 - val_loss: 0.6544\n",
      "Epoch 187/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 0.7012 - val_loss: 0.6544\n",
      "Epoch 188/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 0.7450 - val_loss: 0.6542\n",
      "Epoch 189/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.7971 - val_loss: 0.6541\n",
      "Epoch 190/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.8761 - val_loss: 0.6539\n",
      "Epoch 191/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 0.7298 - val_loss: 0.6539\n",
      "Epoch 192/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.8440 - val_loss: 0.6538\n",
      "Epoch 193/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.8413 - val_loss: 0.6537\n",
      "Epoch 194/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 0.8059 - val_loss: 0.6536\n",
      "Epoch 195/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.7392 - val_loss: 0.6534\n",
      "Epoch 196/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.6699 - val_loss: 0.6533\n",
      "Epoch 197/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7331 - val_loss: 0.6532\n",
      "Epoch 198/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.7344 - val_loss: 0.6531\n",
      "Epoch 199/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.7981 - val_loss: 0.6530\n",
      "Epoch 200/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.8265 - val_loss: 0.6530\n",
      "Epoch 201/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.7945 - val_loss: 0.6529\n",
      "Epoch 202/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.8085 - val_loss: 0.6528\n",
      "Epoch 203/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.8265 - val_loss: 0.6527\n",
      "Epoch 204/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.7222 - val_loss: 0.6526\n",
      "Epoch 205/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.8130 - val_loss: 0.6524\n",
      "Epoch 206/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - loss: 0.7649 - val_loss: 0.6524\n",
      "Epoch 207/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.7214 - val_loss: 0.6524\n",
      "Epoch 208/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.8739 - val_loss: 0.6523\n",
      "Epoch 209/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.7829 - val_loss: 0.6522\n",
      "Epoch 210/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.7148 - val_loss: 0.6521\n",
      "Epoch 211/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.8111 - val_loss: 0.6520\n",
      "Epoch 212/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.7265 - val_loss: 0.6519\n",
      "Epoch 213/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.7768 - val_loss: 0.6520\n",
      "Epoch 214/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.8105 - val_loss: 0.6518\n",
      "Epoch 215/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 0.7559 - val_loss: 0.6518\n",
      "Epoch 216/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.7968 - val_loss: 0.6517\n",
      "Epoch 217/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.7180 - val_loss: 0.6516\n",
      "Epoch 218/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.6828 - val_loss: 0.6516\n",
      "Epoch 219/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.7907 - val_loss: 0.6515\n",
      "Epoch 220/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.8141 - val_loss: 0.6514\n",
      "Epoch 221/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.8061 - val_loss: 0.6513\n",
      "Epoch 222/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 0.7514 - val_loss: 0.6513\n",
      "Epoch 223/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.7638 - val_loss: 0.6512\n",
      "Epoch 224/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7953 - val_loss: 0.6512\n",
      "Epoch 225/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 0.7747 - val_loss: 0.6511\n",
      "Epoch 226/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.7075 - val_loss: 0.6511\n",
      "Epoch 227/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.7782 - val_loss: 0.6510\n",
      "Epoch 228/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.8043 - val_loss: 0.6510\n",
      "Epoch 229/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7589 - val_loss: 0.6510\n",
      "Epoch 230/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.7628 - val_loss: 0.6508\n",
      "Epoch 231/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7904 - val_loss: 0.6509\n",
      "Epoch 232/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.8407 - val_loss: 0.6507\n",
      "Epoch 233/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 0.7956 - val_loss: 0.6507\n",
      "Epoch 234/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.7219 - val_loss: 0.6506\n",
      "Epoch 235/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.7292 - val_loss: 0.6506\n",
      "Epoch 236/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.7819 - val_loss: 0.6505\n",
      "Epoch 237/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 0.6920 - val_loss: 0.6505\n",
      "Epoch 238/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.6895 - val_loss: 0.6504\n",
      "Epoch 239/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.8371 - val_loss: 0.6504\n",
      "Epoch 240/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.7834 - val_loss: 0.6503\n",
      "Epoch 241/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.8066 - val_loss: 0.6503\n",
      "Epoch 242/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.7982 - val_loss: 0.6503\n",
      "Epoch 243/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7818 - val_loss: 0.6501\n",
      "Epoch 244/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.8234 - val_loss: 0.6501\n",
      "Epoch 245/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 0.7528 - val_loss: 0.6501\n",
      "Epoch 246/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.7245 - val_loss: 0.6501\n",
      "Epoch 247/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.8495 - val_loss: 0.6499\n",
      "Epoch 248/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.7591 - val_loss: 0.6499\n",
      "Epoch 249/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.8076 - val_loss: 0.6499\n",
      "Epoch 250/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.7163 - val_loss: 0.6498\n",
      "Epoch 251/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 0.7074 - val_loss: 0.6497\n",
      "Epoch 252/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.7241 - val_loss: 0.6498\n",
      "Epoch 253/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 0.7350 - val_loss: 0.6497\n",
      "Epoch 254/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - loss: 0.7473 - val_loss: 0.6497\n",
      "Epoch 255/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.6746 - val_loss: 0.6496\n",
      "Epoch 256/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.9716 - val_loss: 0.6496\n",
      "Epoch 257/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.7638 - val_loss: 0.6495\n",
      "Epoch 258/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - loss: 0.8568 - val_loss: 0.6495\n",
      "Epoch 259/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.8409 - val_loss: 0.6494\n",
      "Epoch 260/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.8338 - val_loss: 0.6495\n",
      "Epoch 261/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.7448 - val_loss: 0.6494\n",
      "Epoch 262/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 0.6632 - val_loss: 0.6493\n",
      "Epoch 263/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 0.7855 - val_loss: 0.6493\n",
      "Epoch 264/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.8225 - val_loss: 0.6494\n",
      "Epoch 265/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.7151 - val_loss: 0.6492\n",
      "Epoch 266/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.8585 - val_loss: 0.6493\n",
      "Epoch 267/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.9178 - val_loss: 0.6492\n",
      "Epoch 268/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.7898 - val_loss: 0.6492\n",
      "Epoch 269/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.7605 - val_loss: 0.6491\n",
      "Epoch 270/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7961 - val_loss: 0.6491\n",
      "Epoch 271/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.7575 - val_loss: 0.6491\n",
      "Epoch 272/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.7825 - val_loss: 0.6491\n",
      "Epoch 273/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 0.6690 - val_loss: 0.6491\n",
      "Epoch 274/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 1.0327 - val_loss: 0.6489\n",
      "Epoch 275/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.8362 - val_loss: 0.6489\n",
      "Epoch 276/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.7821 - val_loss: 0.6490\n",
      "Epoch 277/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.8578 - val_loss: 0.6490\n",
      "Epoch 278/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 0.8453 - val_loss: 0.6489\n",
      "Epoch 279/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.7215 - val_loss: 0.6489\n",
      "Epoch 280/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.7824 - val_loss: 0.6489\n",
      "Epoch 281/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.8866 - val_loss: 0.6489\n",
      "Epoch 282/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.8214 - val_loss: 0.6488\n",
      "Epoch 283/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.8159 - val_loss: 0.6488\n",
      "Epoch 284/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.8224 - val_loss: 0.6488\n",
      "Epoch 285/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.7586 - val_loss: 0.6488\n",
      "Epoch 286/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.9058 - val_loss: 0.6487\n",
      "Epoch 287/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.6979 - val_loss: 0.6486\n",
      "Epoch 288/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.7273 - val_loss: 0.6487\n",
      "Epoch 289/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.8497 - val_loss: 0.6487\n",
      "Epoch 290/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.7625 - val_loss: 0.6487\n",
      "Epoch 291/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.8250 - val_loss: 0.6486\n",
      "Epoch 292/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.8781 - val_loss: 0.6486\n",
      "Epoch 293/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.7374 - val_loss: 0.6485\n",
      "Epoch 294/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.8380 - val_loss: 0.6486\n",
      "Epoch 295/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 0.7027 - val_loss: 0.6485\n",
      "Epoch 296/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.7458 - val_loss: 0.6484\n",
      "Epoch 297/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.7928 - val_loss: 0.6484\n",
      "Epoch 298/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.8075 - val_loss: 0.6484\n",
      "Epoch 299/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.8787 - val_loss: 0.6485\n",
      "Epoch 300/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 0.7159 - val_loss: 0.6484\n",
      "Epoch 301/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.7076 - val_loss: 0.6484\n",
      "Epoch 302/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.7313 - val_loss: 0.6483\n",
      "Epoch 303/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.7824 - val_loss: 0.6483\n",
      "Epoch 304/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.7636 - val_loss: 0.6484\n",
      "Epoch 305/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.7333 - val_loss: 0.6482\n",
      "Epoch 306/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.8970 - val_loss: 0.6481\n",
      "Epoch 307/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.7301 - val_loss: 0.6482\n",
      "Epoch 308/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 0.6850 - val_loss: 0.6481\n",
      "Epoch 309/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 0.7274 - val_loss: 0.6482\n",
      "Epoch 310/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.7784 - val_loss: 0.6482\n",
      "Epoch 311/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.7538 - val_loss: 0.6481\n",
      "Epoch 312/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 0.7598 - val_loss: 0.6480\n",
      "Epoch 313/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 0.7766 - val_loss: 0.6480\n",
      "Epoch 314/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.8546 - val_loss: 0.6480\n",
      "Epoch 315/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.6796 - val_loss: 0.6480\n",
      "Epoch 316/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 0.7941 - val_loss: 0.6479\n",
      "Epoch 317/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.7462 - val_loss: 0.6479\n",
      "Epoch 318/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.6907 - val_loss: 0.6479\n",
      "Epoch 319/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.6955 - val_loss: 0.6478\n",
      "Epoch 320/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.7442 - val_loss: 0.6480\n",
      "Epoch 321/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.8684 - val_loss: 0.6478\n",
      "Epoch 322/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7989 - val_loss: 0.6478\n",
      "Epoch 323/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.8528 - val_loss: 0.6479\n",
      "Epoch 324/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.7450 - val_loss: 0.6477\n",
      "Epoch 325/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.8167 - val_loss: 0.6478\n",
      "Epoch 326/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8109 - val_loss: 0.6477\n",
      "Epoch 327/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.7126 - val_loss: 0.6477\n",
      "Epoch 328/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.7362 - val_loss: 0.6477\n",
      "Epoch 329/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.7485 - val_loss: 0.6476\n",
      "Epoch 330/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.7125 - val_loss: 0.6477\n",
      "Epoch 331/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.6759 - val_loss: 0.6475\n",
      "Epoch 332/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.7793 - val_loss: 0.6475\n",
      "Epoch 333/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.7989 - val_loss: 0.6475\n",
      "Epoch 334/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7587 - val_loss: 0.6475\n",
      "Epoch 335/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 0.7519 - val_loss: 0.6474\n",
      "Epoch 336/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.7456 - val_loss: 0.6474\n",
      "Epoch 337/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 0.7729 - val_loss: 0.6475\n",
      "Epoch 338/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.9099 - val_loss: 0.6474\n",
      "Epoch 339/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.7802 - val_loss: 0.6474\n",
      "Epoch 340/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.8579 - val_loss: 0.6473\n",
      "Epoch 341/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.8418 - val_loss: 0.6473\n",
      "Epoch 342/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.7830 - val_loss: 0.6473\n",
      "Epoch 343/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - loss: 0.7790 - val_loss: 0.6473\n",
      "Epoch 344/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.7517 - val_loss: 0.6472\n",
      "Epoch 345/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 0.7532 - val_loss: 0.6472\n",
      "Epoch 346/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.8657 - val_loss: 0.6473\n",
      "Epoch 347/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.7481 - val_loss: 0.6471\n",
      "Epoch 348/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.6801 - val_loss: 0.6472\n",
      "Epoch 349/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - loss: 0.7072 - val_loss: 0.6471\n",
      "Epoch 350/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.7277 - val_loss: 0.6471\n",
      "Epoch 351/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.9047 - val_loss: 0.6471\n",
      "Epoch 352/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.7371 - val_loss: 0.6470\n",
      "Epoch 353/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.7313 - val_loss: 0.6469\n",
      "Epoch 354/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.7936 - val_loss: 0.6469\n",
      "Epoch 355/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 0.7333 - val_loss: 0.6470\n",
      "Epoch 356/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.7222 - val_loss: 0.6468\n",
      "Epoch 357/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.8129 - val_loss: 0.6469\n",
      "Epoch 358/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.6710 - val_loss: 0.6469\n",
      "Epoch 359/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.8404 - val_loss: 0.6469\n",
      "Epoch 360/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.7205 - val_loss: 0.6468\n",
      "Epoch 361/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.8564 - val_loss: 0.6467\n",
      "Epoch 362/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.8836 - val_loss: 0.6468\n",
      "Epoch 363/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.8154 - val_loss: 0.6468\n",
      "Epoch 364/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7803 - val_loss: 0.6467\n",
      "Epoch 365/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7348 - val_loss: 0.6467\n",
      "Epoch 366/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.7532 - val_loss: 0.6467\n",
      "Epoch 367/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.7464 - val_loss: 0.6467\n",
      "Epoch 368/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.6921 - val_loss: 0.6466\n",
      "Epoch 369/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 0.8215 - val_loss: 0.6466\n",
      "Epoch 370/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.7844 - val_loss: 0.6466\n",
      "Epoch 371/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7593 - val_loss: 0.6466\n",
      "Epoch 372/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 0.8458 - val_loss: 0.6466\n",
      "Epoch 373/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7735 - val_loss: 0.6466\n",
      "Epoch 374/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.7966 - val_loss: 0.6465\n",
      "Epoch 375/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.7389 - val_loss: 0.6464\n",
      "Epoch 376/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.6919 - val_loss: 0.6465\n",
      "Epoch 377/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.8756 - val_loss: 0.6465\n",
      "Epoch 378/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.8679 - val_loss: 0.6464\n",
      "Epoch 379/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.6338 - val_loss: 0.6464\n",
      "Epoch 380/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.6945 - val_loss: 0.6464\n",
      "Epoch 381/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.7766 - val_loss: 0.6464\n",
      "Epoch 382/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 0.7308 - val_loss: 0.6462\n",
      "Epoch 383/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.7090 - val_loss: 0.6464\n",
      "Epoch 384/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.7354 - val_loss: 0.6463\n",
      "Epoch 385/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - loss: 0.7254 - val_loss: 0.6464\n",
      "Epoch 386/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.8215 - val_loss: 0.6462\n",
      "Epoch 387/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7174 - val_loss: 0.6463\n",
      "Epoch 388/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.7492 - val_loss: 0.6463\n",
      "Epoch 389/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.7394 - val_loss: 0.6463\n",
      "Epoch 390/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.7779 - val_loss: 0.6464\n",
      "Epoch 391/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.7622 - val_loss: 0.6463\n",
      "Epoch 392/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 0.7375 - val_loss: 0.6462\n",
      "Epoch 393/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.7483 - val_loss: 0.6463\n",
      "Epoch 394/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 0.8067 - val_loss: 0.6464\n",
      "Epoch 395/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.8609 - val_loss: 0.6463\n",
      "Epoch 396/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.8166 - val_loss: 0.6462\n",
      "Epoch 397/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.7320 - val_loss: 0.6462\n",
      "Epoch 398/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7940 - val_loss: 0.6462\n",
      "Epoch 399/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.7371 - val_loss: 0.6463\n",
      "Epoch 400/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.7628 - val_loss: 0.6462\n",
      "Epoch 401/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.7070 - val_loss: 0.6463\n",
      "Epoch 402/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 0.7338 - val_loss: 0.6462\n",
      "Epoch 403/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.7359 - val_loss: 0.6461\n",
      "Epoch 404/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.6624 - val_loss: 0.6463\n",
      "Epoch 405/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 0.8185 - val_loss: 0.6462\n",
      "Epoch 406/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 0.8450 - val_loss: 0.6461\n",
      "Epoch 407/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.8710 - val_loss: 0.6461\n",
      "Epoch 408/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.6877 - val_loss: 0.6461\n",
      "Epoch 409/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.7450 - val_loss: 0.6460\n",
      "Epoch 410/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7538 - val_loss: 0.6461\n",
      "Epoch 411/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.7448 - val_loss: 0.6461\n",
      "Epoch 412/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.7632 - val_loss: 0.6460\n",
      "Epoch 413/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.7572 - val_loss: 0.6461\n",
      "Epoch 414/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.7461 - val_loss: 0.6460\n",
      "Epoch 415/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.7864 - val_loss: 0.6461\n",
      "Epoch 416/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.8426 - val_loss: 0.6461\n",
      "Epoch 417/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.8212 - val_loss: 0.6461\n",
      "Epoch 418/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.6812 - val_loss: 0.6459\n",
      "Epoch 419/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.7473 - val_loss: 0.6460\n",
      "Epoch 420/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.8118 - val_loss: 0.6460\n",
      "Epoch 421/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7314 - val_loss: 0.6459\n",
      "Epoch 422/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.8625 - val_loss: 0.6460\n",
      "Epoch 423/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.7575 - val_loss: 0.6461\n",
      "Epoch 424/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.7907 - val_loss: 0.6459\n",
      "Epoch 425/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.7501 - val_loss: 0.6459\n",
      "Epoch 426/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 0.7834 - val_loss: 0.6458\n",
      "Epoch 427/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 0.7174 - val_loss: 0.6458\n",
      "Epoch 428/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.7960 - val_loss: 0.6459\n",
      "Epoch 429/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.7723 - val_loss: 0.6458\n",
      "Epoch 430/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.7703 - val_loss: 0.6458\n",
      "Epoch 431/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.7831 - val_loss: 0.6458\n",
      "Epoch 432/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.7632 - val_loss: 0.6458\n",
      "Epoch 433/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.8595 - val_loss: 0.6458\n",
      "Epoch 434/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.8415 - val_loss: 0.6458\n",
      "Epoch 435/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.8834 - val_loss: 0.6458\n",
      "Epoch 436/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7571 - val_loss: 0.6459\n",
      "Epoch 437/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.7506 - val_loss: 0.6457\n",
      "Epoch 438/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.7852 - val_loss: 0.6457\n",
      "Epoch 439/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 0.8695 - val_loss: 0.6458\n",
      "Epoch 440/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.7496 - val_loss: 0.6456\n",
      "Epoch 441/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.7535 - val_loss: 0.6457\n",
      "Epoch 442/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.7843 - val_loss: 0.6456\n",
      "Epoch 443/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.7768 - val_loss: 0.6456\n",
      "Epoch 444/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.6863 - val_loss: 0.6456\n",
      "Epoch 445/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.6876 - val_loss: 0.6456\n",
      "Epoch 446/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7073 - val_loss: 0.6455\n",
      "Epoch 447/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.7297 - val_loss: 0.6457\n",
      "Epoch 448/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.8353 - val_loss: 0.6455\n",
      "Epoch 449/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.7386 - val_loss: 0.6455\n",
      "Epoch 450/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.8011 - val_loss: 0.6455\n",
      "Epoch 451/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 0.7954 - val_loss: 0.6455\n",
      "Epoch 452/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 0.8837 - val_loss: 0.6455\n",
      "Epoch 453/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.7467 - val_loss: 0.6455\n",
      "Epoch 454/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.7643 - val_loss: 0.6454\n",
      "Epoch 455/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.8067 - val_loss: 0.6455\n",
      "Epoch 456/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.7320 - val_loss: 0.6454\n",
      "Epoch 457/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 0.7302 - val_loss: 0.6454\n",
      "Epoch 458/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.7371 - val_loss: 0.6454\n",
      "Epoch 459/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.7717 - val_loss: 0.6455\n",
      "Epoch 460/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.7072 - val_loss: 0.6452\n",
      "Epoch 461/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.7991 - val_loss: 0.6453\n",
      "Epoch 462/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.7363 - val_loss: 0.6453\n",
      "Epoch 463/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 0.7658 - val_loss: 0.6454\n",
      "Epoch 464/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7691 - val_loss: 0.6453\n",
      "Epoch 465/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.8800 - val_loss: 0.6452\n",
      "Epoch 466/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.8794 - val_loss: 0.6453\n",
      "Epoch 467/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.8025 - val_loss: 0.6453\n",
      "Epoch 468/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.7676 - val_loss: 0.6452\n",
      "Epoch 469/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.8372 - val_loss: 0.6453\n",
      "Epoch 470/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.7600 - val_loss: 0.6453\n",
      "Epoch 471/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7689 - val_loss: 0.6452\n",
      "Epoch 472/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7874 - val_loss: 0.6451\n",
      "Epoch 473/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.7426 - val_loss: 0.6452\n",
      "Epoch 474/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.8048 - val_loss: 0.6453\n",
      "Epoch 475/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.7878 - val_loss: 0.6451\n",
      "Epoch 476/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 0.6984 - val_loss: 0.6451\n",
      "Epoch 477/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.8427 - val_loss: 0.6452\n",
      "Epoch 478/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.7488 - val_loss: 0.6450\n",
      "Epoch 479/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.6857 - val_loss: 0.6450\n",
      "Epoch 480/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.8288 - val_loss: 0.6449\n",
      "Epoch 481/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.7879 - val_loss: 0.6451\n",
      "Epoch 482/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.7768 - val_loss: 0.6450\n",
      "Epoch 483/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - loss: 0.7645 - val_loss: 0.6449\n",
      "Epoch 484/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.8388 - val_loss: 0.6449\n",
      "Epoch 485/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.8300 - val_loss: 0.6450\n",
      "Epoch 486/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.7054 - val_loss: 0.6450\n",
      "Epoch 487/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.7243 - val_loss: 0.6449\n",
      "Epoch 488/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.9280 - val_loss: 0.6449\n",
      "Epoch 489/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.8080 - val_loss: 0.6448\n",
      "Epoch 490/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.8323 - val_loss: 0.6448\n",
      "Epoch 491/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.8148 - val_loss: 0.6448\n",
      "Epoch 492/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - loss: 0.6792 - val_loss: 0.6448\n",
      "Epoch 493/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.8536 - val_loss: 0.6448\n",
      "Epoch 494/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.8373 - val_loss: 0.6447\n",
      "Epoch 495/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.9801 - val_loss: 0.6447\n",
      "Epoch 496/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 0.7483 - val_loss: 0.6446\n",
      "Epoch 497/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.9179 - val_loss: 0.6447\n",
      "Epoch 498/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.7709 - val_loss: 0.6446\n",
      "Epoch 499/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.8246 - val_loss: 0.6446\n",
      "Epoch 500/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.8101 - val_loss: 0.6445\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274us/step\n"
     ]
    }
   ],
   "source": [
    "evaluator = ExplanationModelEvaluator(clf, X_train, categorical_features)\n",
    "evaluator.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6798262974894057"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.faithfullness_correlation(ShapTabularTreeWrapper, X_test.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating metric vectors and calculating correlation between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a few reanknings\n",
    "exps = [LimeWrapper(clf, X_train, categorical_features), ShapTabularTreeWrapper(clf, X_train, categorical_features), AnchorWrapper(clf, X_train, categorical_features)]\n",
    "\n",
    "indexes = np.random.choice(X_test.index, 100, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "comp_arr = [] # original, new1, new2, ...\n",
    "\n",
    "def process_index(idx):\n",
    "    local_comp_arr = []\n",
    "    for exp in exps:\n",
    "        row = []\n",
    "        explanation = exp.explain_instance(X_test.loc[idx])\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=False, iterations=100, len_subset=1))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=False, iterations=100, len_subset=1))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"sum\", iterations=100, len_subset=1))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"percentile\", iterations=100, len_subset=1))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"avg\", iterations=100, len_subset=1))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"inverse\", iterations=100, len_subset=1))\n",
    "        \n",
    "        # row.append(evaluator.complexity(exp, X_test.loc[idx], explanation=explanation))\n",
    "        # row.append(evaluator.nrc(exp, X_test.loc[idx], explanation=explanation))\n",
    "        \n",
    "        local_comp_arr.append(row)\n",
    "    return local_comp_arr\n",
    "    \n",
    "with concurrent.futures.ProcessPoolExecutor(5) as executor:\n",
    "    results = list(executor.map(process_index, indexes))\n",
    "\n",
    "for result in results:\n",
    "    comp_arr.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.11853072900427095,\n",
       "  0.1363129402332447,\n",
       "  0.22931389854833853,\n",
       "  0.13767670846576102,\n",
       "  0.18690747596967347,\n",
       "  0.23892763743694703],\n",
       " [0.49614895884776533,\n",
       "  0.6370574852126564,\n",
       "  0.5242253048981342,\n",
       "  0.48144955150066454,\n",
       "  0.650587077373043,\n",
       "  0.2858447016662469],\n",
       " [0.24361441588445976,\n",
       "  0.2657370872385186,\n",
       "  0.060028358820672485,\n",
       "  0.03908218128636823,\n",
       "  0.039745791091461674,\n",
       "  0.10982513972477666],\n",
       " [0.5688897171793269,\n",
       "  0.5406401056495008,\n",
       "  0.20445491509763636,\n",
       "  0.2973614261818426,\n",
       "  0.2516792443105751,\n",
       "  0.49737282541520594],\n",
       " [0.19421995776279516,\n",
       "  0.23865531496465953,\n",
       "  0.54063228052811,\n",
       "  0.3225396873360568,\n",
       "  0.47197246990570396,\n",
       "  0.035562093058555],\n",
       " [0.4522015331777408,\n",
       "  0.6170251797739762,\n",
       "  0.3117029079183893,\n",
       "  0.36885231721707534,\n",
       "  0.3864003841725882,\n",
       "  0.06298818457351206],\n",
       " [0.5840135795125212,\n",
       "  0.5063126138245544,\n",
       "  0.5589343080776064,\n",
       "  0.481525880538293,\n",
       "  0.25727644488600054,\n",
       "  0.7273977862818175],\n",
       " [0.5628833069866699,\n",
       "  0.4709182680404334,\n",
       "  0.6377530611062153,\n",
       "  0.7397938535086382,\n",
       "  0.6103569237171849,\n",
       "  0.5625316759584669],\n",
       " [0.6286176352222922,\n",
       "  0.6478949375651022,\n",
       "  0.7490367628332629,\n",
       "  0.7136922294742454,\n",
       "  0.517380677635459,\n",
       "  0.6190201277593923],\n",
       " [0.477589774930386,\n",
       "  0.4606569390453239,\n",
       "  0.3376313177868014,\n",
       "  0.39644322553682143,\n",
       "  0.4484412446051722,\n",
       "  0.6762719694739276],\n",
       " [0.5073114275708895,\n",
       "  0.26554605090826366,\n",
       "  0.5583462111847507,\n",
       "  0.5113880537399375,\n",
       "  0.577444034740062,\n",
       "  0.250431595775765],\n",
       " [0.5504369468192877,\n",
       "  0.5527155149477523,\n",
       "  0.5476135762323786,\n",
       "  0.4867053750393663,\n",
       "  0.48273109923636065,\n",
       "  0.3261399446473491],\n",
       " [0.1831089897016503,\n",
       "  0.14264333866141912,\n",
       "  0.4107478070063647,\n",
       "  0.29960527215017585,\n",
       "  0.5185126798895863,\n",
       "  0.27530685211865547],\n",
       " [0.42040504800295964,\n",
       "  0.5562588967688155,\n",
       "  0.6442284403420468,\n",
       "  0.4142141554711604,\n",
       "  0.6137535560932232,\n",
       "  0.5712145021698265],\n",
       " [0.013901492213062708,\n",
       "  0.027595218995299577,\n",
       "  0.058525314146054386,\n",
       "  0.07942420267676191,\n",
       "  0.11813640407130716,\n",
       "  0.0781512545201095],\n",
       " [0.214497668934189,\n",
       "  0.3377447957826019,\n",
       "  0.5204773840599501,\n",
       "  0.35088848920722093,\n",
       "  0.3117649623133646,\n",
       "  0.19085724534325174],\n",
       " [0.37390942099949726,\n",
       "  0.3931061223059044,\n",
       "  0.5101031607663338,\n",
       "  0.5512100051490625,\n",
       "  0.5313178811147308,\n",
       "  0.26498605544108184],\n",
       " [0.020146093570934595,\n",
       "  0.058221404235563406,\n",
       "  0.05263797457994229,\n",
       "  0.04725926008544977,\n",
       "  0.06597709859993975,\n",
       "  0.020189665176164776],\n",
       " [0.011562624482537318,\n",
       "  0.01828496123551282,\n",
       "  0.3285169898596805,\n",
       "  0.19220939539386633,\n",
       "  0.18612626748161934,\n",
       "  0.027282964037516698],\n",
       " [0.7162229100367365,\n",
       "  0.4396887507989785,\n",
       "  0.6514092566281844,\n",
       "  0.5227932443208878,\n",
       "  0.5448495994968325,\n",
       "  0.35343588557001027],\n",
       " [0.6219118760676335,\n",
       "  0.48390836638659973,\n",
       "  0.5359603050587288,\n",
       "  0.346384682732438,\n",
       "  0.5173131761553881,\n",
       "  0.7727869186012197],\n",
       " [0.21300126068353537,\n",
       "  0.18881623936202432,\n",
       "  0.388807144348893,\n",
       "  0.3964420744332387,\n",
       "  0.33358331220512283,\n",
       "  0.3171665853475564],\n",
       " [0.30854999799334226,\n",
       "  0.1744861663879717,\n",
       "  0.10695134927045581,\n",
       "  0.18170844245871878,\n",
       "  0.36113013741425415,\n",
       "  0.1958492411172802],\n",
       " [0.05500406446030984,\n",
       "  0.14709669441693318,\n",
       "  0.03474928091630318,\n",
       "  0.19294909259971565,\n",
       "  0.1850314990044021,\n",
       "  0.0400326501234417],\n",
       " [0.46559286289983615,\n",
       "  0.5404164817803082,\n",
       "  0.42328611466552435,\n",
       "  0.48038203980156186,\n",
       "  0.4705160145782701,\n",
       "  0.5158981738586533],\n",
       " [0.6802480223263196,\n",
       "  0.6449907751785863,\n",
       "  0.7432856639887746,\n",
       "  0.6901058861728547,\n",
       "  0.6284897816339905,\n",
       "  0.7140582659933793],\n",
       " [0.3193353430956595,\n",
       "  0.38709526364208746,\n",
       "  0.2950035564206517,\n",
       "  0.3647638093770208,\n",
       "  0.37412016307355245,\n",
       "  0.5157964193332507],\n",
       " [0.07119346534325356,\n",
       "  0.04373008139863835,\n",
       "  0.30299340837653443,\n",
       "  0.4477943097638668,\n",
       "  0.2837751549886469,\n",
       "  0.10462554329514907],\n",
       " [0.17044287644018613,\n",
       "  0.2560375761674578,\n",
       "  0.6927702473816404,\n",
       "  0.7055332661069169,\n",
       "  0.5797389571725609,\n",
       "  0.4237145390215579],\n",
       " [0.4401211829442214,\n",
       "  0.4990816836410266,\n",
       "  0.2830554730931749,\n",
       "  0.3252719863871416,\n",
       "  0.29362697199901533,\n",
       "  0.24327325218321827],\n",
       " [0.42396821848976196,\n",
       "  0.47303497078023304,\n",
       "  0.5383717268990886,\n",
       "  0.3960131548688719,\n",
       "  0.4518497798246972,\n",
       "  0.43031093273492166],\n",
       " [0.7766819153851912,\n",
       "  0.7559177481807932,\n",
       "  0.5942750044026734,\n",
       "  0.6537472631959123,\n",
       "  0.5728013540909505,\n",
       "  0.6830935494896756],\n",
       " [0.22291634282331407,\n",
       "  0.14009030433136055,\n",
       "  0.20305050582669187,\n",
       "  0.17343085296820512,\n",
       "  0.24505369607988886,\n",
       "  0.14569265286647254],\n",
       " [0.21917898939891614,\n",
       "  0.3418400993540387,\n",
       "  0.26834981983994266,\n",
       "  0.3364921027256366,\n",
       "  0.29804751135017066,\n",
       "  0.15980845289627868],\n",
       " [0.7268162647838046,\n",
       "  0.8030366552174506,\n",
       "  0.6656361926416863,\n",
       "  0.6190470757648371,\n",
       "  0.6092225467889805,\n",
       "  0.6182348498924556],\n",
       " [0.08794400247374239,\n",
       "  0.2826102538307262,\n",
       "  0.12548027407673382,\n",
       "  0.15570575387764402,\n",
       "  0.4759703503314722,\n",
       "  0.13479265608320773],\n",
       " [0.5250175254037556,\n",
       "  0.3482085517849191,\n",
       "  0.30189006625612047,\n",
       "  0.4240074349602016,\n",
       "  0.4500087856336554,\n",
       "  0.4378076690165781],\n",
       " [0.5767187505830756,\n",
       "  0.3263406304073425,\n",
       "  0.3685354038611409,\n",
       "  0.5069759876403566,\n",
       "  0.36681573393212275,\n",
       "  0.45360280535864883],\n",
       " [0.37164232960565935,\n",
       "  0.6660370840040897,\n",
       "  0.47986321331891557,\n",
       "  0.5451355560386139,\n",
       "  0.6335396350751118,\n",
       "  0.6172839132731768],\n",
       " [0.9067802454877891,\n",
       "  0.9499221626536992,\n",
       "  0.46214640536355,\n",
       "  0.3830652112557195,\n",
       "  0.5548423985364251,\n",
       "  0.9034660641555446],\n",
       " [0.5099749285555655,\n",
       "  0.9863994453193666,\n",
       "  0.5350192153582213,\n",
       "  0.5294419120369726,\n",
       "  0.565985128943627,\n",
       "  0.9437789496115871],\n",
       " [0.6787359801670191,\n",
       "  0.8110682767282553,\n",
       "  0.8762315560712979,\n",
       "  0.7600029316342635,\n",
       "  0.7418489699989427,\n",
       "  0.9568763513035123],\n",
       " [0.3585453033650867,\n",
       "  0.5412011577629865,\n",
       "  0.4639757582345764,\n",
       "  0.6815327492828728,\n",
       "  0.29432478098105563,\n",
       "  0.3831383230848603],\n",
       " [0.48466280348078405,\n",
       "  0.45272457784470277,\n",
       "  0.49378699469275034,\n",
       "  0.6451088420452114,\n",
       "  0.43305584042488937,\n",
       "  0.4714126753299638],\n",
       " [0.36211221226061185,\n",
       "  0.193207671296888,\n",
       "  0.3367298269762551,\n",
       "  0.364757591985733,\n",
       "  0.25235770203205293,\n",
       "  0.366070120429554],\n",
       " [0.7972204266164711,\n",
       "  0.7164929313580191,\n",
       "  0.3830483245085904,\n",
       "  0.43355962871879306,\n",
       "  0.2879939732857254,\n",
       "  0.554917880382664],\n",
       " [0.5723680080221183,\n",
       "  0.699448539371349,\n",
       "  0.5706598507274041,\n",
       "  0.6651649958231416,\n",
       "  0.5914785571517633,\n",
       "  0.8198765462248229],\n",
       " [0.7303962427191575,\n",
       "  0.718188847778092,\n",
       "  0.8093964886385197,\n",
       "  0.7931931731985095,\n",
       "  0.7789426781691804,\n",
       "  0.8327996754899922],\n",
       " [0.5235115802317234,\n",
       "  0.5002897352324306,\n",
       "  0.6274950845904239,\n",
       "  0.4845787127740564,\n",
       "  0.6277042439505999,\n",
       "  0.5736950874859466],\n",
       " [0.6894843591292437,\n",
       "  0.6244191506650318,\n",
       "  0.587006547055887,\n",
       "  0.591128624611243,\n",
       "  0.513835585991884,\n",
       "  0.5688060677403675],\n",
       " [0.40511359739882824,\n",
       "  0.19103845812653192,\n",
       "  0.18393387372154646,\n",
       "  0.1449036590372094,\n",
       "  0.20849078222922657,\n",
       "  0.09769173672758899],\n",
       " [0.6129499317612535,\n",
       "  0.6418071999420523,\n",
       "  0.35527332030216535,\n",
       "  0.37834877436883985,\n",
       "  0.5415222026343975,\n",
       "  0.5833686372799664],\n",
       " [0.4632635705953202,\n",
       "  0.6968608232076916,\n",
       "  0.6608161312063081,\n",
       "  0.7872263039401484,\n",
       "  0.8033791058660033,\n",
       "  0.16520730850371476],\n",
       " [0.6064059184017666,\n",
       "  0.7053411442792508,\n",
       "  0.4139842946878804,\n",
       "  0.6060501448141756,\n",
       "  0.4940467710635349,\n",
       "  0.556095536690451],\n",
       " [0.10421763541381036,\n",
       "  0.06228778703528295,\n",
       "  0.31307900795103816,\n",
       "  0.3108421708019869,\n",
       "  0.4667240192987376,\n",
       "  0.025068481795761852],\n",
       " [0.6400251545552662,\n",
       "  0.5499812523462926,\n",
       "  0.4682083107844229,\n",
       "  0.5211701469195547,\n",
       "  0.46172434500334314,\n",
       "  0.48825237615961015],\n",
       " [0.3655517000654328,\n",
       "  0.13799705049215585,\n",
       "  0.7863190012507506,\n",
       "  0.3773121520514539,\n",
       "  0.4680240712889594,\n",
       "  0.5603928464157818],\n",
       " [0.10044534421799087,\n",
       "  0.07092264408619878,\n",
       "  0.3387347715693393,\n",
       "  0.32227655554717105,\n",
       "  0.256768981198804,\n",
       "  0.005635873398479079],\n",
       " [0.06724256529565174,\n",
       "  0.07409327933504074,\n",
       "  0.5127749794854836,\n",
       "  0.40496323855117233,\n",
       "  0.5762188358574323,\n",
       "  0.1263624789561046],\n",
       " [0.18308883341221965,\n",
       "  0.46310136295223836,\n",
       "  0.4647831997616031,\n",
       "  0.39765764170823387,\n",
       "  0.2040755128008337,\n",
       "  0.570076347774182],\n",
       " [0.8033105049027581,\n",
       "  0.7390387198026167,\n",
       "  0.705113883560271,\n",
       "  0.6305441133475986,\n",
       "  0.6248855166234032,\n",
       "  0.7514062411848068],\n",
       " [0.6785074809015053,\n",
       "  0.7240251540392667,\n",
       "  0.6873009124058806,\n",
       "  0.6445539479236183,\n",
       "  0.6419002544221215,\n",
       "  0.7482715476266594],\n",
       " [0.3328402088482536,\n",
       "  0.21431870305515072,\n",
       "  0.5716050806347133,\n",
       "  0.3391866472847241,\n",
       "  0.24028734205468388,\n",
       "  0.2080123594301777],\n",
       " [0.5783563563142893,\n",
       "  0.5368433571280388,\n",
       "  0.5449167528707921,\n",
       "  0.6838194427564768,\n",
       "  0.5780828243192299,\n",
       "  0.6425246475747156],\n",
       " [0.4872990087523352,\n",
       "  0.29630622659741246,\n",
       "  0.4906661279278542,\n",
       "  0.4112197875821767,\n",
       "  0.5152126762421443,\n",
       "  0.13009770438621227],\n",
       " [0.24437142073568402,\n",
       "  0.2287460468902721,\n",
       "  0.14829554953566737,\n",
       "  0.21308060187329653,\n",
       "  0.3435000557521653,\n",
       "  0.043747622167758016],\n",
       " [0.10012939769078341,\n",
       "  0.0539211733002392,\n",
       "  0.47832967357097433,\n",
       "  0.3887537388206953,\n",
       "  0.511077686404337,\n",
       "  0.3811079746699067],\n",
       " [0.7329462516618945,\n",
       "  0.8517222609163259,\n",
       "  0.7600551500723253,\n",
       "  0.6958978237296287,\n",
       "  0.8256512838710378,\n",
       "  0.19590271345681332],\n",
       " [0.3750934020333467,\n",
       "  0.4518145030982989,\n",
       "  0.3752726937127999,\n",
       "  0.03636042087095566,\n",
       "  0.34285851111343774,\n",
       "  0.33970986025557737],\n",
       " [0.7570041575509585,\n",
       "  0.8638386325330307,\n",
       "  0.5646448682424019,\n",
       "  0.5905574047698046,\n",
       "  0.6471046384300874,\n",
       "  0.6902880539700788],\n",
       " [0.7525681627077458,\n",
       "  0.8272076985172722,\n",
       "  0.5709289620604042,\n",
       "  0.5625754729523479,\n",
       "  0.6282470883024335,\n",
       "  0.7580233244903416],\n",
       " [0.38860996711151413,\n",
       "  0.44417708577492987,\n",
       "  0.48343867477280456,\n",
       "  0.45155396369711553,\n",
       "  0.4943453285051986,\n",
       "  0.3013576317156371],\n",
       " [0.045186301120746626,\n",
       "  0.09557131112937553,\n",
       "  0.34912052286450646,\n",
       "  0.33818095600898646,\n",
       "  0.3561068432464383,\n",
       "  0.04919421929191087],\n",
       " [0.3259712050603869,\n",
       "  0.4239553952436076,\n",
       "  0.44429034747115903,\n",
       "  0.6405132178331576,\n",
       "  0.5450144785566382,\n",
       "  0.10241806069607556],\n",
       " [0.3520216921400686,\n",
       "  0.1317600429576728,\n",
       "  0.2750895004707457,\n",
       "  0.3355100410021796,\n",
       "  0.32303560440072293,\n",
       "  0.00781626010576382],\n",
       " [0.46954979112167716,\n",
       "  0.5160618855595352,\n",
       "  0.46033170461858025,\n",
       "  0.13935859275876356,\n",
       "  0.2930883425641094,\n",
       "  0.44040647367591296],\n",
       " [0.8166854331331413,\n",
       "  0.6749264884923061,\n",
       "  0.7221914870122695,\n",
       "  0.7283060131332001,\n",
       "  0.7850988891373777,\n",
       "  0.7525884187245283],\n",
       " [0.7699451959901289,\n",
       "  0.7002148985279193,\n",
       "  0.7302675526003437,\n",
       "  0.77988002224334,\n",
       "  0.6783576892266663,\n",
       "  0.581943124325106],\n",
       " [0.6456139096891819,\n",
       "  0.6133846971593103,\n",
       "  0.7978231858668339,\n",
       "  0.7935023260009917,\n",
       "  0.7380640689273341,\n",
       "  0.7708683222831512],\n",
       " [0.3164854782944181,\n",
       "  0.2346375271411762,\n",
       "  0.5111034709489425,\n",
       "  0.5519611884403537,\n",
       "  0.7015055501993521,\n",
       "  0.32574645593788437],\n",
       " [0.1339463279047002,\n",
       "  0.05042781067942617,\n",
       "  0.02997649639984228,\n",
       "  0.011444050188907158,\n",
       "  0.19602109919952748,\n",
       "  0.10708934427028377],\n",
       " [0.23081610098312755,\n",
       "  0.03586569878209017,\n",
       "  0.48197592384971666,\n",
       "  0.3498511055191992,\n",
       "  0.3680286074911906,\n",
       "  0.20836428805658644],\n",
       " [0.4325009480160873,\n",
       "  0.616130310017743,\n",
       "  0.656808505322182,\n",
       "  0.6299647829244936,\n",
       "  0.6645887002726998,\n",
       "  0.3651250255166386],\n",
       " [0.6334868912217866,\n",
       "  0.7005397389497918,\n",
       "  0.6423296487687908,\n",
       "  0.6445279202997161,\n",
       "  0.6445173133432797,\n",
       "  0.42595021969847024],\n",
       " [0.9088399622804132,\n",
       "  0.7686889374438426,\n",
       "  0.3079341075324957,\n",
       "  0.6381196663264369,\n",
       "  0.5498343026243824,\n",
       "  0.7454040374571305],\n",
       " [0.8611967723004136,\n",
       "  0.5288151226260409,\n",
       "  0.700427792872024,\n",
       "  0.5188180256236263,\n",
       "  0.7267806576678394,\n",
       "  0.7940684337003403],\n",
       " [0.6709925944311473,\n",
       "  0.7480286925731127,\n",
       "  0.8683479016138884,\n",
       "  0.5004980265867195,\n",
       "  0.677105682136689,\n",
       "  0.8217997883180164],\n",
       " [0.776928296164961,\n",
       "  0.15341535115251936,\n",
       "  0.6305293379317731,\n",
       "  0.3542427301477324,\n",
       "  0.634155844449388,\n",
       "  0.755258515597294],\n",
       " [0.7452483162115806,\n",
       "  0.8629738358773527,\n",
       "  0.6522444473386443,\n",
       "  0.6085832068429051,\n",
       "  0.5832063838797441,\n",
       "  0.8951191043990614],\n",
       " [0.6376238269851562,\n",
       "  0.8050629576596615,\n",
       "  0.9226632131020785,\n",
       "  0.8957790637597727,\n",
       "  0.9266427910154048,\n",
       "  0.8334742502250727],\n",
       " [0.8136910268068426,\n",
       "  0.7639653860474198,\n",
       "  0.5407050349384002,\n",
       "  0.6928871790718434,\n",
       "  0.5540280328606695,\n",
       "  0.8191103636520998],\n",
       " [0.517571659471646,\n",
       "  0.524856018120111,\n",
       "  0.715994083356069,\n",
       "  0.6590193584641636,\n",
       "  0.6733279264595888,\n",
       "  0.6422675797601483],\n",
       " [0.4067444388913666,\n",
       "  0.39594083988952916,\n",
       "  0.3457097799403108,\n",
       "  0.4525797728844089,\n",
       "  0.22593750873059526,\n",
       "  0.5009789374214018],\n",
       " [0.7705761955965766,\n",
       "  0.46373761413115944,\n",
       "  0.4298084688078363,\n",
       "  0.4383182256262597,\n",
       "  0.5887804777074058,\n",
       "  0.5606096704349399],\n",
       " [0.5232583854032684,\n",
       "  0.5182267203077616,\n",
       "  0.6154177993163885,\n",
       "  0.5995555322497871,\n",
       "  0.6156269650831852,\n",
       "  0.14041849913899557],\n",
       " [0.6666923805675633,\n",
       "  0.433317181704052,\n",
       "  0.7232216048943343,\n",
       "  0.426576475728791,\n",
       "  0.2141990114050186,\n",
       "  0.232868261866245],\n",
       " [0.8038342693464454,\n",
       "  0.9134969399577189,\n",
       "  0.6079546495520426,\n",
       "  0.5846755472453469,\n",
       "  0.6267917429210519,\n",
       "  0.8600511268570509],\n",
       " [0.6423638772945355,\n",
       "  0.8374787862075014,\n",
       "  0.6450743105212958,\n",
       "  0.6312260864563903,\n",
       "  0.6136456324846389,\n",
       "  0.36542754106157294],\n",
       " [0.5096078966978501,\n",
       "  0.4504294336014416,\n",
       "  0.4352823361658491,\n",
       "  0.5559166816579487,\n",
       "  0.5982560287626403,\n",
       "  0.42906264693504303],\n",
       " [0.53229373314359,\n",
       "  0.330393567577858,\n",
       "  0.40723700484744135,\n",
       "  0.486816694665018,\n",
       "  0.3690896625866522,\n",
       "  0.39897716976980646],\n",
       " [0.7334248392322599,\n",
       "  0.6659232222987982,\n",
       "  0.7948039262580209,\n",
       "  0.7810964089611235,\n",
       "  0.8127830294893377,\n",
       "  0.5923905391307245],\n",
       " [0.39092631385076937,\n",
       "  0.18431477352685402,\n",
       "  0.279386621413074,\n",
       "  0.4260535574691171,\n",
       "  0.2916335960018276,\n",
       "  0.3414647616301528],\n",
       " [0.8335842704482114,\n",
       "  0.9016346396553643,\n",
       "  0.4125903867553955,\n",
       "  0.5523843470616915,\n",
       "  0.5185681859155236,\n",
       "  0.7636928761342923],\n",
       " [0.8991138743438983,\n",
       "  0.9035397307345603,\n",
       "  0.7715750813958703,\n",
       "  0.8426066326232668,\n",
       "  0.7826789865149941,\n",
       "  0.8753562201311627],\n",
       " [0.6874406068974391,\n",
       "  0.6765034946535446,\n",
       "  0.5981753351172328,\n",
       "  0.7922784049555252,\n",
       "  0.5989048531210677,\n",
       "  0.7843506602945303],\n",
       " [0.1718590212589761,\n",
       "  0.17717589265680106,\n",
       "  0.40537182149987633,\n",
       "  0.26779775554110924,\n",
       "  0.23625518423945335,\n",
       "  0.2826831808373352],\n",
       " [0.21467744063993774,\n",
       "  0.1458250696779727,\n",
       "  0.4363013052106771,\n",
       "  0.425636603512768,\n",
       "  0.4856636439506384,\n",
       "  0.38783335389848783],\n",
       " [0.15077776044916505,\n",
       "  0.07557024026689677,\n",
       "  0.009967845150947199,\n",
       "  0.06237786434370521,\n",
       "  0.029571307136199813,\n",
       "  0.08890074340138555],\n",
       " [0.0814873732760496,\n",
       "  0.1299393144274949,\n",
       "  0.5667644640218347,\n",
       "  0.534267712484342,\n",
       "  0.6212094378295101,\n",
       "  0.12291096719254087],\n",
       " [0.7279638762627133,\n",
       "  0.5929989989357076,\n",
       "  0.6576939101078698,\n",
       "  0.7044114415992098,\n",
       "  0.7245118594007052,\n",
       "  0.5744747437261848],\n",
       " [0.14422432334862073,\n",
       "  0.19475064773136427,\n",
       "  0.40275492175318256,\n",
       "  0.24338241483599804,\n",
       "  0.278692723159215,\n",
       "  0.285720119818916],\n",
       " [0.47248645394495903,\n",
       "  0.45000360419696184,\n",
       "  0.5394511210797881,\n",
       "  0.37744308152326755,\n",
       "  0.35735486820235896,\n",
       "  0.5595560848000484],\n",
       " [0.6281563494177167,\n",
       "  0.7839993172927022,\n",
       "  0.5065745482519723,\n",
       "  0.5620045385480589,\n",
       "  0.5472498545480539,\n",
       "  0.5473499173109648],\n",
       " [0.2882767707257009,\n",
       "  0.10589141664574526,\n",
       "  0.20336069360725,\n",
       "  0.22696060814168734,\n",
       "  0.20117370538735507,\n",
       "  0.08173269995139054],\n",
       " [0.3426628241948339,\n",
       "  0.41836454752822405,\n",
       "  0.31703539575275097,\n",
       "  0.4762925947374521,\n",
       "  0.5031802657522222,\n",
       "  0.48709610160795835],\n",
       " [0.9601410559230843,\n",
       "  0.8866550427457589,\n",
       "  0.8953691036541891,\n",
       "  0.8796252731831347,\n",
       "  0.8877735614874673,\n",
       "  0.8773777386024231],\n",
       " [0.35266073603354264,\n",
       "  0.4269032191318816,\n",
       "  0.286873342524527,\n",
       "  0.14058347932446508,\n",
       "  0.3834170632573449,\n",
       "  0.12345667720571316],\n",
       " [0.9018833976143821,\n",
       "  0.8339858630123616,\n",
       "  0.6097776465352094,\n",
       "  0.6610443607379295,\n",
       "  0.5831272728306904,\n",
       "  0.8333220162046151],\n",
       " [0.30492443260518903,\n",
       "  0.24608724974201057,\n",
       "  0.5843177899456647,\n",
       "  0.5915578554125701,\n",
       "  0.5933390705150132,\n",
       "  0.6207421753805036],\n",
       " [0.5170344917842431,\n",
       "  0.5719842108345589,\n",
       "  0.5393348801319335,\n",
       "  0.4519616392060122,\n",
       "  0.3796247414675893,\n",
       "  0.12418863883852604],\n",
       " [0.05529448867165324,\n",
       "  0.04606357129521485,\n",
       "  0.3346263770674283,\n",
       "  0.3297853679108268,\n",
       "  0.37583799951837404,\n",
       "  0.00931730701883195],\n",
       " [0.6081455711343139,\n",
       "  0.6590894299583483,\n",
       "  0.43451501387766545,\n",
       "  0.5067009190599162,\n",
       "  0.42201944454724694,\n",
       "  0.38902369137127923],\n",
       " [0.35687156483424487,\n",
       "  0.4473623410544463,\n",
       "  0.6595390732838279,\n",
       "  0.36923723329829783,\n",
       "  0.5274617291923678,\n",
       "  0.35541083001884133],\n",
       " [0.1358766026794458,\n",
       "  0.07712037281225873,\n",
       "  0.4331621056763666,\n",
       "  0.2726862001669806,\n",
       "  0.3664158584804865,\n",
       "  0.1336347213736621],\n",
       " [0.2328382622066544,\n",
       "  0.217593653211418,\n",
       "  0.4865371291789571,\n",
       "  0.4087340947198183,\n",
       "  0.3631504956143835,\n",
       "  0.23373289515833245],\n",
       " [0.4773865271340806,\n",
       "  0.37402954056033766,\n",
       "  0.47558042708540205,\n",
       "  0.3633835221866458,\n",
       "  0.3349492136437267,\n",
       "  0.3883647306149288],\n",
       " [0.8195440496876769,\n",
       "  0.6064877780754603,\n",
       "  0.5207568423504229,\n",
       "  0.6255775943652088,\n",
       "  0.6887967639330492,\n",
       "  0.7169744048091022],\n",
       " [0.781356716311401,\n",
       "  0.8042074440234832,\n",
       "  0.6447041940463691,\n",
       "  0.6599408045617673,\n",
       "  0.5914495803614948,\n",
       "  0.8148707306225051],\n",
       " [0.34732806468340915,\n",
       "  0.41228711526885614,\n",
       "  0.590475157800602,\n",
       "  0.5496650067438784,\n",
       "  0.5419100229566021,\n",
       "  0.016288236841856457],\n",
       " [0.701077087646629,\n",
       "  0.6910147234826829,\n",
       "  0.3084164581128247,\n",
       "  0.521820860990305,\n",
       "  0.5871290382827724,\n",
       "  0.7736055234523722],\n",
       " [0.23672732344649933,\n",
       "  0.40976959166283744,\n",
       "  0.4989668446312413,\n",
       "  0.5757743955708243,\n",
       "  0.5300924888593282,\n",
       "  0.611735663830092],\n",
       " [0.5244959034241823,\n",
       "  0.6199516779652594,\n",
       "  0.49530837810009093,\n",
       "  0.503533963810098,\n",
       "  0.7405052400162728,\n",
       "  0.4121292147260762],\n",
       " [0.9609748499478914,\n",
       "  0.7477806285624466,\n",
       "  0.7231862063511364,\n",
       "  0.661856914843915,\n",
       "  0.5980255308396271,\n",
       "  0.9233227787971616],\n",
       " [0.7314695647372178,\n",
       "  0.8483344343794768,\n",
       "  0.6795374633322938,\n",
       "  0.6384661003403256,\n",
       "  0.7525993579605617,\n",
       "  0.8301108043510321],\n",
       " [0.8945056008480207,\n",
       "  0.6641657276545452,\n",
       "  0.8819663530721268,\n",
       "  0.5732756148552337,\n",
       "  0.7462534688329356,\n",
       "  0.7542411391403251],\n",
       " [0.868213234919686,\n",
       "  0.8981334409341306,\n",
       "  0.7102823911259991,\n",
       "  0.6107417037837554,\n",
       "  0.68674394929067,\n",
       "  0.8382754778028699],\n",
       " [0.8350620135148922,\n",
       "  0.8939845403245545,\n",
       "  0.7515949336614502,\n",
       "  0.824025407893355,\n",
       "  0.6765888079273027,\n",
       "  0.9475271277676773],\n",
       " [0.6287085989589966,\n",
       "  0.3902475775163071,\n",
       "  0.5123997449472548,\n",
       "  0.8168988015095586,\n",
       "  0.6766973551207303,\n",
       "  0.7304698199122664],\n",
       " [0.8484080817523454,\n",
       "  0.6968593233047854,\n",
       "  0.5426270544877252,\n",
       "  0.6629001781079329,\n",
       "  0.5748550608597242,\n",
       "  0.837443698408477],\n",
       " [0.6181942602856945,\n",
       "  0.5613455471806896,\n",
       "  0.5715469841394764,\n",
       "  0.6317708772394848,\n",
       "  0.540106030314887,\n",
       "  0.8614873817617859],\n",
       " [0.6618803891486273,\n",
       "  0.30204319667334145,\n",
       "  0.364587714620718,\n",
       "  0.7185501853992022,\n",
       "  0.4529398717016484,\n",
       "  0.4169389269521182],\n",
       " [0.7827936994117621,\n",
       "  0.9512181230171581,\n",
       "  0.5292211631411292,\n",
       "  0.5107232775808297,\n",
       "  0.3515215663333678,\n",
       "  0.9208785206785111],\n",
       " [0.8910291266694899,\n",
       "  0.8862392561290039,\n",
       "  0.6140662532269797,\n",
       "  0.5965824265796822,\n",
       "  0.6364435361052216,\n",
       "  0.814411654659722],\n",
       " [0.9594182981075652,\n",
       "  0.9551845060289688,\n",
       "  0.9523375014804843,\n",
       "  0.949246283764153,\n",
       "  0.9147245671239465,\n",
       "  0.8636979935924258],\n",
       " [0.1516008387516335,\n",
       "  0.3362588914359459,\n",
       "  0.21631839659236035,\n",
       "  0.4088210301786289,\n",
       "  0.22028033833174182,\n",
       "  0.1371395495447547],\n",
       " [0.03571876462126225,\n",
       "  0.23647778330930277,\n",
       "  0.3718530928655459,\n",
       "  0.24510038809080892,\n",
       "  0.26593723956358106,\n",
       "  0.011428516760669728],\n",
       " [0.3755927933843089,\n",
       "  0.4200216435251889,\n",
       "  0.3093916865401292,\n",
       "  0.33739699583167293,\n",
       "  0.3558205191917589,\n",
       "  0.20303173367218527],\n",
       " [0.1034547339374987,\n",
       "  0.21517254132826666,\n",
       "  0.33022580724268225,\n",
       "  0.290533829591642,\n",
       "  0.31057034802348304,\n",
       "  0.13669290101667655],\n",
       " [0.49233188263255623,\n",
       "  0.561730002437889,\n",
       "  0.5288186695458146,\n",
       "  0.5749177328840337,\n",
       "  0.6294497832848861,\n",
       "  0.5920231255107362],\n",
       " [0.26034158546148506,\n",
       "  0.0008631775395422231,\n",
       "  0.2438798682393803,\n",
       "  0.056415659982023234,\n",
       "  0.19737833207009714,\n",
       "  0.46583170107035843],\n",
       " [0.7148726477801313,\n",
       "  0.834207104613912,\n",
       "  0.4547026659656952,\n",
       "  0.5113786611183841,\n",
       "  0.4924341922573003,\n",
       "  0.585286444462282],\n",
       " [0.6869815049448711,\n",
       "  0.35198157831371074,\n",
       "  0.6011157700039464,\n",
       "  0.6271961389198386,\n",
       "  0.7019198308401433,\n",
       "  0.6699635756416061],\n",
       " [0.711507498850844,\n",
       "  0.6701036911831398,\n",
       "  0.6007652741221909,\n",
       "  0.30129393215646116,\n",
       "  0.6693348132523423,\n",
       "  0.7462296538283643],\n",
       " [0.46301448261049927,\n",
       "  0.5021205090004033,\n",
       "  0.5491585954602373,\n",
       "  0.5736940821284418,\n",
       "  0.4702930370375767,\n",
       "  0.6442408901046834],\n",
       " [0.7510332243196107,\n",
       "  0.6243541160215116,\n",
       "  0.49014661755605127,\n",
       "  0.41199421912406864,\n",
       "  0.43933118059505893,\n",
       "  0.6642564626499552],\n",
       " [0.1606677321105039,\n",
       "  0.24811849911020126,\n",
       "  0.24612129324030357,\n",
       "  0.3171471411904961,\n",
       "  0.16097423594168725,\n",
       "  0.18726993408504122],\n",
       " [0.6251081867951038,\n",
       "  0.6599685847761291,\n",
       "  0.47467209021628365,\n",
       "  0.5808754352942329,\n",
       "  0.5241808108317401,\n",
       "  0.6859205477052246],\n",
       " [0.5061505843784025,\n",
       "  0.4539359856656827,\n",
       "  0.571770208541858,\n",
       "  0.5187745455829995,\n",
       "  0.6055155087059081,\n",
       "  0.4598398108606306],\n",
       " [0.7196855263613033,\n",
       "  0.6345694554201606,\n",
       "  0.6436017087667164,\n",
       "  0.8207108053337742,\n",
       "  0.6805696261114526,\n",
       "  0.6712341223842773],\n",
       " [0.8455748566885977,\n",
       "  0.5070963840910777,\n",
       "  0.46313961329484765,\n",
       "  0.4816225403980658,\n",
       "  0.4018586356569608,\n",
       "  0.5838592127948874],\n",
       " [0.48550364333910323,\n",
       "  0.46908327321887733,\n",
       "  0.7738775685665333,\n",
       "  0.6627752441870232,\n",
       "  0.7367409349340839,\n",
       "  0.8282698318590714],\n",
       " [0.22817109321760004,\n",
       "  0.14645305400455993,\n",
       "  0.06523081414580341,\n",
       "  0.13779946739355858,\n",
       "  0.2756354763813285,\n",
       "  0.08296433504134273],\n",
       " [0.7917358668767833,\n",
       "  0.7615862021908364,\n",
       "  0.6304094206919956,\n",
       "  0.5521389040619643,\n",
       "  0.5588457423109665,\n",
       "  0.4027784572688037],\n",
       " [0.7781511444533817,\n",
       "  0.7568315115303521,\n",
       "  0.6603184737505929,\n",
       "  0.7688256730658403,\n",
       "  0.7286412932378389,\n",
       "  0.7067045046417031],\n",
       " [0.2666162335205525,\n",
       "  0.23914249348575867,\n",
       "  0.2701899994912852,\n",
       "  0.1669076602604029,\n",
       "  0.18663743577575062,\n",
       "  0.10609125923735954],\n",
       " [0.407352808065897,\n",
       "  0.31523558739431584,\n",
       "  0.536973539351265,\n",
       "  0.47199738044345985,\n",
       "  0.4131541365329142,\n",
       "  0.5351619689823515],\n",
       " [0.6937038743021728,\n",
       "  0.688919099912616,\n",
       "  0.7522358927112749,\n",
       "  0.6248689976473095,\n",
       "  0.6739115490655917,\n",
       "  0.7637515999093039],\n",
       " [0.22592586073919382,\n",
       "  0.19081459844159476,\n",
       "  0.17079195017111898,\n",
       "  0.25662254988669636,\n",
       "  0.11037164266777727,\n",
       "  0.0062494670559127725],\n",
       " [0.3225978393398422,\n",
       "  0.4419562860885364,\n",
       "  0.6212401678981565,\n",
       "  0.5155285472937043,\n",
       "  0.5610993679803988,\n",
       "  0.4555584316286832],\n",
       " [0.5780848769557141,\n",
       "  0.6120598368340188,\n",
       "  0.7399537745112933,\n",
       "  0.6794295080295907,\n",
       "  0.5531478568922066,\n",
       "  0.5603665116262975],\n",
       " [0.5473313001432518,\n",
       "  0.5899421174954,\n",
       "  0.6514077611747802,\n",
       "  0.5433628986468956,\n",
       "  0.4667118693002036,\n",
       "  0.39990414732642693],\n",
       " [0.21153757084326685,\n",
       "  0.2643790731540287,\n",
       "  0.24517412665281926,\n",
       "  0.14456097151493763,\n",
       "  0.2611171590241378,\n",
       "  0.0727243106319676],\n",
       " [0.2841744721559213,\n",
       "  0.24017065994783807,\n",
       "  0.5966552104683414,\n",
       "  0.4269614798051709,\n",
       "  0.36876483845006947,\n",
       "  0.2623359332098698],\n",
       " [0.46835499079622467,\n",
       "  0.5075828633849782,\n",
       "  0.4715729430100182,\n",
       "  0.6084288227363754,\n",
       "  0.5105850552208384,\n",
       "  0.5608522213886358],\n",
       " [0.4536775945437058,\n",
       "  0.3018781291410879,\n",
       "  0.3316571445661921,\n",
       "  0.41533103574960584,\n",
       "  0.28527041371783013,\n",
       "  0.3378467347064897],\n",
       " [0.9112960910660317,\n",
       "  0.788517419126049,\n",
       "  0.8036711267180603,\n",
       "  0.7167323401913848,\n",
       "  0.7947519506701725,\n",
       "  0.8998788082376774],\n",
       " [0.6630562514435565,\n",
       "  0.6668370417645332,\n",
       "  0.7165534125356569,\n",
       "  0.7007530020685708,\n",
       "  0.5952891858223829,\n",
       "  0.555182370943984],\n",
       " [0.9320234990946428,\n",
       "  0.9555269510157001,\n",
       "  0.43973172384864156,\n",
       "  0.6031691949523248,\n",
       "  0.6410872421671683,\n",
       "  0.744136377889932],\n",
       " [0.8070593472007332,\n",
       "  0.7185902934086139,\n",
       "  0.5476292721577747,\n",
       "  0.5885219092160748,\n",
       "  0.5999538427692466,\n",
       "  0.6193035813887976],\n",
       " [0.7946858248836643,\n",
       "  0.3027670908118688,\n",
       "  0.8204809090906696,\n",
       "  0.5591323908228415,\n",
       "  0.6882330147173221,\n",
       "  0.8101930547839749],\n",
       " [0.024411680107241127,\n",
       "  0.008208305525359436,\n",
       "  0.1357830244041969,\n",
       "  0.08141857227682212,\n",
       "  0.1198127706430321,\n",
       "  0.03653516498883577],\n",
       " [0.14561079383151362,\n",
       "  0.03792426145272112,\n",
       "  0.3560741728511686,\n",
       "  0.6085916144781793,\n",
       "  0.40617646701763,\n",
       "  0.01566377715509875],\n",
       " [0.07020090078250112,\n",
       "  0.059532376377267646,\n",
       "  0.055668943293601544,\n",
       "  0.11723682355556028,\n",
       "  0.0933468968584205,\n",
       "  0.01083459560889452],\n",
       " [0.7276904292786939,\n",
       "  0.8362359289583533,\n",
       "  0.6319460497037396,\n",
       "  0.6445809086838528,\n",
       "  0.6488439993836377,\n",
       "  0.7798562268831039],\n",
       " [0.906024116994354,\n",
       "  0.9656845682927355,\n",
       "  0.7784058064917263,\n",
       "  0.7760781792335185,\n",
       "  0.7823067378707531,\n",
       "  0.8882681084643023],\n",
       " [0.24018129522026002,\n",
       "  0.20908048985672872,\n",
       "  0.23401776499940397,\n",
       "  0.19273176411369838,\n",
       "  0.2391528437551394,\n",
       "  0.21276979965004966],\n",
       " [0.21924481346207386,\n",
       "  0.21955854705264666,\n",
       "  0.3748902889905559,\n",
       "  0.32451493781827967,\n",
       "  0.42985307780466475,\n",
       "  0.1934893487570229],\n",
       " [0.3164037919983069,\n",
       "  0.2612523720274793,\n",
       "  0.46530091313781685,\n",
       "  0.6139649714809365,\n",
       "  0.5405065990003273,\n",
       "  0.17397317293744802],\n",
       " [0.10476714180861019,\n",
       "  0.44870054537349446,\n",
       "  0.2418476008042028,\n",
       "  0.33579920281577746,\n",
       "  0.15205303269152554,\n",
       "  0.2271601393383015],\n",
       " [0.34870862087000043,\n",
       "  0.38696381526330426,\n",
       "  0.42003817642265995,\n",
       "  0.27051098381836475,\n",
       "  0.37606835080305084,\n",
       "  0.45556058335208305],\n",
       " [0.5858845463372817,\n",
       "  0.5752125067672338,\n",
       "  0.6372519896053424,\n",
       "  0.770870868285537,\n",
       "  0.7834015893064872,\n",
       "  0.5446973282247597],\n",
       " [0.4521544083628676,\n",
       "  0.4931588603759559,\n",
       "  0.48349852329697296,\n",
       "  0.3625004559170703,\n",
       "  0.6248137884937813,\n",
       "  0.44357601287691295],\n",
       " [0.10595524451293273,\n",
       "  0.04575058150709267,\n",
       "  0.5277762277724002,\n",
       "  0.5496434059069278,\n",
       "  0.6148594302749003,\n",
       "  0.11700962094674891],\n",
       " [0.3757808561278187,\n",
       "  0.15632726439942576,\n",
       "  0.46470947988986677,\n",
       "  0.3824105226308646,\n",
       "  0.33839287873238166,\n",
       "  0.2590096005114894],\n",
       " [0.24180665683732666,\n",
       "  0.24830357168744993,\n",
       "  0.17903496526047982,\n",
       "  0.18242428785936884,\n",
       "  0.2465965763093752,\n",
       "  0.061501971452328524],\n",
       " [0.005957089013615802,\n",
       "  0.062035663755637155,\n",
       "  0.08215080377857181,\n",
       "  0.23339530597660713,\n",
       "  0.031696741020842606,\n",
       "  0.1295585877845003],\n",
       " [0.27208594696307953,\n",
       "  0.28299872363995365,\n",
       "  0.25154901391613915,\n",
       "  0.294425831084528,\n",
       "  0.44523427129655113,\n",
       "  0.1399127547064827],\n",
       " [0.5084239033093387,\n",
       "  0.66299039833252,\n",
       "  0.21156667501875992,\n",
       "  0.43920262509761204,\n",
       "  0.4558114224194723,\n",
       "  0.3240457162525797],\n",
       " [0.6950696249761694,\n",
       "  0.79594185332664,\n",
       "  0.3521123493560594,\n",
       "  0.27079990465968656,\n",
       "  0.2598424604365278,\n",
       "  0.6560963111985867],\n",
       " [0.6605539280717385,\n",
       "  0.7220664918771453,\n",
       "  0.7124793026910827,\n",
       "  0.7065071397817173,\n",
       "  0.668696147900721,\n",
       "  0.821125586623543],\n",
       " [0.2985762610066065,\n",
       "  0.3544883834609093,\n",
       "  0.17104503854948566,\n",
       "  0.19405240122345074,\n",
       "  0.28775629041832473,\n",
       "  0.08680733848507186],\n",
       " [0.5688405686771307,\n",
       "  0.47162153362689085,\n",
       "  0.572896863756866,\n",
       "  0.6835742471993288,\n",
       "  0.672367621336561,\n",
       "  0.5213767568689738],\n",
       " [0.3662056292252119,\n",
       "  0.16472933832611858,\n",
       "  0.40677961039206634,\n",
       "  0.3979277984151377,\n",
       "  0.21887588595455212,\n",
       "  0.10641168205268514],\n",
       " [0.271988776436521,\n",
       "  0.18313194967191676,\n",
       "  0.19521662406960608,\n",
       "  0.34060682704464473,\n",
       "  0.44087041255310233,\n",
       "  0.31065364891675157],\n",
       " [0.27663588143326034,\n",
       "  0.16633075346751416,\n",
       "  0.39284291733467963,\n",
       "  0.21820763562256976,\n",
       "  0.4009983410214774,\n",
       "  0.3508683168270419],\n",
       " [0.3973857247623357,\n",
       "  0.3386382419659689,\n",
       "  0.6345678145810998,\n",
       "  0.7762646032547438,\n",
       "  0.5593209780725944,\n",
       "  0.388705899215105],\n",
       " [0.37236706183179513,\n",
       "  0.35255512325452504,\n",
       "  0.4193408736409733,\n",
       "  0.3637063568947093,\n",
       "  0.4825259338273175,\n",
       "  0.4003528203590564],\n",
       " [0.6170821524748099,\n",
       "  0.5799727573596127,\n",
       "  0.5330311798646261,\n",
       "  0.534950548740926,\n",
       "  0.5650195704684619,\n",
       "  0.6334777121859241],\n",
       " [0.8101236241442316,\n",
       "  0.7636956090898862,\n",
       "  0.6223484586455954,\n",
       "  0.5172847265687903,\n",
       "  0.6556730001437221,\n",
       "  0.8727253928248533],\n",
       " [0.13653989354148358,\n",
       "  0.06059562891543863,\n",
       "  0.04486915279516236,\n",
       "  0.033326649010925816,\n",
       "  0.10578190519040895,\n",
       "  0.03153859330835784],\n",
       " [0.5122593286985954,\n",
       "  0.5177486385710984,\n",
       "  0.38780850121467686,\n",
       "  0.554469976062892,\n",
       "  0.598749489579451,\n",
       "  0.7860971644749273],\n",
       " [0.796657759191808,\n",
       "  0.7524314172962334,\n",
       "  0.65864727015556,\n",
       "  0.4466445864329936,\n",
       "  0.43349169324452486,\n",
       "  0.8731605060842139],\n",
       " [0.6840521603298875,\n",
       "  0.40653642190415507,\n",
       "  0.5425941563037332,\n",
       "  0.4579450440254388,\n",
       "  0.5600187047468183,\n",
       "  0.4674462082802454],\n",
       " [0.20737909827529039,\n",
       "  0.22409573525048038,\n",
       "  0.3515825112356938,\n",
       "  0.41836987010929133,\n",
       "  0.2380103139260748,\n",
       "  0.20589189827790716],\n",
       " [0.4919241946750318,\n",
       "  0.6828332198008085,\n",
       "  0.6279365737956513,\n",
       "  0.7000083828775243,\n",
       "  0.6494976855245235,\n",
       "  0.26332152998047753],\n",
       " [0.4021947582499823,\n",
       "  0.4843493063900727,\n",
       "  0.4580478860478362,\n",
       "  0.45595168062644104,\n",
       "  0.32072515109989164,\n",
       "  0.2943419457440296],\n",
       " [0.19416799185513,\n",
       "  0.34270446634171037,\n",
       "  0.5050667117215631,\n",
       "  0.4485826336367935,\n",
       "  0.36325583927607163,\n",
       "  0.017926230228069696],\n",
       " [0.4756625969932196,\n",
       "  0.5201433577179612,\n",
       "  0.5599567889336258,\n",
       "  0.5459457283253115,\n",
       "  0.6074511896960015,\n",
       "  0.18918142783388836],\n",
       " [0.3007421369898979,\n",
       "  0.37001577716025386,\n",
       "  0.48349905466492626,\n",
       "  0.0520611142710703,\n",
       "  0.5352575906856067,\n",
       "  0.4358473706291162],\n",
       " [0.8820783147657625,\n",
       "  0.8486422632709086,\n",
       "  0.4981662750065511,\n",
       "  0.3975141420336763,\n",
       "  0.39030151801462865,\n",
       "  0.741791980223174],\n",
       " [0.5179717731278943,\n",
       "  0.570584234536699,\n",
       "  0.6197855715399889,\n",
       "  0.3385339635625251,\n",
       "  0.5060252035534889,\n",
       "  0.9093704727386756],\n",
       " [0.6958406222203032,\n",
       "  0.7233840439450632,\n",
       "  0.7806335444309678,\n",
       "  0.8583487870068682,\n",
       "  0.8762634563813915,\n",
       "  0.9594690636913021],\n",
       " [0.18115994545137715,\n",
       "  0.3071549661332813,\n",
       "  0.38695974990807347,\n",
       "  0.40521752313813486,\n",
       "  0.3656540193665751,\n",
       "  0.601113209007047],\n",
       " [0.7726360799770633,\n",
       "  0.650356917991116,\n",
       "  0.7243637283373947,\n",
       "  0.6736398818048916,\n",
       "  0.6498690819526005,\n",
       "  0.6907110285156772],\n",
       " [0.150633745529988,\n",
       "  0.4254935100242247,\n",
       "  0.3621191353674007,\n",
       "  0.4683896780990088,\n",
       "  0.5134273356703574,\n",
       "  0.5117970181734882],\n",
       " [0.22942399278008718,\n",
       "  0.1753978514947122,\n",
       "  0.5891374573721577,\n",
       "  0.5775372138550151,\n",
       "  0.5570276437292379,\n",
       "  0.3257125680397649],\n",
       " [0.3211277503843526,\n",
       "  0.19450550701827518,\n",
       "  0.6658561335479596,\n",
       "  0.7076421524988621,\n",
       "  0.7071623824358261,\n",
       "  0.4330738002695774],\n",
       " [0.6637699774007217,\n",
       "  0.5452467342560893,\n",
       "  0.5975891395739765,\n",
       "  0.5581562630804698,\n",
       "  0.540452092958394,\n",
       "  0.38494525285001546],\n",
       " [0.8574205091893763,\n",
       "  0.8899083720587679,\n",
       "  0.5742520011118977,\n",
       "  0.5598873754682546,\n",
       "  0.6806521664126631,\n",
       "  0.6124860973815467],\n",
       " [0.8627267827680865,\n",
       "  0.774402472555304,\n",
       "  0.6857541306712331,\n",
       "  0.5527623503727745,\n",
       "  0.5891097515320329,\n",
       "  0.6848614918816344],\n",
       " [0.20546727047945834,\n",
       "  0.2426366816437478,\n",
       "  0.23469991132308965,\n",
       "  0.22432379905019206,\n",
       "  0.18739660211711376,\n",
       "  0.2004964602503717],\n",
       " [0.8563574488943138,\n",
       "  0.7884221206318591,\n",
       "  0.6407565581547416,\n",
       "  0.6527895326961926,\n",
       "  0.5963554142291607,\n",
       "  0.8600943483912014],\n",
       " [0.7593173284907905,\n",
       "  0.5300845725769467,\n",
       "  0.7190484479234356,\n",
       "  0.7355243665307587,\n",
       "  0.769293662832486,\n",
       "  0.38202882374471203],\n",
       " [0.48343444985878603,\n",
       "  0.5861052606130053,\n",
       "  0.4600403156020025,\n",
       "  0.5704159536164892,\n",
       "  0.5040553786734037,\n",
       "  0.4934919411347669],\n",
       " [0.7597122834777283,\n",
       "  0.6773186991217975,\n",
       "  0.6448692424302633,\n",
       "  0.7436344516539636,\n",
       "  0.5636115683903776,\n",
       "  0.6655222298169536],\n",
       " [0.6186207587434284,\n",
       "  0.25869887311517725,\n",
       "  0.7268244788730778,\n",
       "  0.5011375947795872,\n",
       "  0.6829650247077016,\n",
       "  0.8551956260754751],\n",
       " [0.6520136862868606,\n",
       "  0.46970429157908866,\n",
       "  0.6436847959441189,\n",
       "  0.5835771107080622,\n",
       "  0.5243860001950877,\n",
       "  0.4138419867383409],\n",
       " [0.48548752255035116,\n",
       "  0.5481921268570851,\n",
       "  0.3799801761028137,\n",
       "  0.354967863786394,\n",
       "  0.27136681465653334,\n",
       "  0.7501611863512763],\n",
       " [0.5189844375490036,\n",
       "  0.7246548821148251,\n",
       "  0.5278648678968257,\n",
       "  0.6792670599848629,\n",
       "  0.5441758464874566,\n",
       "  0.4622007025272985],\n",
       " [0.4249412137510728,\n",
       "  0.3457568713475061,\n",
       "  0.457452134637746,\n",
       "  0.6540325979518476,\n",
       "  0.30396742097066787,\n",
       "  0.49711379557115953],\n",
       " [0.30718554578033574,\n",
       "  0.36373303655927963,\n",
       "  0.5409693158283465,\n",
       "  0.43076610086900213,\n",
       "  0.4306974180264268,\n",
       "  0.2151555923612687],\n",
       " [0.11790159346106714,\n",
       "  0.1303975400805747,\n",
       "  0.39153653274751454,\n",
       "  0.16907091746719055,\n",
       "  0.28525730998841,\n",
       "  0.010076382374181132],\n",
       " [0.0024668052850303386,\n",
       "  0.0007290803157419899,\n",
       "  0.17825713193819706,\n",
       "  0.16805059532082034,\n",
       "  0.2702435925839214,\n",
       "  0.13809360051576028],\n",
       " [0.1278184186076446,\n",
       "  0.1302650241679875,\n",
       "  0.38606663083831455,\n",
       "  0.3677655647155214,\n",
       "  0.3705304807872908,\n",
       "  0.03983003740977793],\n",
       " [0.5459050978883335,\n",
       "  0.33136209276006984,\n",
       "  0.5241725413383711,\n",
       "  0.534468446605412,\n",
       "  0.6209306430487403,\n",
       "  0.0982272621034748],\n",
       " [0.46915794461862154,\n",
       "  0.38000048938028497,\n",
       "  0.44314521089505116,\n",
       "  0.504581638891626,\n",
       "  0.434034399317455,\n",
       "  0.09423604909690664],\n",
       " [0.14434595175745427,\n",
       "  0.052542509286814344,\n",
       "  0.4797191128691799,\n",
       "  0.22350836423267137,\n",
       "  0.3345934349595304,\n",
       "  0.2207598327380993],\n",
       " [0.630151382214654,\n",
       "  0.6717191569522589,\n",
       "  0.6481298689456877,\n",
       "  0.5039775920127718,\n",
       "  0.538250033272906,\n",
       "  0.6411374631050321],\n",
       " [0.16445769158931767,\n",
       "  0.06502523905156445,\n",
       "  0.236685716675082,\n",
       "  0.18813291303064955,\n",
       "  0.028057134086205013,\n",
       "  0.18732694273160558],\n",
       " [0.27383554697520296,\n",
       "  0.23828896452446396,\n",
       "  0.15016700256279608,\n",
       "  0.1434287811915759,\n",
       "  0.20627324038932845,\n",
       "  0.15872672372078672],\n",
       " [0.754332008249132,\n",
       "  0.7030882271259808,\n",
       "  0.7579170748530892,\n",
       "  0.6870835011814957,\n",
       "  0.7056748105272723,\n",
       "  0.3969208399891878],\n",
       " [0.5785330475046663,\n",
       "  0.543441859352944,\n",
       "  0.7466449412662411,\n",
       "  0.6337156494980944,\n",
       "  0.770026970176075,\n",
       "  0.5003526319139001],\n",
       " [0.7823847116828353,\n",
       "  0.8000784933785514,\n",
       "  0.5914612331630152,\n",
       "  0.5698477257708661,\n",
       "  0.5389125230878103,\n",
       "  0.5978423980536506],\n",
       " [0.6137547830843681,\n",
       "  0.7924320103458067,\n",
       "  0.7333913972505631,\n",
       "  0.7392860212292538,\n",
       "  0.7117398914871376,\n",
       "  0.7978937227715628],\n",
       " [0.9263499013234252,\n",
       "  0.9131527440250489,\n",
       "  0.930791754692671,\n",
       "  0.8928804533904331,\n",
       "  0.9027464610753333,\n",
       "  0.7993035671083619],\n",
       " [0.798636578742773,\n",
       "  0.8686797019763554,\n",
       "  0.47738107260011836,\n",
       "  0.5243509581881429,\n",
       "  0.5304304178790276,\n",
       "  0.5688093955706427],\n",
       " [0.7828579576943321,\n",
       "  0.8128986124532438,\n",
       "  0.7333807333004128,\n",
       "  0.7291095079202301,\n",
       "  0.6372928851216699,\n",
       "  0.6998461688131847],\n",
       " [0.5365791401725173,\n",
       "  0.6181388193381976,\n",
       "  0.6962897719019225,\n",
       "  0.5770014783965518,\n",
       "  0.5094572886071577,\n",
       "  0.4193023384171634],\n",
       " [0.5619579090478486,\n",
       "  0.6971599830238951,\n",
       "  0.47372746585372605,\n",
       "  0.6089237381139576,\n",
       "  0.5813780039844424,\n",
       "  0.7174529283847049],\n",
       " [0.45003942822855475,\n",
       "  0.7608706715044609,\n",
       "  0.4994945272923267,\n",
       "  0.5801189559715729,\n",
       "  0.6341092121390904,\n",
       "  0.35841662298367005],\n",
       " [0.36534137250994103,\n",
       "  0.22876455967909728,\n",
       "  0.4908538065506176,\n",
       "  0.2653470669649396,\n",
       "  0.37891580169153316,\n",
       "  0.2362347055623636],\n",
       " [0.6674736036405488,\n",
       "  0.4484386855074439,\n",
       "  0.549253629495058,\n",
       "  0.49387239777965286,\n",
       "  0.35170182373167713,\n",
       "  0.4528498845050365],\n",
       " [0.2247161206763487,\n",
       "  0.24525468276358317,\n",
       "  0.258166358617558,\n",
       "  0.5327874769149463,\n",
       "  0.4273063021529595,\n",
       "  0.09704500132120442],\n",
       " [0.3408654415096509,\n",
       "  0.24794700524142851,\n",
       "  0.5877924112526773,\n",
       "  0.5179993845810509,\n",
       "  0.44721525151947117,\n",
       "  0.2921693673729192],\n",
       " [0.3155781891780986,\n",
       "  0.23947604430366976,\n",
       "  0.442744944962046,\n",
       "  0.35896021267343764,\n",
       "  0.4553638010324139,\n",
       "  0.2370797631169285],\n",
       " [0.5603515485367396,\n",
       "  0.6951676579002777,\n",
       "  0.5417025376060529,\n",
       "  0.690102217108135,\n",
       "  0.7481844593816207,\n",
       "  0.5395996577198497],\n",
       " [0.46792983590596154,\n",
       "  0.5047677316666739,\n",
       "  0.31034864120809125,\n",
       "  0.4591175410562846,\n",
       "  0.4300898856218329,\n",
       "  0.36062122860296186],\n",
       " [0.33703117039239,\n",
       "  0.3691519149131857,\n",
       "  0.5196158910806408,\n",
       "  0.5310720076616814,\n",
       "  0.5108987796216438,\n",
       "  0.35791934507412154],\n",
       " [0.4564388613729283,\n",
       "  0.6287876486126069,\n",
       "  0.7229181204339166,\n",
       "  0.5902373322923571,\n",
       "  0.7891580872206186,\n",
       "  0.45716674342520264],\n",
       " [0.6652548384266054,\n",
       "  0.7461997469054718,\n",
       "  0.7737645953312638,\n",
       "  0.7523905364860035,\n",
       "  0.743542730157616,\n",
       "  0.7515532647210014],\n",
       " [0.509185962377257,\n",
       "  0.45844581848553306,\n",
       "  0.6317965760290731,\n",
       "  0.5883046385404822,\n",
       "  0.4967676516835414,\n",
       "  0.2878659526167874],\n",
       " [0.6442572329905343,\n",
       "  0.6761476085648515,\n",
       "  0.7352719816692403,\n",
       "  0.7131588918567507,\n",
       "  0.68439879375248,\n",
       "  0.6283710681221066],\n",
       " [0.20745974880316218,\n",
       "  0.14703120030765832,\n",
       "  0.30036090941885696,\n",
       "  0.30019321091924045,\n",
       "  0.24114115246091183,\n",
       "  0.4525594818265797],\n",
       " [0.710529493402307,\n",
       "  0.9175477512217018,\n",
       "  0.5663744247831919,\n",
       "  0.7228922874480139,\n",
       "  0.6258960021314103,\n",
       "  0.8746007074276145],\n",
       " [0.4666234067460935,\n",
       "  0.7784540231233879,\n",
       "  0.5732129287981155,\n",
       "  0.64269576273912,\n",
       "  0.7120381817572168,\n",
       "  0.711144309820738],\n",
       " [0.6825916521171318,\n",
       "  0.7126924262532204,\n",
       "  0.6369230496397849,\n",
       "  0.6277014368145768,\n",
       "  0.591148808708595,\n",
       "  0.5111126254659547],\n",
       " [0.6411826023492099,\n",
       "  0.8219443566023719,\n",
       "  0.5443066695715053,\n",
       "  0.44904858159518285,\n",
       "  0.5632320415975989,\n",
       "  0.7626607281289018],\n",
       " [0.8485048689944357,\n",
       "  0.8957737475572012,\n",
       "  0.7283438822669438,\n",
       "  0.8123116195562196,\n",
       "  0.6815449603960124,\n",
       "  0.8758232701517605],\n",
       " [0.6009109259095383,\n",
       "  0.6214745193564282,\n",
       "  0.7779308318563511,\n",
       "  0.7030455397341149,\n",
       "  0.5322688892115365,\n",
       "  0.8904808664409112],\n",
       " [0.450202010424386,\n",
       "  0.5478927893782406,\n",
       "  0.5686221877043782,\n",
       "  0.664367199742352,\n",
       "  0.5239354757813045,\n",
       "  0.3933926439875689],\n",
       " [0.6685853167726629,\n",
       "  0.5039839958840492,\n",
       "  0.6868978458190749,\n",
       "  0.7433828605984437,\n",
       "  0.6905600076861214,\n",
       "  0.16445416341297978],\n",
       " [0.2259527780874945,\n",
       "  0.30667364457452667,\n",
       "  0.21065302343201908,\n",
       "  0.24096476630486308,\n",
       "  0.33840550107537626,\n",
       "  0.17656774133532144],\n",
       " [0.19496897142059103,\n",
       "  0.32671402053811854,\n",
       "  0.27077772705370196,\n",
       "  0.4932753314275457,\n",
       "  0.2844918889658892,\n",
       "  0.12171867380546073],\n",
       " [0.6928598294886251,\n",
       "  0.6596217968156328,\n",
       "  0.7853536319251655,\n",
       "  0.7807743014559864,\n",
       "  0.7880916839732839,\n",
       "  0.476939365554757],\n",
       " [0.17545347554337715,\n",
       "  0.31350705219647834,\n",
       "  0.2284295298935465,\n",
       "  0.16101593130184372,\n",
       "  0.12586989417098066,\n",
       "  0.16165386957519015],\n",
       " [0.8135322538896892,\n",
       "  0.7489125149529151,\n",
       "  0.6268201008692623,\n",
       "  0.4612240603851524,\n",
       "  0.5686330288930495,\n",
       "  0.773918851290048],\n",
       " [0.8272948982502073,\n",
       "  0.9424433364750158,\n",
       "  0.7190766841302734,\n",
       "  0.7441844652375478,\n",
       "  0.6580877772712332,\n",
       "  0.9681659089343592],\n",
       " [0.8823115540167104,\n",
       "  0.7470028871875642,\n",
       "  0.7310298430362554,\n",
       "  0.878990634025701,\n",
       "  0.9265676720157268,\n",
       "  0.9088534882477985],\n",
       " [0.8947835279142871,\n",
       "  0.12946008493600172,\n",
       "  0.27373216212464496,\n",
       "  0.40350182379364574,\n",
       "  0.38128396171632645,\n",
       "  0.6834263569327923],\n",
       " [0.8523356521922796,\n",
       "  0.7371029905608042,\n",
       "  0.6066189136130427,\n",
       "  0.5770984344724328,\n",
       "  0.5059536777241245,\n",
       "  0.8490289079427155],\n",
       " [0.8744980831317002,\n",
       "  0.6887125609326894,\n",
       "  0.6897492552697295,\n",
       "  0.8734211742631672,\n",
       "  0.49915284907005314,\n",
       "  0.8425003394152099],\n",
       " [0.5727817593121928,\n",
       "  0.7635288999498028,\n",
       "  0.5656434458450877,\n",
       "  0.6511029714476289,\n",
       "  0.5345185045887736,\n",
       "  0.5087243365430832],\n",
       " [0.7578922775350575,\n",
       "  0.7628358999030465,\n",
       "  0.8670967062473434,\n",
       "  0.7339758962567224,\n",
       "  0.7718199618340473,\n",
       "  0.5487983294588552],\n",
       " [0.421670449938995,\n",
       "  0.38461921326472615,\n",
       "  0.36320739248398826,\n",
       "  0.34478426337490203,\n",
       "  0.35552515777305216,\n",
       "  0.11616932102488567],\n",
       " [0.6406313606124948,\n",
       "  0.7929793554844746,\n",
       "  0.5722368123950541,\n",
       "  0.7302318614286116,\n",
       "  0.6545370731454427,\n",
       "  0.676796235145186],\n",
       " [0.8811228046779183,\n",
       "  0.8727949679543636,\n",
       "  0.8135247813089617,\n",
       "  0.766139086439706,\n",
       "  0.8063127594551569,\n",
       "  0.7949132326054869],\n",
       " [0.5480091693087672,\n",
       "  0.3009371723008829,\n",
       "  0.561940960286319,\n",
       "  0.3205770490537587,\n",
       "  0.26252727533080206,\n",
       "  0.17577721930820717],\n",
       " [0.5455677854780496,\n",
       "  0.45495067306050563,\n",
       "  0.673996169719645,\n",
       "  0.5530329801673899,\n",
       "  0.6582881706019645,\n",
       "  0.5243463239474816],\n",
       " [0.8452828307589808,\n",
       "  0.8180786051211965,\n",
       "  0.7696417394268233,\n",
       "  0.7595952730358441,\n",
       "  0.807319800431906,\n",
       "  0.7429803632068719],\n",
       " [0.1224593036140996,\n",
       "  0.13737693746649343,\n",
       "  0.05901269102850602,\n",
       "  0.24435086006025256,\n",
       "  0.12452354533003704,\n",
       "  0.09629504908769847]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAKDD2010\n",
      "The following are the correlations between the faithfullness metric and... (abs fixed, subset_len=1, iterations=100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank Algorithm</th>\n",
       "      <th>Pearson Correlation</th>\n",
       "      <th>Spearman Correlation</th>\n",
       "      <th>Kendall Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness (itself)</td>\n",
       "      <td>0.847137</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>0.662609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rank_based_fairthfulness:sum</td>\n",
       "      <td>0.675876</td>\n",
       "      <td>0.666648</td>\n",
       "      <td>0.479331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rank_based_fairthfulness:percentile</td>\n",
       "      <td>0.684090</td>\n",
       "      <td>0.672473</td>\n",
       "      <td>0.480045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rank_based_fairthfulness:avg</td>\n",
       "      <td>0.673816</td>\n",
       "      <td>0.664134</td>\n",
       "      <td>0.475630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rank_based_fairthfulness:inverse</td>\n",
       "      <td>0.802265</td>\n",
       "      <td>0.801084</td>\n",
       "      <td>0.601338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Rank Algorithm  Pearson Correlation  \\\n",
       "0                faithfulness (itself)             0.847137   \n",
       "1         rank_based_fairthfulness:sum             0.675876   \n",
       "2  rank_based_fairthfulness:percentile             0.684090   \n",
       "3         rank_based_fairthfulness:avg             0.673816   \n",
       "4     rank_based_fairthfulness:inverse             0.802265   \n",
       "\n",
       "   Spearman Correlation  Kendall Correlation  \n",
       "0              0.841800             0.662609  \n",
       "1              0.666648             0.479331  \n",
       "2              0.672473             0.480045  \n",
       "3              0.664134             0.475630  \n",
       "4              0.801084             0.601338  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "comp_arr = np.array(comp_arr)\n",
    "rank_algs = [\"faithfulness (itself)\", \"rank_based_fairthfulness:sum\", \"rank_based_fairthfulness:percentile\", \"rank_based_fairthfulness:avg\", \"rank_based_fairthfulness:inverse\"]\n",
    "# rank_algs = [\"NRC\"]\n",
    "\n",
    "pearson_corr = [np.corrcoef(comp_arr[:, 0], comp_arr[:, i])[0, 1] for i in range(1, comp_arr.shape[1])]\n",
    "spearman_corr = [spearmanr(comp_arr[:, 0], comp_arr[:, i]).correlation for i in range(1, comp_arr.shape[1])]\n",
    "kendall_corr = [kendalltau(comp_arr[:, 0], comp_arr[:, i]).correlation for i in range(1, comp_arr.shape[1])]\n",
    "\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Rank Algorithm': rank_algs,\n",
    "    'Pearson Correlation': pearson_corr,\n",
    "    'Spearman Correlation': spearman_corr,\n",
    "    'Kendall Correlation': kendall_corr\n",
    "})\n",
    "\n",
    "print(dataset_name)\n",
    "print(\"The following are the correlations between the faithfullness metric and... (abs fixed, subset_len=1, iterations=100)\")\n",
    "display(correlation_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
