{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 14:06:01.390083: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-16 14:06:01.413689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from data_loading import *\n",
    "\n",
    "from xai_agg.agg_exp import *\n",
    "from xai_agg.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7466666666666667\n",
      "ROC AUC: 0.5158014399393709\n"
     ]
    }
   ],
   "source": [
    "dataset_name, preprocessed_data, categorical_features, X, y, X_train, X_test, y_train, y_test, clf = load_pakdd2010_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2481 - val_loss: 1.0735\n",
      "Epoch 2/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 1.3362 - val_loss: 1.0234\n",
      "Epoch 3/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 1.1877 - val_loss: 0.9750\n",
      "Epoch 4/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 1.0771 - val_loss: 0.9349\n",
      "Epoch 5/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 1.0896 - val_loss: 0.9042\n",
      "Epoch 6/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.9829 - val_loss: 0.8814\n",
      "Epoch 7/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.9799 - val_loss: 0.8644\n",
      "Epoch 8/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 1.0897 - val_loss: 0.8507\n",
      "Epoch 9/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.9986 - val_loss: 0.8390\n",
      "Epoch 10/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 1.0154 - val_loss: 0.8278\n",
      "Epoch 11/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.9178 - val_loss: 0.8175\n",
      "Epoch 12/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.9621 - val_loss: 0.8079\n",
      "Epoch 13/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.9369 - val_loss: 0.7990\n",
      "Epoch 14/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 0.8067 - val_loss: 0.7911\n",
      "Epoch 15/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.9871 - val_loss: 0.7840\n",
      "Epoch 16/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.9203 - val_loss: 0.7778\n",
      "Epoch 17/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 0.9505 - val_loss: 0.7722\n",
      "Epoch 18/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.9319 - val_loss: 0.7672\n",
      "Epoch 19/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.9348 - val_loss: 0.7627\n",
      "Epoch 20/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.9179 - val_loss: 0.7587\n",
      "Epoch 21/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 1.0103 - val_loss: 0.7550\n",
      "Epoch 22/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.9381 - val_loss: 0.7517\n",
      "Epoch 23/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 0.8416 - val_loss: 0.7485\n",
      "Epoch 24/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.8742 - val_loss: 0.7454\n",
      "Epoch 25/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.9355 - val_loss: 0.7426\n",
      "Epoch 26/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.7935 - val_loss: 0.7400\n",
      "Epoch 27/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.8363 - val_loss: 0.7374\n",
      "Epoch 28/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 0.8608 - val_loss: 0.7350\n",
      "Epoch 29/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.8320 - val_loss: 0.7325\n",
      "Epoch 30/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.8488 - val_loss: 0.7302\n",
      "Epoch 31/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.9434 - val_loss: 0.7279\n",
      "Epoch 32/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.8996 - val_loss: 0.7257\n",
      "Epoch 33/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.9640 - val_loss: 0.7235\n",
      "Epoch 34/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.9201 - val_loss: 0.7216\n",
      "Epoch 35/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.8367 - val_loss: 0.7195\n",
      "Epoch 36/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.8283 - val_loss: 0.7175\n",
      "Epoch 37/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 0.8116 - val_loss: 0.7156\n",
      "Epoch 38/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 0.9358 - val_loss: 0.7137\n",
      "Epoch 39/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.8489 - val_loss: 0.7119\n",
      "Epoch 40/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.9945 - val_loss: 0.7101\n",
      "Epoch 41/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.7897 - val_loss: 0.7083\n",
      "Epoch 42/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509us/step - loss: 0.8470 - val_loss: 0.7066\n",
      "Epoch 43/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.8476 - val_loss: 0.7049\n",
      "Epoch 44/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.8213 - val_loss: 0.7033\n",
      "Epoch 45/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.7377 - val_loss: 0.7018\n",
      "Epoch 46/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.7727 - val_loss: 0.7003\n",
      "Epoch 47/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.7948 - val_loss: 0.6988\n",
      "Epoch 48/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 0.8405 - val_loss: 0.6973\n",
      "Epoch 49/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.8609 - val_loss: 0.6961\n",
      "Epoch 50/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.8427 - val_loss: 0.6947\n",
      "Epoch 51/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.7689 - val_loss: 0.6935\n",
      "Epoch 52/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.7973 - val_loss: 0.6922\n",
      "Epoch 53/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - loss: 0.8873 - val_loss: 0.6911\n",
      "Epoch 54/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.7781 - val_loss: 0.6900\n",
      "Epoch 55/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.8156 - val_loss: 0.6890\n",
      "Epoch 56/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.8650 - val_loss: 0.6880\n",
      "Epoch 57/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 0.8380 - val_loss: 0.6870\n",
      "Epoch 58/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.7906 - val_loss: 0.6861\n",
      "Epoch 59/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.8057 - val_loss: 0.6853\n",
      "Epoch 60/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.7015 - val_loss: 0.6845\n",
      "Epoch 61/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.9698 - val_loss: 0.6837\n",
      "Epoch 62/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.8310 - val_loss: 0.6830\n",
      "Epoch 63/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.7574 - val_loss: 0.6823\n",
      "Epoch 64/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.7110 - val_loss: 0.6816\n",
      "Epoch 65/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.7438 - val_loss: 0.6809\n",
      "Epoch 66/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.7673 - val_loss: 0.6802\n",
      "Epoch 67/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8147 - val_loss: 0.6797\n",
      "Epoch 68/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 0.7946 - val_loss: 0.6791\n",
      "Epoch 69/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - loss: 0.7425 - val_loss: 0.6784\n",
      "Epoch 70/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.8008 - val_loss: 0.6778\n",
      "Epoch 71/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7240 - val_loss: 0.6773\n",
      "Epoch 72/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.8840 - val_loss: 0.6767\n",
      "Epoch 73/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9190 - val_loss: 0.6761\n",
      "Epoch 74/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.8109 - val_loss: 0.6755\n",
      "Epoch 75/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7489 - val_loss: 0.6749\n",
      "Epoch 76/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7925 - val_loss: 0.6744\n",
      "Epoch 77/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 0.7744 - val_loss: 0.6738\n",
      "Epoch 78/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7811 - val_loss: 0.6734\n",
      "Epoch 79/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9166 - val_loss: 0.6729\n",
      "Epoch 80/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 0.7621 - val_loss: 0.6724\n",
      "Epoch 81/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.8626 - val_loss: 0.6720\n",
      "Epoch 82/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.8309 - val_loss: 0.6716\n",
      "Epoch 83/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - loss: 0.6837 - val_loss: 0.6711\n",
      "Epoch 84/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 0.9157 - val_loss: 0.6707\n",
      "Epoch 85/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.8088 - val_loss: 0.6704\n",
      "Epoch 86/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.7404 - val_loss: 0.6699\n",
      "Epoch 87/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.9438 - val_loss: 0.6696\n",
      "Epoch 88/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.7133 - val_loss: 0.6692\n",
      "Epoch 89/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 0.9203 - val_loss: 0.6688\n",
      "Epoch 90/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.7600 - val_loss: 0.6685\n",
      "Epoch 91/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.6991 - val_loss: 0.6682\n",
      "Epoch 92/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.7565 - val_loss: 0.6679\n",
      "Epoch 93/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.7922 - val_loss: 0.6676\n",
      "Epoch 94/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.8267 - val_loss: 0.6673\n",
      "Epoch 95/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.7100 - val_loss: 0.6671\n",
      "Epoch 96/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.7237 - val_loss: 0.6667\n",
      "Epoch 97/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.8940 - val_loss: 0.6665\n",
      "Epoch 98/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.8155 - val_loss: 0.6662\n",
      "Epoch 99/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step - loss: 0.7792 - val_loss: 0.6660\n",
      "Epoch 100/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.8340 - val_loss: 0.6657\n",
      "Epoch 101/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.8419 - val_loss: 0.6654\n",
      "Epoch 102/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 0.7817 - val_loss: 0.6651\n",
      "Epoch 103/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.9305 - val_loss: 0.6649\n",
      "Epoch 104/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.8261 - val_loss: 0.6647\n",
      "Epoch 105/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 0.8762 - val_loss: 0.6644\n",
      "Epoch 106/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.8787 - val_loss: 0.6641\n",
      "Epoch 107/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.6960 - val_loss: 0.6640\n",
      "Epoch 108/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.7390 - val_loss: 0.6637\n",
      "Epoch 109/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 0.7632 - val_loss: 0.6635\n",
      "Epoch 110/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.7834 - val_loss: 0.6633\n",
      "Epoch 111/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.7236 - val_loss: 0.6630\n",
      "Epoch 112/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.7747 - val_loss: 0.6628\n",
      "Epoch 113/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.7432 - val_loss: 0.6625\n",
      "Epoch 114/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.7066 - val_loss: 0.6624\n",
      "Epoch 115/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.7589 - val_loss: 0.6621\n",
      "Epoch 116/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.7442 - val_loss: 0.6620\n",
      "Epoch 117/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 0.7562 - val_loss: 0.6618\n",
      "Epoch 118/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.8008 - val_loss: 0.6616\n",
      "Epoch 119/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 0.7083 - val_loss: 0.6615\n",
      "Epoch 120/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.7503 - val_loss: 0.6613\n",
      "Epoch 121/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 0.7908 - val_loss: 0.6612\n",
      "Epoch 122/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 0.7790 - val_loss: 0.6611\n",
      "Epoch 123/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.8409 - val_loss: 0.6609\n",
      "Epoch 124/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.8071 - val_loss: 0.6607\n",
      "Epoch 125/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.8000 - val_loss: 0.6606\n",
      "Epoch 126/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 0.7600 - val_loss: 0.6605\n",
      "Epoch 127/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.7447 - val_loss: 0.6603\n",
      "Epoch 128/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.8322 - val_loss: 0.6602\n",
      "Epoch 129/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.7729 - val_loss: 0.6600\n",
      "Epoch 130/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.9381 - val_loss: 0.6598\n",
      "Epoch 131/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.7398 - val_loss: 0.6597\n",
      "Epoch 132/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.7766 - val_loss: 0.6594\n",
      "Epoch 133/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.7613 - val_loss: 0.6593\n",
      "Epoch 134/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.8606 - val_loss: 0.6591\n",
      "Epoch 135/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.8019 - val_loss: 0.6590\n",
      "Epoch 136/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - loss: 0.7265 - val_loss: 0.6587\n",
      "Epoch 137/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.7645 - val_loss: 0.6586\n",
      "Epoch 138/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 0.7741 - val_loss: 0.6585\n",
      "Epoch 139/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.9086 - val_loss: 0.6582\n",
      "Epoch 140/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.7639 - val_loss: 0.6581\n",
      "Epoch 141/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7955 - val_loss: 0.6579\n",
      "Epoch 142/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.7473 - val_loss: 0.6577\n",
      "Epoch 143/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.7714 - val_loss: 0.6576\n",
      "Epoch 144/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7571 - val_loss: 0.6575\n",
      "Epoch 145/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7247 - val_loss: 0.6572\n",
      "Epoch 146/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.7517 - val_loss: 0.6571\n",
      "Epoch 147/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.7623 - val_loss: 0.6569\n",
      "Epoch 148/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7011 - val_loss: 0.6568\n",
      "Epoch 149/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.9214 - val_loss: 0.6567\n",
      "Epoch 150/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.8885 - val_loss: 0.6566\n",
      "Epoch 151/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 0.7822 - val_loss: 0.6564\n",
      "Epoch 152/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.7667 - val_loss: 0.6563\n",
      "Epoch 153/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.8045 - val_loss: 0.6562\n",
      "Epoch 154/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.7410 - val_loss: 0.6560\n",
      "Epoch 155/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7587 - val_loss: 0.6560\n",
      "Epoch 156/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.9115 - val_loss: 0.6558\n",
      "Epoch 157/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.8147 - val_loss: 0.6557\n",
      "Epoch 158/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.7638 - val_loss: 0.6556\n",
      "Epoch 159/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 0.8110 - val_loss: 0.6555\n",
      "Epoch 160/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.8179 - val_loss: 0.6555\n",
      "Epoch 161/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 1.1355 - val_loss: 0.6553\n",
      "Epoch 162/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.8655 - val_loss: 0.6552\n",
      "Epoch 163/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 0.7042 - val_loss: 0.6551\n",
      "Epoch 164/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 0.7513 - val_loss: 0.6549\n",
      "Epoch 165/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.8006 - val_loss: 0.6549\n",
      "Epoch 166/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8208 - val_loss: 0.6548\n",
      "Epoch 167/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.8199 - val_loss: 0.6548\n",
      "Epoch 168/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7826 - val_loss: 0.6547\n",
      "Epoch 169/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.7440 - val_loss: 0.6545\n",
      "Epoch 170/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.8129 - val_loss: 0.6545\n",
      "Epoch 171/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.7874 - val_loss: 0.6544\n",
      "Epoch 172/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.8075 - val_loss: 0.6542\n",
      "Epoch 173/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 0.7262 - val_loss: 0.6544\n",
      "Epoch 174/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.7792 - val_loss: 0.6540\n",
      "Epoch 175/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.8015 - val_loss: 0.6540\n",
      "Epoch 176/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.7767 - val_loss: 0.6539\n",
      "Epoch 177/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7804 - val_loss: 0.6538\n",
      "Epoch 178/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 0.7988 - val_loss: 0.6536\n",
      "Epoch 179/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.7553 - val_loss: 0.6537\n",
      "Epoch 180/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.7489 - val_loss: 0.6535\n",
      "Epoch 181/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 0.7431 - val_loss: 0.6534\n",
      "Epoch 182/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 0.7221 - val_loss: 0.6533\n",
      "Epoch 183/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.8079 - val_loss: 0.6533\n",
      "Epoch 184/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.8335 - val_loss: 0.6532\n",
      "Epoch 185/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.8845 - val_loss: 0.6530\n",
      "Epoch 186/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 0.7707 - val_loss: 0.6530\n",
      "Epoch 187/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.7231 - val_loss: 0.6529\n",
      "Epoch 188/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.8818 - val_loss: 0.6529\n",
      "Epoch 189/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7936 - val_loss: 0.6529\n",
      "Epoch 190/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.7768 - val_loss: 0.6527\n",
      "Epoch 191/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.8629 - val_loss: 0.6526\n",
      "Epoch 192/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.8247 - val_loss: 0.6526\n",
      "Epoch 193/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.9223 - val_loss: 0.6526\n",
      "Epoch 194/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8000 - val_loss: 0.6525\n",
      "Epoch 195/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 0.8417 - val_loss: 0.6525\n",
      "Epoch 196/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.8244 - val_loss: 0.6522\n",
      "Epoch 197/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.7451 - val_loss: 0.6522\n",
      "Epoch 198/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.8943 - val_loss: 0.6521\n",
      "Epoch 199/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 0.8001 - val_loss: 0.6520\n",
      "Epoch 200/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7670 - val_loss: 0.6519\n",
      "Epoch 201/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.7155 - val_loss: 0.6520\n",
      "Epoch 202/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.8961 - val_loss: 0.6518\n",
      "Epoch 203/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.7126 - val_loss: 0.6518\n",
      "Epoch 204/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.8178 - val_loss: 0.6518\n",
      "Epoch 205/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.7755 - val_loss: 0.6517\n",
      "Epoch 206/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.7228 - val_loss: 0.6516\n",
      "Epoch 207/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - loss: 1.0625 - val_loss: 0.6516\n",
      "Epoch 208/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.9064 - val_loss: 0.6516\n",
      "Epoch 209/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6918 - val_loss: 0.6514\n",
      "Epoch 210/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.7711 - val_loss: 0.6513\n",
      "Epoch 211/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.8558 - val_loss: 0.6513\n",
      "Epoch 212/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.6965 - val_loss: 0.6512\n",
      "Epoch 213/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.7339 - val_loss: 0.6511\n",
      "Epoch 214/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.7229 - val_loss: 0.6511\n",
      "Epoch 215/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 0.7892 - val_loss: 0.6510\n",
      "Epoch 216/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.8001 - val_loss: 0.6510\n",
      "Epoch 217/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 0.8251 - val_loss: 0.6509\n",
      "Epoch 218/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.7149 - val_loss: 0.6509\n",
      "Epoch 219/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.8373 - val_loss: 0.6508\n",
      "Epoch 220/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.7172 - val_loss: 0.6508\n",
      "Epoch 221/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.8000 - val_loss: 0.6507\n",
      "Epoch 222/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 0.7627 - val_loss: 0.6506\n",
      "Epoch 223/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7733 - val_loss: 0.6505\n",
      "Epoch 224/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.7469 - val_loss: 0.6504\n",
      "Epoch 225/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495us/step - loss: 0.7760 - val_loss: 0.6504\n",
      "Epoch 226/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.9642 - val_loss: 0.6504\n",
      "Epoch 227/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.7230 - val_loss: 0.6503\n",
      "Epoch 228/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.7774 - val_loss: 0.6503\n",
      "Epoch 229/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7752 - val_loss: 0.6502\n",
      "Epoch 230/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 0.6926 - val_loss: 0.6501\n",
      "Epoch 231/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7450 - val_loss: 0.6501\n",
      "Epoch 232/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.7706 - val_loss: 0.6500\n",
      "Epoch 233/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 0.7342 - val_loss: 0.6500\n",
      "Epoch 234/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.7419 - val_loss: 0.6499\n",
      "Epoch 235/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.7165 - val_loss: 0.6498\n",
      "Epoch 236/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7210 - val_loss: 0.6498\n",
      "Epoch 237/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.7629 - val_loss: 0.6497\n",
      "Epoch 238/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 0.7696 - val_loss: 0.6497\n",
      "Epoch 239/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 0.7498 - val_loss: 0.6496\n",
      "Epoch 240/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.8941 - val_loss: 0.6496\n",
      "Epoch 241/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 1.0200 - val_loss: 0.6495\n",
      "Epoch 242/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - loss: 0.7013 - val_loss: 0.6494\n",
      "Epoch 243/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.8767 - val_loss: 0.6494\n",
      "Epoch 244/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.7449 - val_loss: 0.6494\n",
      "Epoch 245/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.7599 - val_loss: 0.6492\n",
      "Epoch 246/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.7174 - val_loss: 0.6492\n",
      "Epoch 247/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.7899 - val_loss: 0.6492\n",
      "Epoch 248/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.8421 - val_loss: 0.6491\n",
      "Epoch 249/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.8441 - val_loss: 0.6490\n",
      "Epoch 250/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 0.7702 - val_loss: 0.6490\n",
      "Epoch 251/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.7393 - val_loss: 0.6489\n",
      "Epoch 252/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.7404 - val_loss: 0.6489\n",
      "Epoch 253/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.7626 - val_loss: 0.6488\n",
      "Epoch 254/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.7947 - val_loss: 0.6487\n",
      "Epoch 255/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7522 - val_loss: 0.6487\n",
      "Epoch 256/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.8696 - val_loss: 0.6486\n",
      "Epoch 257/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.6999 - val_loss: 0.6485\n",
      "Epoch 258/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.8784 - val_loss: 0.6486\n",
      "Epoch 259/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.6938 - val_loss: 0.6484\n",
      "Epoch 260/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.8801 - val_loss: 0.6485\n",
      "Epoch 261/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.8054 - val_loss: 0.6484\n",
      "Epoch 262/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 0.8192 - val_loss: 0.6483\n",
      "Epoch 263/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.6937 - val_loss: 0.6482\n",
      "Epoch 264/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.7431 - val_loss: 0.6483\n",
      "Epoch 265/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.8025 - val_loss: 0.6482\n",
      "Epoch 266/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.7071 - val_loss: 0.6482\n",
      "Epoch 267/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 0.7730 - val_loss: 0.6480\n",
      "Epoch 268/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.7464 - val_loss: 0.6481\n",
      "Epoch 269/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.9232 - val_loss: 0.6480\n",
      "Epoch 270/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.8712 - val_loss: 0.6479\n",
      "Epoch 271/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7218 - val_loss: 0.6479\n",
      "Epoch 272/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.8181 - val_loss: 0.6478\n",
      "Epoch 273/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.8432 - val_loss: 0.6478\n",
      "Epoch 274/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.7231 - val_loss: 0.6477\n",
      "Epoch 275/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.7789 - val_loss: 0.6477\n",
      "Epoch 276/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.7241 - val_loss: 0.6477\n",
      "Epoch 277/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - loss: 0.7305 - val_loss: 0.6476\n",
      "Epoch 278/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 0.7902 - val_loss: 0.6475\n",
      "Epoch 279/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.8626 - val_loss: 0.6475\n",
      "Epoch 280/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7684 - val_loss: 0.6475\n",
      "Epoch 281/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.7248 - val_loss: 0.6475\n",
      "Epoch 282/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.7757 - val_loss: 0.6473\n",
      "Epoch 283/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.8241 - val_loss: 0.6473\n",
      "Epoch 284/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.7169 - val_loss: 0.6473\n",
      "Epoch 285/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.7334 - val_loss: 0.6473\n",
      "Epoch 286/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.8313 - val_loss: 0.6472\n",
      "Epoch 287/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.6947 - val_loss: 0.6472\n",
      "Epoch 288/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.7484 - val_loss: 0.6470\n",
      "Epoch 289/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.8634 - val_loss: 0.6471\n",
      "Epoch 290/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.7199 - val_loss: 0.6469\n",
      "Epoch 291/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.8383 - val_loss: 0.6469\n",
      "Epoch 292/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7562 - val_loss: 0.6468\n",
      "Epoch 293/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.7230 - val_loss: 0.6468\n",
      "Epoch 294/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.8725 - val_loss: 0.6467\n",
      "Epoch 295/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.7446 - val_loss: 0.6467\n",
      "Epoch 296/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.7435 - val_loss: 0.6466\n",
      "Epoch 297/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.7301 - val_loss: 0.6466\n",
      "Epoch 298/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.8231 - val_loss: 0.6466\n",
      "Epoch 299/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.7336 - val_loss: 0.6465\n",
      "Epoch 300/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 0.7286 - val_loss: 0.6464\n",
      "Epoch 301/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7531 - val_loss: 0.6464\n",
      "Epoch 302/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 0.7426 - val_loss: 0.6463\n",
      "Epoch 303/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.9969 - val_loss: 0.6462\n",
      "Epoch 304/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.7307 - val_loss: 0.6463\n",
      "Epoch 305/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.7565 - val_loss: 0.6462\n",
      "Epoch 306/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.8690 - val_loss: 0.6461\n",
      "Epoch 307/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.7175 - val_loss: 0.6460\n",
      "Epoch 308/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.7928 - val_loss: 0.6460\n",
      "Epoch 309/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.8195 - val_loss: 0.6459\n",
      "Epoch 310/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.6876 - val_loss: 0.6459\n",
      "Epoch 311/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - loss: 0.7440 - val_loss: 0.6458\n",
      "Epoch 312/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 0.7082 - val_loss: 0.6457\n",
      "Epoch 313/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.7249 - val_loss: 0.6457\n",
      "Epoch 314/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8039 - val_loss: 0.6457\n",
      "Epoch 315/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.7645 - val_loss: 0.6456\n",
      "Epoch 316/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.6983 - val_loss: 0.6456\n",
      "Epoch 317/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - loss: 0.7918 - val_loss: 0.6455\n",
      "Epoch 318/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.8740 - val_loss: 0.6455\n",
      "Epoch 319/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.8076 - val_loss: 0.6454\n",
      "Epoch 320/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.6749 - val_loss: 0.6454\n",
      "Epoch 321/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.7396 - val_loss: 0.6452\n",
      "Epoch 322/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.9262 - val_loss: 0.6451\n",
      "Epoch 323/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7576 - val_loss: 0.6451\n",
      "Epoch 324/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.9263 - val_loss: 0.6451\n",
      "Epoch 325/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.8161 - val_loss: 0.6450\n",
      "Epoch 326/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.7714 - val_loss: 0.6448\n",
      "Epoch 327/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - loss: 0.9353 - val_loss: 0.6448\n",
      "Epoch 328/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.6929 - val_loss: 0.6448\n",
      "Epoch 329/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 0.7029 - val_loss: 0.6447\n",
      "Epoch 330/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - loss: 0.9214 - val_loss: 0.6447\n",
      "Epoch 331/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.7898 - val_loss: 0.6446\n",
      "Epoch 332/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.7612 - val_loss: 0.6447\n",
      "Epoch 333/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.7156 - val_loss: 0.6446\n",
      "Epoch 334/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7849 - val_loss: 0.6446\n",
      "Epoch 335/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.7739 - val_loss: 0.6445\n",
      "Epoch 336/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.7306 - val_loss: 0.6445\n",
      "Epoch 337/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 0.6753 - val_loss: 0.6444\n",
      "Epoch 338/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.7487 - val_loss: 0.6444\n",
      "Epoch 339/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.7962 - val_loss: 0.6443\n",
      "Epoch 340/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.7310 - val_loss: 0.6444\n",
      "Epoch 341/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.7265 - val_loss: 0.6442\n",
      "Epoch 342/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.7668 - val_loss: 0.6443\n",
      "Epoch 343/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.7105 - val_loss: 0.6442\n",
      "Epoch 344/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7445 - val_loss: 0.6441\n",
      "Epoch 345/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.7058 - val_loss: 0.6441\n",
      "Epoch 346/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - loss: 0.7207 - val_loss: 0.6441\n",
      "Epoch 347/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.7473 - val_loss: 0.6440\n",
      "Epoch 348/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.8892 - val_loss: 0.6440\n",
      "Epoch 349/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.7405 - val_loss: 0.6440\n",
      "Epoch 350/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.7593 - val_loss: 0.6440\n",
      "Epoch 351/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - loss: 0.7213 - val_loss: 0.6439\n",
      "Epoch 352/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7968 - val_loss: 0.6440\n",
      "Epoch 353/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.7347 - val_loss: 0.6440\n",
      "Epoch 354/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7432 - val_loss: 0.6439\n",
      "Epoch 355/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.8914 - val_loss: 0.6439\n",
      "Epoch 356/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.8001 - val_loss: 0.6438\n",
      "Epoch 357/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.7778 - val_loss: 0.6438\n",
      "Epoch 358/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.8872 - val_loss: 0.6437\n",
      "Epoch 359/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.6662 - val_loss: 0.6437\n",
      "Epoch 360/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 0.7280 - val_loss: 0.6438\n",
      "Epoch 361/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.7317 - val_loss: 0.6437\n",
      "Epoch 362/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.8088 - val_loss: 0.6437\n",
      "Epoch 363/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7727 - val_loss: 0.6436\n",
      "Epoch 364/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.7230 - val_loss: 0.6436\n",
      "Epoch 365/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.7785 - val_loss: 0.6437\n",
      "Epoch 366/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.7527 - val_loss: 0.6436\n",
      "Epoch 367/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.8636 - val_loss: 0.6436\n",
      "Epoch 368/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.9272 - val_loss: 0.6436\n",
      "Epoch 369/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 0.8572 - val_loss: 0.6435\n",
      "Epoch 370/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.7224 - val_loss: 0.6435\n",
      "Epoch 371/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.7722 - val_loss: 0.6435\n",
      "Epoch 372/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.6778 - val_loss: 0.6435\n",
      "Epoch 373/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7715 - val_loss: 0.6435\n",
      "Epoch 374/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.7961 - val_loss: 0.6434\n",
      "Epoch 375/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.6800 - val_loss: 0.6434\n",
      "Epoch 376/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7794 - val_loss: 0.6434\n",
      "Epoch 377/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.6928 - val_loss: 0.6433\n",
      "Epoch 378/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.7929 - val_loss: 0.6433\n",
      "Epoch 379/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7750 - val_loss: 0.6433\n",
      "Epoch 380/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.8556 - val_loss: 0.6433\n",
      "Epoch 381/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 0.6975 - val_loss: 0.6433\n",
      "Epoch 382/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.8421 - val_loss: 0.6434\n",
      "Epoch 383/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.8841 - val_loss: 0.6433\n",
      "Epoch 384/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.6670 - val_loss: 0.6432\n",
      "Epoch 385/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.8787 - val_loss: 0.6432\n",
      "Epoch 386/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 0.7020 - val_loss: 0.6431\n",
      "Epoch 387/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.6811 - val_loss: 0.6432\n",
      "Epoch 388/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.9308 - val_loss: 0.6432\n",
      "Epoch 389/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.7801 - val_loss: 0.6431\n",
      "Epoch 390/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7929 - val_loss: 0.6432\n",
      "Epoch 391/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 0.9378 - val_loss: 0.6431\n",
      "Epoch 392/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.8132 - val_loss: 0.6431\n",
      "Epoch 393/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.7672 - val_loss: 0.6431\n",
      "Epoch 394/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.7350 - val_loss: 0.6431\n",
      "Epoch 395/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7925 - val_loss: 0.6431\n",
      "Epoch 396/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.7657 - val_loss: 0.6431\n",
      "Epoch 397/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.9479 - val_loss: 0.6430\n",
      "Epoch 398/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.8089 - val_loss: 0.6431\n",
      "Epoch 399/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.7681 - val_loss: 0.6430\n",
      "Epoch 400/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.7016 - val_loss: 0.6430\n",
      "Epoch 401/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.8803 - val_loss: 0.6430\n",
      "Epoch 402/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.7348 - val_loss: 0.6430\n",
      "Epoch 403/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.9016 - val_loss: 0.6430\n",
      "Epoch 404/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.8723 - val_loss: 0.6429\n",
      "Epoch 405/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7259 - val_loss: 0.6429\n",
      "Epoch 406/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.7051 - val_loss: 0.6429\n",
      "Epoch 407/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.7474 - val_loss: 0.6430\n",
      "Epoch 408/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.7529 - val_loss: 0.6430\n",
      "Epoch 409/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.7278 - val_loss: 0.6428\n",
      "Epoch 410/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.7037 - val_loss: 0.6428\n",
      "Epoch 411/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7989 - val_loss: 0.6428\n",
      "Epoch 412/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.6625 - val_loss: 0.6428\n",
      "Epoch 413/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7041 - val_loss: 0.6428\n",
      "Epoch 414/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.8431 - val_loss: 0.6428\n",
      "Epoch 415/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 0.7510 - val_loss: 0.6427\n",
      "Epoch 416/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.7875 - val_loss: 0.6427\n",
      "Epoch 417/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.8167 - val_loss: 0.6428\n",
      "Epoch 418/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.8320 - val_loss: 0.6427\n",
      "Epoch 419/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.7198 - val_loss: 0.6427\n",
      "Epoch 420/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.7318 - val_loss: 0.6427\n",
      "Epoch 421/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.7915 - val_loss: 0.6426\n",
      "Epoch 422/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7400 - val_loss: 0.6425\n",
      "Epoch 423/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.8282 - val_loss: 0.6425\n",
      "Epoch 424/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - loss: 0.7309 - val_loss: 0.6425\n",
      "Epoch 425/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 0.7398 - val_loss: 0.6425\n",
      "Epoch 426/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - loss: 0.8881 - val_loss: 0.6425\n",
      "Epoch 427/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.7122 - val_loss: 0.6425\n",
      "Epoch 428/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.8036 - val_loss: 0.6424\n",
      "Epoch 429/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.7552 - val_loss: 0.6424\n",
      "Epoch 430/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.8253 - val_loss: 0.6424\n",
      "Epoch 431/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.7682 - val_loss: 0.6423\n",
      "Epoch 432/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.7919 - val_loss: 0.6424\n",
      "Epoch 433/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.7167 - val_loss: 0.6423\n",
      "Epoch 434/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.7809 - val_loss: 0.6423\n",
      "Epoch 435/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.7173 - val_loss: 0.6423\n",
      "Epoch 436/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 0.7143 - val_loss: 0.6423\n",
      "Epoch 437/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.7236 - val_loss: 0.6422\n",
      "Epoch 438/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.7974 - val_loss: 0.6422\n",
      "Epoch 439/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.6779 - val_loss: 0.6422\n",
      "Epoch 440/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.8340 - val_loss: 0.6423\n",
      "Epoch 441/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7560 - val_loss: 0.6421\n",
      "Epoch 442/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.7642 - val_loss: 0.6421\n",
      "Epoch 443/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.6797 - val_loss: 0.6422\n",
      "Epoch 444/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.7490 - val_loss: 0.6421\n",
      "Epoch 445/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.7133 - val_loss: 0.6420\n",
      "Epoch 446/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.8960 - val_loss: 0.6421\n",
      "Epoch 447/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.8874 - val_loss: 0.6420\n",
      "Epoch 448/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.8533 - val_loss: 0.6421\n",
      "Epoch 449/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.9048 - val_loss: 0.6420\n",
      "Epoch 450/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.8038 - val_loss: 0.6421\n",
      "Epoch 451/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.8210 - val_loss: 0.6420\n",
      "Epoch 452/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.8045 - val_loss: 0.6419\n",
      "Epoch 453/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 0.8207 - val_loss: 0.6420\n",
      "Epoch 454/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.8226 - val_loss: 0.6420\n",
      "Epoch 455/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.7320 - val_loss: 0.6419\n",
      "Epoch 456/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.7447 - val_loss: 0.6421\n",
      "Epoch 457/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - loss: 0.6843 - val_loss: 0.6419\n",
      "Epoch 458/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.9405 - val_loss: 0.6419\n",
      "Epoch 459/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.7222 - val_loss: 0.6419\n",
      "Epoch 460/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.6999 - val_loss: 0.6419\n",
      "Epoch 461/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 0.7779 - val_loss: 0.6420\n",
      "Epoch 462/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.7361 - val_loss: 0.6420\n",
      "Epoch 463/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.8254 - val_loss: 0.6418\n",
      "Epoch 464/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.7054 - val_loss: 0.6419\n",
      "Epoch 465/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.8270 - val_loss: 0.6419\n",
      "Epoch 466/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.7238 - val_loss: 0.6419\n",
      "Epoch 467/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.7966 - val_loss: 0.6419\n",
      "Epoch 468/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 0.9473 - val_loss: 0.6418\n",
      "Epoch 469/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.7705 - val_loss: 0.6418\n",
      "Epoch 470/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 0.7332 - val_loss: 0.6419\n",
      "Epoch 471/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.7762 - val_loss: 0.6418\n",
      "Epoch 472/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.7273 - val_loss: 0.6418\n",
      "Epoch 473/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.7532 - val_loss: 0.6418\n",
      "Epoch 474/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.7468 - val_loss: 0.6418\n",
      "Epoch 475/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.7374 - val_loss: 0.6418\n",
      "Epoch 476/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.7122 - val_loss: 0.6417\n",
      "Epoch 477/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.7295 - val_loss: 0.6418\n",
      "Epoch 478/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 0.7507 - val_loss: 0.6417\n",
      "Epoch 479/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.7810 - val_loss: 0.6416\n",
      "Epoch 480/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.8291 - val_loss: 0.6417\n",
      "Epoch 481/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 0.8142 - val_loss: 0.6417\n",
      "Epoch 482/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 0.7565 - val_loss: 0.6417\n",
      "Epoch 483/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.7145 - val_loss: 0.6417\n",
      "Epoch 484/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.8303 - val_loss: 0.6416\n",
      "Epoch 485/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 0.6683 - val_loss: 0.6417\n",
      "Epoch 486/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.7244 - val_loss: 0.6417\n",
      "Epoch 487/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7887 - val_loss: 0.6416\n",
      "Epoch 488/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.6808 - val_loss: 0.6417\n",
      "Epoch 489/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.8220 - val_loss: 0.6416\n",
      "Epoch 490/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.8116 - val_loss: 0.6416\n",
      "Epoch 491/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.6807 - val_loss: 0.6416\n",
      "Epoch 492/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 0.8212 - val_loss: 0.6416\n",
      "Epoch 493/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.7196 - val_loss: 0.6415\n",
      "Epoch 494/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8308 - val_loss: 0.6416\n",
      "Epoch 495/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.7170 - val_loss: 0.6416\n",
      "Epoch 496/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 0.7621 - val_loss: 0.6415\n",
      "Epoch 497/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.7690 - val_loss: 0.6415\n",
      "Epoch 498/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.7234 - val_loss: 0.6415\n",
      "Epoch 499/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 0.8288 - val_loss: 0.6414\n",
      "Epoch 500/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - loss: 0.7630 - val_loss: 0.6414\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283us/step\n"
     ]
    }
   ],
   "source": [
    "evaluator = ExplanationModelEvaluator(clf, X_train, categorical_features)\n",
    "evaluator.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777488762318155"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.faithfullness_correlation(ShapTabularTreeWrapper, X_test.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating metric vectors and calculating correlation between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a few reanknings\n",
    "exps = [LimeWrapper(clf, X_train, categorical_features), ShapTabularTreeWrapper(clf, X_train, categorical_features), AnchorWrapper(clf, X_train, categorical_features)]\n",
    "\n",
    "indexes = np.random.choice(X_test.index, 100, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "comp_arr = [] # original, new1, new2, ...\n",
    "\n",
    "def process_index(idx):\n",
    "    local_comp_arr = []\n",
    "    for exp in exps:\n",
    "        row = []\n",
    "        explanation = exp.explain_instance(X_test.loc[idx])\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=False, , iterations=100))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=False, iterations=100))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"sum\", iterations=100))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"percentile\", iterations=100))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"avg\", iterations=100))\n",
    "        row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"inverse\", iterations=100))\n",
    "        \n",
    "        # row.append(evaluator.complexity(exp, X_test.loc[idx], explanation=explanation))\n",
    "        # row.append(evaluator.nrc(exp, X_test.loc[idx], explanation=explanation))\n",
    "        \n",
    "        local_comp_arr.append(row)\n",
    "    return local_comp_arr\n",
    "    \n",
    "with concurrent.futures.ProcessPoolExecutor(5) as executor:\n",
    "    results = list(executor.map(process_index, indexes))\n",
    "\n",
    "for result in results:\n",
    "    comp_arr.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.042115543204383134,\n",
       "  0.13445546806318182,\n",
       "  0.11436112086461492,\n",
       "  0.004476653799282627,\n",
       "  0.15830155924273018,\n",
       "  0.08761371994818586],\n",
       " [0.44438414286074734,\n",
       "  0.4905288579636767,\n",
       "  0.1846125996468011,\n",
       "  0.023305663290801505,\n",
       "  0.14152649503780784,\n",
       "  0.3781719792645691],\n",
       " [0.2421357931634488,\n",
       "  0.1809830944143485,\n",
       "  0.15659202459607105,\n",
       "  0.23378478210239495,\n",
       "  0.11329380276126018,\n",
       "  0.3280958031604405],\n",
       " [0.12546042965181842,\n",
       "  0.1007693722381217,\n",
       "  0.40954456566364494,\n",
       "  0.029202581414015372,\n",
       "  0.14651027960845947,\n",
       "  0.07926901605266641],\n",
       " [0.2851447656599366,\n",
       "  0.13393960528010046,\n",
       "  0.027752752562134365,\n",
       "  0.06822357388189883,\n",
       "  0.09415349021629964,\n",
       "  0.021604855129709645],\n",
       " [0.11642779312049328,\n",
       "  0.03500871204275638,\n",
       "  0.2001644266776521,\n",
       "  0.22853582645026913,\n",
       "  0.05705961365388699,\n",
       "  0.01448423244314698],\n",
       " [0.21770121545548302,\n",
       "  0.47430974073754356,\n",
       "  0.2583442218816916,\n",
       "  0.19753170323817715,\n",
       "  0.27416070244243596,\n",
       "  0.1788910881387213],\n",
       " [0.20520339537895843,\n",
       "  0.3445363710423437,\n",
       "  0.10199376862635487,\n",
       "  0.036885983740151555,\n",
       "  0.09270664437307352,\n",
       "  0.11248639046034867],\n",
       " [0.2836687710879901,\n",
       "  0.15488446816804033,\n",
       "  0.24215598021451767,\n",
       "  0.06871500383807416,\n",
       "  0.29295619083239127,\n",
       "  0.11480062240428174],\n",
       " [0.08381921852975381,\n",
       "  0.08149343613291507,\n",
       "  0.10005015696766693,\n",
       "  0.1914219678295207,\n",
       "  0.14126395314599655,\n",
       "  0.15079700173117977],\n",
       " [0.2517536136640489,\n",
       "  0.2917417630859982,\n",
       "  0.2467368749695671,\n",
       "  0.2259382083230213,\n",
       "  0.14630774720454154,\n",
       "  0.10871229906196009],\n",
       " [0.5985738708255517,\n",
       "  0.6123918846257413,\n",
       "  0.2556409524009391,\n",
       "  0.09155380676836652,\n",
       "  0.26249837434237144,\n",
       "  0.06418048567858986],\n",
       " [0.007889338145215967,\n",
       "  0.07921512687033731,\n",
       "  0.05084480662567736,\n",
       "  0.05320246029854901,\n",
       "  0.05788181249976647,\n",
       "  0.08083392872562184],\n",
       " [0.3318920514974812,\n",
       "  0.4186036707981875,\n",
       "  0.05700352689734274,\n",
       "  0.02983028459818733,\n",
       "  0.057967935155768084,\n",
       "  0.027623063390112966],\n",
       " [0.2343907460676299,\n",
       "  0.09932207671495204,\n",
       "  0.25004581564138273,\n",
       "  0.4206627734855656,\n",
       "  0.26118542867863415,\n",
       "  0.22607385315564785],\n",
       " [0.1442230784439033,\n",
       "  0.19174163716639403,\n",
       "  0.1995250059615404,\n",
       "  0.36966020459601573,\n",
       "  0.22123907485231714,\n",
       "  0.2506710097056663],\n",
       " [0.12403760802287077,\n",
       "  0.09541652603950222,\n",
       "  0.03440342304972164,\n",
       "  0.154501342352868,\n",
       "  0.22489447053336856,\n",
       "  0.09698590103586623],\n",
       " [0.07713946472799804,\n",
       "  0.20509445045065816,\n",
       "  0.11700809248596902,\n",
       "  0.10153417161098337,\n",
       "  0.006413955988825895,\n",
       "  0.2314873434620875],\n",
       " [0.535509336194356,\n",
       "  0.3348902693728628,\n",
       "  0.2568287212066055,\n",
       "  0.2694385052189207,\n",
       "  0.36258587873340936,\n",
       "  0.2630253348072617],\n",
       " [0.5111372086620011,\n",
       "  0.5322935232014224,\n",
       "  0.09299170718315163,\n",
       "  0.1541337020060848,\n",
       "  0.07764552588510622,\n",
       "  0.006135953055748211],\n",
       " [0.24725199347674398,\n",
       "  0.43149210711892316,\n",
       "  0.22385820906361703,\n",
       "  0.21086282285254446,\n",
       "  0.2838681833031095,\n",
       "  0.22717051361686813],\n",
       " [0.5079202755845929,\n",
       "  0.45159538239949104,\n",
       "  0.29507904370162685,\n",
       "  0.27990757332860355,\n",
       "  0.4601264276059913,\n",
       "  0.4623296343143499],\n",
       " [0.6092467403670233,\n",
       "  0.6912800978785277,\n",
       "  0.02611795994952626,\n",
       "  0.048488924459504645,\n",
       "  0.13844371108479647,\n",
       "  0.19809090271931873],\n",
       " [0.7642869915221391,\n",
       "  0.846893870603892,\n",
       "  0.6451049976226773,\n",
       "  0.7279789737415716,\n",
       "  0.749306427011057,\n",
       "  0.6873661464463989],\n",
       " [0.12911148710101794,\n",
       "  0.0750180794668413,\n",
       "  0.060199803807465785,\n",
       "  0.1222864408891936,\n",
       "  0.16446891364776797,\n",
       "  0.20193922151856608],\n",
       " [0.6752309598861319,\n",
       "  0.6734128070393268,\n",
       "  0.38918163301393005,\n",
       "  0.424190519572309,\n",
       "  0.24983634288709047,\n",
       "  0.1542732910796615],\n",
       " [0.25793749520755727,\n",
       "  0.4940321692632197,\n",
       "  0.4648395131875608,\n",
       "  0.2746635609884457,\n",
       "  0.2797502599217798,\n",
       "  0.4617473658476132],\n",
       " [0.7303982491879384,\n",
       "  0.728603567407418,\n",
       "  0.6176272612765918,\n",
       "  0.5361734586753197,\n",
       "  0.45998889235698726,\n",
       "  0.5831878186038167],\n",
       " [0.7150160933417109,\n",
       "  0.7179688561744423,\n",
       "  0.32121561793594006,\n",
       "  0.20181792739416518,\n",
       "  0.1389112091424639,\n",
       "  0.27461262543697357],\n",
       " [0.32995079014214945,\n",
       "  0.41831491506979485,\n",
       "  0.45786151180506574,\n",
       "  0.39166400404719615,\n",
       "  0.5395585868066806,\n",
       "  0.5697708981472314],\n",
       " [0.7154971903684024,\n",
       "  0.6476902029680638,\n",
       "  0.4893554560876473,\n",
       "  0.3305829746118639,\n",
       "  0.4921438750928517,\n",
       "  0.2616574336306936],\n",
       " [0.5626306818399001,\n",
       "  0.5937390884284164,\n",
       "  0.043331278562087086,\n",
       "  0.05766392482384202,\n",
       "  0.021926358808927723,\n",
       "  0.002514002555355402],\n",
       " [0.5659984271189001,\n",
       "  0.5609547920311378,\n",
       "  0.2916676001315335,\n",
       "  0.4460928749378933,\n",
       "  0.42020557404113246,\n",
       "  0.3697616447434381],\n",
       " [0.43462764150856553,\n",
       "  0.34618255831453565,\n",
       "  0.11690077051415734,\n",
       "  0.40325744858647955,\n",
       "  0.31285149165463416,\n",
       "  0.40415171884411666],\n",
       " [0.03892368545875329,\n",
       "  0.041420909899384926,\n",
       "  0.11818323209379915,\n",
       "  0.04396033283891777,\n",
       "  0.022356415509526967,\n",
       "  0.004898523446707333],\n",
       " [0.09379963138066055,\n",
       "  0.1780933883058856,\n",
       "  0.00960650420540934,\n",
       "  0.20887216585779628,\n",
       "  0.08177689611924917,\n",
       "  0.09132218810969278],\n",
       " [0.1630523101834605,\n",
       "  0.19238795284483004,\n",
       "  0.19834605056452692,\n",
       "  0.08781985060217964,\n",
       "  0.0018679152690358955,\n",
       "  0.12917344171881448],\n",
       " [0.11443269943126586,\n",
       "  0.07749178608554211,\n",
       "  0.24600335783655047,\n",
       "  0.1470769790465477,\n",
       "  0.10832204532328599,\n",
       "  0.1698409973554818],\n",
       " [0.17770415201080197,\n",
       "  0.02816024557495679,\n",
       "  0.10482446011116582,\n",
       "  0.12833100872910963,\n",
       "  0.2532787679344717,\n",
       "  0.031885201749129025],\n",
       " [0.6495560094949646,\n",
       "  0.5790957978151772,\n",
       "  0.1776999195805627,\n",
       "  0.14597323706674323,\n",
       "  0.10738100715725127,\n",
       "  0.30993681479397106],\n",
       " [0.2409335571192567,\n",
       "  0.3882431969077996,\n",
       "  0.021946511581888525,\n",
       "  0.12561549503928654,\n",
       "  0.05736472991336602,\n",
       "  0.012545397333677773],\n",
       " [0.40458171005636595,\n",
       "  0.46840540997553437,\n",
       "  0.30365944575601833,\n",
       "  0.4169278692201893,\n",
       "  0.2816017595732773,\n",
       "  0.2716786115197768],\n",
       " [0.7849208788070159,\n",
       "  0.8256245302401124,\n",
       "  0.6315398171465558,\n",
       "  0.5806169688512115,\n",
       "  0.5169919666795699,\n",
       "  0.5306668035935559],\n",
       " [0.8815156245076101,\n",
       "  0.8507456193052197,\n",
       "  0.543908623642077,\n",
       "  0.43059571266704655,\n",
       "  0.39313359548494486,\n",
       "  0.4678788148503585],\n",
       " [0.8913508041420715,\n",
       "  0.874518584388141,\n",
       "  0.8287211108358323,\n",
       "  0.8552841232506014,\n",
       "  0.8723517789684696,\n",
       "  0.8581303595314052],\n",
       " [0.7658384642708873,\n",
       "  0.6933053394346324,\n",
       "  0.37892386417522517,\n",
       "  0.5367057710910518,\n",
       "  0.5661207921058434,\n",
       "  0.36876915475446975],\n",
       " [0.7690096472672564,\n",
       "  0.6686207220749276,\n",
       "  0.49343016523193334,\n",
       "  0.39438741673156097,\n",
       "  0.361057902710216,\n",
       "  0.4311034679896556],\n",
       " [0.4446025804730289,\n",
       "  0.437062907114917,\n",
       "  0.4614622422283431,\n",
       "  0.37353111052777366,\n",
       "  0.6931543605681588,\n",
       "  0.3909010209314091],\n",
       " [0.1365252644994833,\n",
       "  0.1298475151401077,\n",
       "  0.2457646201133385,\n",
       "  0.01818012270282552,\n",
       "  0.2724963257363773,\n",
       "  0.11350888389030822],\n",
       " [0.6246905823859263,\n",
       "  0.692637182395866,\n",
       "  0.03422852047102112,\n",
       "  0.1868846208010026,\n",
       "  0.0980055524181412,\n",
       "  0.06345823400939216],\n",
       " [0.33114254376521773,\n",
       "  0.39807631569580815,\n",
       "  0.28701636423546706,\n",
       "  0.01955142842767756,\n",
       "  0.0035716089551149147,\n",
       "  0.23387240516384622],\n",
       " [0.22246522664556778,\n",
       "  0.3926547989642156,\n",
       "  0.24474292734778272,\n",
       "  0.2610291402474144,\n",
       "  0.10901788311349689,\n",
       "  0.08089997340493761],\n",
       " [0.5091377947791094,\n",
       "  0.40524700201960745,\n",
       "  0.2336085112116179,\n",
       "  0.11773947347594435,\n",
       "  0.03627725408647825,\n",
       "  0.1297940606097494],\n",
       " [0.24788882073319105,\n",
       "  0.3515084308749938,\n",
       "  0.021876468870863808,\n",
       "  0.08157842175848817,\n",
       "  0.1418121583724332,\n",
       "  0.17523082045258803],\n",
       " [0.6606300416143398,\n",
       "  0.49125228609522575,\n",
       "  0.062416761402786805,\n",
       "  0.4817849887141884,\n",
       "  0.3666343897933316,\n",
       "  0.009947471681207479],\n",
       " [0.6047145059199844,\n",
       "  0.5042598354809068,\n",
       "  0.15983265191715124,\n",
       "  0.36765449280220486,\n",
       "  0.25480775106716474,\n",
       "  0.11783349064382417],\n",
       " [0.37733485451859683,\n",
       "  0.4103224586286661,\n",
       "  0.16121763069198974,\n",
       "  0.33882923429157963,\n",
       "  0.21192158708571612,\n",
       "  0.11714601353541901],\n",
       " [0.20023398079231133,\n",
       "  0.2723685282406004,\n",
       "  0.028041160776645296,\n",
       "  0.10868497691468976,\n",
       "  0.1291820489113114,\n",
       "  0.021421517139673885],\n",
       " [0.3118607857831386,\n",
       "  0.19702353100510595,\n",
       "  0.1808827388794011,\n",
       "  0.22148750757142066,\n",
       "  0.2285269460766569,\n",
       "  0.11155415384049182],\n",
       " [0.07313950062043427,\n",
       "  0.0951715388601237,\n",
       "  0.04665514054908114,\n",
       "  0.27664653057943306,\n",
       "  0.03775875215236638,\n",
       "  0.1243685714620951],\n",
       " [0.8646659249426749,\n",
       "  0.8521275234553161,\n",
       "  0.5901930296044018,\n",
       "  0.41956445075659726,\n",
       "  0.6534295632262729,\n",
       "  0.6027524464393217],\n",
       " [0.7341155821302412,\n",
       "  0.7275980182769515,\n",
       "  0.2755462217231163,\n",
       "  0.17227224411437456,\n",
       "  0.2594620907589802,\n",
       "  0.31489590694831304],\n",
       " [0.6489624323928079,\n",
       "  0.6337360436312187,\n",
       "  0.4085808011800479,\n",
       "  0.7735967736496612,\n",
       "  0.6888163412022087,\n",
       "  0.632606074667884],\n",
       " [0.20479915455338255,\n",
       "  0.18190022543803303,\n",
       "  0.043565990571405616,\n",
       "  0.14672044843146098,\n",
       "  0.07249578203338766,\n",
       "  0.09423959151778964],\n",
       " [0.44619148202698766,\n",
       "  0.4522254836896049,\n",
       "  0.05410727337422405,\n",
       "  0.11819353519590055,\n",
       "  0.10647899961556698,\n",
       "  0.11406011986709794],\n",
       " [0.42708965806361376,\n",
       "  0.4180523298656924,\n",
       "  0.1452870565122268,\n",
       "  0.004717957179402973,\n",
       "  0.10373282439913462,\n",
       "  0.045064201681491486],\n",
       " [0.11473763623556418,\n",
       "  0.16182382390928746,\n",
       "  0.08661666014505814,\n",
       "  0.05335330025131412,\n",
       "  0.2297203614405416,\n",
       "  0.1869704949423183],\n",
       " [0.08478657990186537,\n",
       "  0.08782545928627328,\n",
       "  0.024666203592384722,\n",
       "  0.11153278237756196,\n",
       "  0.2683855094015086,\n",
       "  0.14118382577477287],\n",
       " [0.12996781340587898,\n",
       "  0.02670579263287301,\n",
       "  0.1394737762180785,\n",
       "  0.0753421418472777,\n",
       "  0.12008141338928982,\n",
       "  0.11887283867058202],\n",
       " [0.23013531951820082,\n",
       "  0.029655282817304354,\n",
       "  0.03425090528306156,\n",
       "  0.04152561669012193,\n",
       "  0.1868409125365441,\n",
       "  0.04260861818887374],\n",
       " [0.5162687416610618,\n",
       "  0.3346375728142749,\n",
       "  0.22484022452260055,\n",
       "  0.3170115343476885,\n",
       "  0.09700791185066654,\n",
       "  0.03286093971367624],\n",
       " [0.020255832560399117,\n",
       "  0.19324862549794966,\n",
       "  0.2220529377458028,\n",
       "  0.19791246014846828,\n",
       "  0.29076761092493175,\n",
       "  0.1880804015762578],\n",
       " [0.056516265416711,\n",
       "  0.09951773897185447,\n",
       "  0.11449912908472676,\n",
       "  0.07902073846955261,\n",
       "  0.1235126553966725,\n",
       "  0.08952099396826288],\n",
       " [0.2870207104496788,\n",
       "  0.25948740198998643,\n",
       "  0.03662470537612357,\n",
       "  0.027636671966171155,\n",
       "  0.06222869686739972,\n",
       "  0.017846373763382446],\n",
       " [0.0707728678811574,\n",
       "  0.06704688946794526,\n",
       "  0.06685028486083304,\n",
       "  0.0454218450871874,\n",
       "  0.008558397338087387,\n",
       "  0.2940291467501385],\n",
       " [0.1214333027552989,\n",
       "  0.16593208443883936,\n",
       "  0.2670308246612342,\n",
       "  0.06708460754545774,\n",
       "  0.25787619023142516,\n",
       "  0.04077416856217687],\n",
       " [0.3447175694306036,\n",
       "  0.3677085211997988,\n",
       "  0.06475914437552985,\n",
       "  0.0662105145850955,\n",
       "  0.11750241752525553,\n",
       "  0.01334196095048372],\n",
       " [0.3544079429103234,\n",
       "  0.4675703917443212,\n",
       "  0.27222654257354995,\n",
       "  0.42487755290736684,\n",
       "  0.3101593798927992,\n",
       "  0.191249787947813],\n",
       " [0.020049064702793412,\n",
       "  0.10092110726029183,\n",
       "  0.1901539769310287,\n",
       "  0.09380703479276609,\n",
       "  0.05564959855855299,\n",
       "  0.23483344801782352],\n",
       " [0.46149525225647153,\n",
       "  0.4441834374117449,\n",
       "  0.15885088952825438,\n",
       "  0.0759146311057837,\n",
       "  0.26837940901632407,\n",
       "  0.30975187745302313],\n",
       " [0.35746803479159533,\n",
       "  0.31171287135202785,\n",
       "  0.01910435920916098,\n",
       "  0.18957028433794548,\n",
       "  0.15191998547163996,\n",
       "  0.21182558536846607],\n",
       " [0.7432601460543629,\n",
       "  0.6425003818011561,\n",
       "  0.4384580836144226,\n",
       "  0.4487652938933682,\n",
       "  0.5036762478744927,\n",
       "  0.20031591452151531],\n",
       " [0.5276142216770036,\n",
       "  0.46167415450101784,\n",
       "  0.16422257072770985,\n",
       "  0.01557693648834433,\n",
       "  0.00949226281571404,\n",
       "  0.12008305038426932],\n",
       " [0.46310485507443566,\n",
       "  0.41339434123707675,\n",
       "  0.2800268442046316,\n",
       "  0.4532136530282265,\n",
       "  0.3108100372515285,\n",
       "  0.33327442349177083],\n",
       " [0.4429841986374803,\n",
       "  0.4464647259399321,\n",
       "  0.2534416975179235,\n",
       "  0.36298953340807016,\n",
       "  0.42996136305868654,\n",
       "  0.27442016815117115],\n",
       " [0.5326372088127922,\n",
       "  0.36069849424726164,\n",
       "  0.19952282287948875,\n",
       "  0.2572498152168079,\n",
       "  0.07158294156758474,\n",
       "  0.1256837726744239],\n",
       " [0.11728970591016097,\n",
       "  0.16779855622804235,\n",
       "  0.20500433892430425,\n",
       "  0.2533169684287587,\n",
       "  0.23525232370034874,\n",
       "  0.1409641431664308],\n",
       " [0.6900159220654343,\n",
       "  0.7603153633933829,\n",
       "  0.35306114805834754,\n",
       "  0.46126663993365463,\n",
       "  0.26111231588018613,\n",
       "  0.3363302141882282],\n",
       " [0.644911175787638,\n",
       "  0.6660764343680907,\n",
       "  0.16912491293523002,\n",
       "  0.26813782291917265,\n",
       "  0.32708704654636084,\n",
       "  0.08417223643757096],\n",
       " [0.5796211536598718,\n",
       "  0.4880216380038042,\n",
       "  0.4308204692650944,\n",
       "  0.32495778018155896,\n",
       "  0.5638762561074115,\n",
       "  0.43863284789691814],\n",
       " [0.2407476506461783,\n",
       "  0.1462318860685473,\n",
       "  0.12986021727778047,\n",
       "  0.11958912400730456,\n",
       "  0.07173060570068641,\n",
       "  0.00235044554959897],\n",
       " [0.4812905696731765,\n",
       "  0.5106268231823899,\n",
       "  0.007395639715742128,\n",
       "  0.28756344991212146,\n",
       "  0.01888510364987951,\n",
       "  0.06292841001447949],\n",
       " [0.4683963262127172,\n",
       "  0.5747688528297026,\n",
       "  0.43757714713295337,\n",
       "  0.43516175468606183,\n",
       "  0.39133118421548757,\n",
       "  0.5165055761615379],\n",
       " [0.048311927765964845,\n",
       "  0.06990648279844777,\n",
       "  0.04916201243585665,\n",
       "  0.1728886344373151,\n",
       "  0.10244700639025427,\n",
       "  0.11931720199630626],\n",
       " [0.2721529175144027,\n",
       "  0.20881535562227355,\n",
       "  0.27783410231345573,\n",
       "  0.12648493759637097,\n",
       "  0.2221979774362909,\n",
       "  0.04722524301233471],\n",
       " [0.4889779061523515,\n",
       "  0.5452815915620698,\n",
       "  0.306998627215611,\n",
       "  0.2940117642554768,\n",
       "  0.2298479885129222,\n",
       "  0.31893608789806166],\n",
       " [0.20189157889278042,\n",
       "  0.27152447745169817,\n",
       "  0.008732765365938672,\n",
       "  0.040195199266974194,\n",
       "  0.06788864025715323,\n",
       "  0.05096225147311777],\n",
       " [0.09472485677610207,\n",
       "  0.012590365531420546,\n",
       "  0.2775022520115711,\n",
       "  0.06061932215686055,\n",
       "  0.10042029479110606,\n",
       "  0.023808927274369528],\n",
       " [0.0623005068703187,\n",
       "  0.05379727865587121,\n",
       "  0.03577620495032573,\n",
       "  0.020767909892887286,\n",
       "  0.12567430505758906,\n",
       "  0.07167507923022443],\n",
       " [0.17367330431156414,\n",
       "  0.02403223061361706,\n",
       "  0.09290279860206924,\n",
       "  0.0271859733850991,\n",
       "  0.0398779735066509,\n",
       "  0.08463498977924445],\n",
       " [0.1692989541132662,\n",
       "  0.05427289460082975,\n",
       "  0.06222415526868688,\n",
       "  0.00842296464572325,\n",
       "  0.029495112289323562,\n",
       "  0.016421926613909732],\n",
       " [0.03138030473359418,\n",
       "  0.22288638470841748,\n",
       "  0.1110383123413393,\n",
       "  0.03827712182252533,\n",
       "  0.11134704663640957,\n",
       "  0.05345850904765452],\n",
       " [0.1206131872422962,\n",
       "  0.06050122759732543,\n",
       "  0.2438538057091857,\n",
       "  0.18708845461011522,\n",
       "  0.06418635777744217,\n",
       "  0.12828856964589858],\n",
       " [0.19786373217199651,\n",
       "  0.09620030171144572,\n",
       "  0.004853013638578964,\n",
       "  0.1476686656024353,\n",
       "  0.07696462527289129,\n",
       "  0.11987999560233001],\n",
       " [0.10938136925931871,\n",
       "  0.05196623045442206,\n",
       "  0.14753398535453668,\n",
       "  0.08403053260612836,\n",
       "  0.12253564762274152,\n",
       "  0.262198642696293],\n",
       " [0.1110277746972955,\n",
       "  0.293721851159202,\n",
       "  0.04043093908627182,\n",
       "  0.11613306219624608,\n",
       "  0.09703991286639427,\n",
       "  0.07282556352551534],\n",
       " [0.434596276409728,\n",
       "  0.22838102701154755,\n",
       "  0.05369812808981807,\n",
       "  0.06079571029519683,\n",
       "  0.07792485648088862,\n",
       "  0.052818543038149444],\n",
       " [0.30849840106215415,\n",
       "  0.30303186292845674,\n",
       "  0.023517423459073927,\n",
       "  0.05425837181514521,\n",
       "  0.12410529279610207,\n",
       "  0.18786415170269427],\n",
       " [0.47905179405282616,\n",
       "  0.6374439757105836,\n",
       "  0.5344246754832203,\n",
       "  0.43020936526047643,\n",
       "  0.5804352216379396,\n",
       "  0.3198050003317228],\n",
       " [0.09053586414598633,\n",
       "  0.007857692072660988,\n",
       "  0.18686182189723755,\n",
       "  0.21564838882721066,\n",
       "  0.2001369929581232,\n",
       "  0.03996352549519734],\n",
       " [0.32836985501992333,\n",
       "  0.3973482441398063,\n",
       "  0.16063556475647928,\n",
       "  0.38306439868872677,\n",
       "  0.2912684486226843,\n",
       "  0.26637727999856975],\n",
       " [0.2009040007163066,\n",
       "  0.17601271740619673,\n",
       "  0.05703611493735397,\n",
       "  0.0723442897252872,\n",
       "  0.23456522077624117,\n",
       "  0.08704033186999703],\n",
       " [0.4987399234564401,\n",
       "  0.6312845507091509,\n",
       "  0.14231529495819412,\n",
       "  0.1021567336648401,\n",
       "  0.027459180065601344,\n",
       "  0.14503731502446682],\n",
       " [0.5623424386242717,\n",
       "  0.6898384820394908,\n",
       "  0.2897752331832652,\n",
       "  0.38448665147494104,\n",
       "  0.36574919032793374,\n",
       "  0.3032836343655136],\n",
       " [0.512143835391591,\n",
       "  0.7265053376461997,\n",
       "  0.4387675055969966,\n",
       "  0.3015488707342982,\n",
       "  0.26832959212149815,\n",
       "  0.3906773192198735],\n",
       " [0.6549943056379873,\n",
       "  0.6579444226685707,\n",
       "  0.4222980376102019,\n",
       "  0.18357395900648982,\n",
       "  0.49530949359745413,\n",
       "  0.5737542454160383],\n",
       " [0.4381865105911077,\n",
       "  0.5472598719109989,\n",
       "  0.5335901562535847,\n",
       "  0.4723830561662161,\n",
       "  0.5327525700327761,\n",
       "  0.5188659825925792],\n",
       " [0.7542561957993629,\n",
       "  0.7508847081031561,\n",
       "  0.548693061573148,\n",
       "  0.37663352072453493,\n",
       "  0.6117073915194979,\n",
       "  0.5397556066425663],\n",
       " [0.7901591630624669,\n",
       "  0.8124843761564133,\n",
       "  0.18961382226752488,\n",
       "  0.5417296848327697,\n",
       "  0.15049942101154898,\n",
       "  0.34577859288408785],\n",
       " [0.5406821146131815,\n",
       "  0.6604105807896705,\n",
       "  0.4025386039812502,\n",
       "  0.59881096364337,\n",
       "  0.5208336361999257,\n",
       "  0.3749320463501981],\n",
       " [0.7699509472425187,\n",
       "  0.7016755430955404,\n",
       "  0.46058426875762015,\n",
       "  0.5542526148964757,\n",
       "  0.49184470468426833,\n",
       "  0.3945266315069709],\n",
       " [0.869488097496152,\n",
       "  0.8066240849886448,\n",
       "  0.4050307843185972,\n",
       "  0.5009816849981095,\n",
       "  0.4356569416687146,\n",
       "  0.39956173371287373],\n",
       " [0.7831268918720629,\n",
       "  0.7771725285465132,\n",
       "  0.4870226676394676,\n",
       "  0.547069594733693,\n",
       "  0.5346395327582805,\n",
       "  0.4849949578045545],\n",
       " [0.38453065973770617,\n",
       "  0.26594994389088233,\n",
       "  0.25175868272935664,\n",
       "  0.45302231825969486,\n",
       "  0.23874010039512483,\n",
       "  0.17956946116100933],\n",
       " [0.04855198354030167,\n",
       "  0.0021360075246492916,\n",
       "  0.07185982773346136,\n",
       "  0.21921216114418837,\n",
       "  0.05264171864925471,\n",
       "  0.09483273237209341],\n",
       " [0.20885425917587847,\n",
       "  0.0670081098872993,\n",
       "  0.2314067383576147,\n",
       "  0.1510345779492492,\n",
       "  0.18311008796780348,\n",
       "  0.18143330158206328],\n",
       " [0.643742428816841,\n",
       "  0.5939023655011751,\n",
       "  0.08980862489473848,\n",
       "  0.348522385408302,\n",
       "  0.368480525047011,\n",
       "  0.10536746627128081],\n",
       " [0.6744232005969429,\n",
       "  0.7463191201603994,\n",
       "  0.11460394906424196,\n",
       "  0.1431035781813674,\n",
       "  0.3639115632004866,\n",
       "  0.1968279507988221],\n",
       " [0.5326337497849303,\n",
       "  0.48150693015704893,\n",
       "  0.5276228389922586,\n",
       "  0.5127981518298443,\n",
       "  0.46233396999242393,\n",
       "  0.3115490257595716],\n",
       " [0.19679456346794844,\n",
       "  0.22326645075996293,\n",
       "  0.02678183822525599,\n",
       "  0.013191885434063876,\n",
       "  0.18727834383390585,\n",
       "  0.011386775227190157],\n",
       " [0.5645496468354334,\n",
       "  0.39757145418044854,\n",
       "  0.2622392480280467,\n",
       "  0.13626062590356844,\n",
       "  0.20529772944620972,\n",
       "  0.036898867007301696],\n",
       " [0.029601531733746286,\n",
       "  0.14284032913665562,\n",
       "  0.1922951337669523,\n",
       "  0.036654083077808625,\n",
       "  0.13729649716568115,\n",
       "  0.015634896270324744],\n",
       " [0.16223655240913326,\n",
       "  0.18487618304051257,\n",
       "  0.03058253534437215,\n",
       "  0.00927518573140021,\n",
       "  0.022932547534528606,\n",
       "  0.049051405847506224],\n",
       " [0.26313687683041087,\n",
       "  0.4817884770490748,\n",
       "  0.15263277802854153,\n",
       "  0.1831175217467986,\n",
       "  0.2100742655659822,\n",
       "  0.14751206104156298],\n",
       " [0.5268655994774724,\n",
       "  0.3982552472171628,\n",
       "  0.2404374807458719,\n",
       "  0.2825027415594151,\n",
       "  0.2193625303267251,\n",
       "  0.11052706495164812],\n",
       " [0.6723379672935779,\n",
       "  0.7141258491916059,\n",
       "  0.192664894937127,\n",
       "  0.2844682190708616,\n",
       "  0.36767040415698443,\n",
       "  0.4337330433092813],\n",
       " [0.4542195967594318,\n",
       "  0.5288562520575264,\n",
       "  0.04249195824090335,\n",
       "  0.24952231151688928,\n",
       "  0.1443965854960565,\n",
       "  0.19828874559034754],\n",
       " [0.7618938763350684,\n",
       "  0.7925655118176491,\n",
       "  0.5870340779521368,\n",
       "  0.6293617680463995,\n",
       "  0.5064782449732208,\n",
       "  0.6428051049770456],\n",
       " [0.7691182950168796,\n",
       "  0.66268333374461,\n",
       "  0.5685786339519018,\n",
       "  0.5166600893662736,\n",
       "  0.6198326400941938,\n",
       "  0.6214988352035385],\n",
       " [0.27598088417079064,\n",
       "  0.2220595494648162,\n",
       "  0.14464507342910699,\n",
       "  0.08057837608885415,\n",
       "  0.03636216441763936,\n",
       "  0.20248028090026043],\n",
       " [0.27228017518042824,\n",
       "  0.2417658975980931,\n",
       "  0.25625960290383387,\n",
       "  0.26409691694323634,\n",
       "  0.251970230464304,\n",
       "  0.23852639998534236],\n",
       " [0.08053882362375832,\n",
       "  0.0967785714669785,\n",
       "  0.06181054828353445,\n",
       "  0.07616097853652806,\n",
       "  0.1393337035978282,\n",
       "  0.09757743738805386],\n",
       " [0.122078966931071,\n",
       "  0.3304850416930342,\n",
       "  0.05658830995968653,\n",
       "  0.10478737151658732,\n",
       "  0.20740941371628088,\n",
       "  0.18094080885404507],\n",
       " [0.12245436498241946,\n",
       "  0.021912490252019107,\n",
       "  0.01136770488347563,\n",
       "  0.0024927563484668236,\n",
       "  0.11683579712133658,\n",
       "  0.028988307079363812],\n",
       " [0.1861193238334254,\n",
       "  0.2567925806906034,\n",
       "  0.00805549073920456,\n",
       "  0.11779715855420911,\n",
       "  0.06219166878511127,\n",
       "  0.06718198003542958],\n",
       " [0.3543156576148133,\n",
       "  0.392098276442769,\n",
       "  0.14555248575926508,\n",
       "  0.21215642066835064,\n",
       "  0.05620170283018692,\n",
       "  0.24194562439960277],\n",
       " [0.3090550548521428,\n",
       "  0.21847866564396987,\n",
       "  0.15355940617916708,\n",
       "  0.203579762000343,\n",
       "  0.22663024085251815,\n",
       "  0.3470782837301858],\n",
       " [0.7382193085890179,\n",
       "  0.7584474506725668,\n",
       "  0.5143172754519822,\n",
       "  0.4995197619898555,\n",
       "  0.5448776979306879,\n",
       "  0.40714175802753694],\n",
       " [0.5255931513120671,\n",
       "  0.620372263244394,\n",
       "  0.03683373356992266,\n",
       "  0.26701979188936437,\n",
       "  0.2773598931867201,\n",
       "  0.019758615665713915],\n",
       " [0.5666070219753742,\n",
       "  0.5446405099877759,\n",
       "  0.4516288911125267,\n",
       "  0.47459739325758005,\n",
       "  0.4391962069082037,\n",
       "  0.45361396996910375],\n",
       " [0.5599107459468811,\n",
       "  0.5783905622712098,\n",
       "  0.0269263108174074,\n",
       "  0.2802109817631166,\n",
       "  0.16372177834935656,\n",
       "  0.15291230560790275],\n",
       " [0.4053861624470495,\n",
       "  0.545627156868346,\n",
       "  0.14217487437289728,\n",
       "  0.1660769090926611,\n",
       "  0.014665103123759637,\n",
       "  0.19615269820962594],\n",
       " [0.49964545181177067,\n",
       "  0.4441233975080725,\n",
       "  0.2341242172964163,\n",
       "  0.3350346455101795,\n",
       "  0.32939126238054484,\n",
       "  0.2448226810849586],\n",
       " [0.32105570410331835,\n",
       "  0.2600823149162105,\n",
       "  0.16649730542641267,\n",
       "  0.20553179908311903,\n",
       "  0.3238713381834928,\n",
       "  0.296328525486646],\n",
       " [0.2410533376279453,\n",
       "  0.18567428106654454,\n",
       "  0.035680619286003384,\n",
       "  0.14804796767935424,\n",
       "  0.10885364322337004,\n",
       "  0.16733945653245202],\n",
       " [0.3230151005383398,\n",
       "  0.3169568602094103,\n",
       "  0.10296588289022274,\n",
       "  0.012336853065831446,\n",
       "  0.09623711146600485,\n",
       "  0.1340273297587112],\n",
       " [0.30946579715709177,\n",
       "  0.47405691819348084,\n",
       "  0.302261457004202,\n",
       "  0.10144737005166828,\n",
       "  0.20500579877072264,\n",
       "  0.1635127954082285],\n",
       " [0.33319085532309944,\n",
       "  0.40741417194174273,\n",
       "  0.0034728061684354133,\n",
       "  0.08270944849799114,\n",
       "  0.05880507852973817,\n",
       "  0.15027028377926988],\n",
       " [0.3516756058668643,\n",
       "  0.26370542999058055,\n",
       "  0.20234327108373265,\n",
       "  0.08221194483152029,\n",
       "  0.15716830690360645,\n",
       "  0.027680641195281255],\n",
       " [0.4517187835003589,\n",
       "  0.5629224225690792,\n",
       "  0.33136214752104864,\n",
       "  0.33923036255628447,\n",
       "  0.3431203156157473,\n",
       "  0.02003795716695337],\n",
       " [0.8205225684332034,\n",
       "  0.8171226306270606,\n",
       "  0.1939008136865427,\n",
       "  0.24407716088179704,\n",
       "  0.011320923131750031,\n",
       "  0.019510771980561385],\n",
       " [0.7674744360710134,\n",
       "  0.7520604927089625,\n",
       "  0.6806452634831741,\n",
       "  0.7099072309189647,\n",
       "  0.7709008375848148,\n",
       "  0.6795707692472869],\n",
       " [0.48841228036619594,\n",
       "  0.6194957836088917,\n",
       "  0.4861135110114875,\n",
       "  0.5738624010346274,\n",
       "  0.429881269438899,\n",
       "  0.38136312712189036],\n",
       " [0.14428248145909445,\n",
       "  0.09529605651954048,\n",
       "  0.08760694646067144,\n",
       "  0.04700670466224943,\n",
       "  0.19084661787324614,\n",
       "  0.17447981976849714],\n",
       " [0.2438364888778871,\n",
       "  0.26450774566664104,\n",
       "  0.13928078100673244,\n",
       "  0.3092683106962396,\n",
       "  0.1305859974671624,\n",
       "  0.050153133626238855],\n",
       " [0.3015302403432511,\n",
       "  0.31688461303076254,\n",
       "  0.20536987841512783,\n",
       "  0.0018601657446120912,\n",
       "  0.05718645096251108,\n",
       "  0.22209904985841117],\n",
       " [0.36915969432238893,\n",
       "  0.21958706403346218,\n",
       "  0.044056074234527044,\n",
       "  0.00357966168027965,\n",
       "  0.08592246813487944,\n",
       "  0.1366643321334055],\n",
       " [0.0704215662422262,\n",
       "  0.1950204012810142,\n",
       "  0.26916309917162184,\n",
       "  0.11898538308030637,\n",
       "  0.21597943224084448,\n",
       "  0.023801386085970128],\n",
       " [0.6329667760212281,\n",
       "  0.3778858178695734,\n",
       "  0.3973542151341593,\n",
       "  0.23910411714811558,\n",
       "  0.30332435945446856,\n",
       "  0.3013336947929011],\n",
       " [0.473890802224155,\n",
       "  0.44648415942414327,\n",
       "  0.15104783070283928,\n",
       "  0.0011324492644941703,\n",
       "  0.36789479117645874,\n",
       "  0.07735201717020805],\n",
       " [0.6957072219260532,\n",
       "  0.7226519934376909,\n",
       "  0.6128869438492175,\n",
       "  0.624266131661583,\n",
       "  0.6537912590040844,\n",
       "  0.5986922123464596],\n",
       " [0.5905984005912309,\n",
       "  0.718007592232417,\n",
       "  0.448814519438169,\n",
       "  0.38551516792680096,\n",
       "  0.41354103772539347,\n",
       "  0.42394031032663015],\n",
       " [0.659691657011235,\n",
       "  0.649858094143177,\n",
       "  0.28786274757666314,\n",
       "  0.14108347167301027,\n",
       "  0.15693546642832412,\n",
       "  0.14221193453643344],\n",
       " [0.5438286643604724,\n",
       "  0.5047748049610432,\n",
       "  0.4989050005173114,\n",
       "  0.27957409283642665,\n",
       "  0.5630371462279915,\n",
       "  0.5185584974744288],\n",
       " [0.06033070845956882,\n",
       "  0.011373655003323317,\n",
       "  0.1926462850421855,\n",
       "  0.1145242651303892,\n",
       "  0.21705207285607703,\n",
       "  0.01139888115622615],\n",
       " [0.06662748346709738,\n",
       "  0.036955943761982,\n",
       "  0.20371066412737188,\n",
       "  0.015577266341470504,\n",
       "  0.07493039742558744,\n",
       "  0.11452112408510319],\n",
       " [0.15212855404429515,\n",
       "  0.02300899862300767,\n",
       "  0.26505667945919,\n",
       "  0.17861002187743905,\n",
       "  0.3042041060916607,\n",
       "  0.09463306262318408],\n",
       " [0.03754460461506798,\n",
       "  0.05434964068437793,\n",
       "  0.15134556244788633,\n",
       "  0.0737004457385764,\n",
       "  0.055508784766248216,\n",
       "  0.022426084356642097],\n",
       " [0.2286555513746694,\n",
       "  0.20051176930037762,\n",
       "  0.03174834612377801,\n",
       "  0.06544715416661204,\n",
       "  0.1719620636222174,\n",
       "  0.044650700321674605],\n",
       " [0.12904831194452704,\n",
       "  0.1893455171531268,\n",
       "  0.21081454222252116,\n",
       "  0.16575445649217918,\n",
       "  0.19341676397884058,\n",
       "  0.12504726396580337],\n",
       " [0.15355124422331065,\n",
       "  0.0050481239415294255,\n",
       "  0.046586623225028094,\n",
       "  0.08197658240286623,\n",
       "  0.0007413764043346996,\n",
       "  0.05240870612984182],\n",
       " [0.017415525320825177,\n",
       "  0.026798432530792157,\n",
       "  0.08694787997472442,\n",
       "  0.07174358015993534,\n",
       "  0.18104335114705278,\n",
       "  0.12995486939119855],\n",
       " [0.17956873527715494,\n",
       "  0.03126085161236264,\n",
       "  0.22409153942691173,\n",
       "  0.29796447758009875,\n",
       "  0.3242558336478254,\n",
       "  0.20151016826568685],\n",
       " [0.317434755290011,\n",
       "  0.20747369033334218,\n",
       "  0.08803759366084175,\n",
       "  0.02294986476298462,\n",
       "  0.2070164625148609,\n",
       "  0.11100704136793371],\n",
       " [0.010204306343185643,\n",
       "  0.07168667290547442,\n",
       "  0.003485532239760128,\n",
       "  0.021176674313139812,\n",
       "  0.022372055725788313,\n",
       "  0.09354221426346127],\n",
       " [0.15212894520891124,\n",
       "  0.18166375072805846,\n",
       "  0.18996296937294724,\n",
       "  0.14024663263787726,\n",
       "  0.16412152146712292,\n",
       "  0.0752541700929685],\n",
       " [0.571871454223758,\n",
       "  0.6267358687642952,\n",
       "  0.5926453971152461,\n",
       "  0.42652443793670336,\n",
       "  0.5550526191278982,\n",
       "  0.5391486223345179],\n",
       " [0.6975916630452208,\n",
       "  0.7676170444818537,\n",
       "  0.1859892459195915,\n",
       "  0.30702957813939635,\n",
       "  0.14320336905810427,\n",
       "  0.22788609168279983],\n",
       " [0.5134192664544148,\n",
       "  0.5829605657811809,\n",
       "  0.4616369064623523,\n",
       "  0.5422759301591534,\n",
       "  0.45549289615626787,\n",
       "  0.36649909504636385],\n",
       " [0.7554837476516161,\n",
       "  0.7229267278989897,\n",
       "  0.5828549673808527,\n",
       "  0.47211740286795756,\n",
       "  0.3718551314243982,\n",
       "  0.34472246304425797],\n",
       " [0.8675655136633287,\n",
       "  0.8821031985342902,\n",
       "  0.5866182113051959,\n",
       "  0.4893469646078366,\n",
       "  0.38718379076389653,\n",
       "  0.3211753876224043],\n",
       " [0.5208632242519579,\n",
       "  0.7494180763407301,\n",
       "  0.5475374876383947,\n",
       "  0.5009198856009273,\n",
       "  0.47726308421431685,\n",
       "  0.5321024227071152],\n",
       " [0.21047502198962495,\n",
       "  0.06744176066104772,\n",
       "  0.0786110603814408,\n",
       "  0.19347243972335026,\n",
       "  0.030464662518199043,\n",
       "  0.001054654120563242],\n",
       " [0.42282106861727964,\n",
       "  0.45304893558310505,\n",
       "  0.021166996933395535,\n",
       "  4.459691263686283e-05,\n",
       "  0.05011095175021302,\n",
       "  0.1584186767196053],\n",
       " [0.1160154355258497,\n",
       "  0.08523520091846089,\n",
       "  0.1216888897325211,\n",
       "  0.18501391838216769,\n",
       "  0.14772732752254175,\n",
       "  0.016313727505845945],\n",
       " [0.5991980098187266,\n",
       "  0.5392422654673399,\n",
       "  0.5089874093719662,\n",
       "  0.5205847409004193,\n",
       "  0.4393280816948437,\n",
       "  0.370393795274289],\n",
       " [0.10034214187850227,\n",
       "  0.02495087063648093,\n",
       "  0.22564162081443856,\n",
       "  0.007243925441901146,\n",
       "  0.09668901093133012,\n",
       "  0.16660754265031608],\n",
       " [0.6848475232922423,\n",
       "  0.649257778873124,\n",
       "  0.4096974140557986,\n",
       "  0.48839038072078245,\n",
       "  0.3994126205625752,\n",
       "  0.26254827620534604],\n",
       " [0.684891930382781,\n",
       "  0.7371181871533434,\n",
       "  0.308848294058059,\n",
       "  0.44637634854041663,\n",
       "  0.3883222055826244,\n",
       "  0.21277158314109262],\n",
       " [0.629466450534489,\n",
       "  0.4592703770683033,\n",
       "  0.07169307229988263,\n",
       "  0.08849899186573155,\n",
       "  0.3555834094479587,\n",
       "  0.08571470625611738],\n",
       " [0.8114904086180801,\n",
       "  0.737459729421443,\n",
       "  0.8257596044500136,\n",
       "  0.7960640906565031,\n",
       "  0.6637550747078818,\n",
       "  0.8140400870191296],\n",
       " [0.8006644802364014,\n",
       "  0.6462957014838224,\n",
       "  0.5031373020659242,\n",
       "  0.5510248581818946,\n",
       "  0.4633390877887366,\n",
       "  0.2923215285152392],\n",
       " [0.6993702771330085,\n",
       "  0.7848291690758604,\n",
       "  0.31590897832093523,\n",
       "  0.07780440433152813,\n",
       "  0.36150631541041134,\n",
       "  0.09964089947165533],\n",
       " [0.11704903104100685,\n",
       "  0.30859757863541504,\n",
       "  0.34510502554838673,\n",
       "  0.31319748704968364,\n",
       "  0.355349708248621,\n",
       "  0.22764510138953264],\n",
       " [0.5041743533391225,\n",
       "  0.5127913938313151,\n",
       "  0.27012798480243766,\n",
       "  0.29232349333555374,\n",
       "  0.3775909867127219,\n",
       "  0.249331863199053],\n",
       " [0.4137380108831642,\n",
       "  0.35544002460621005,\n",
       "  0.22477376547116104,\n",
       "  0.022435737683179885,\n",
       "  0.16958466630860877,\n",
       "  0.1018617663700203],\n",
       " [0.681712031086656,\n",
       "  0.6823142616158555,\n",
       "  0.39492668164158495,\n",
       "  0.4929821196553138,\n",
       "  0.5088435044719617,\n",
       "  0.4709201437154094],\n",
       " [0.35143078847647247,\n",
       "  0.23376945208620037,\n",
       "  0.0783522481277212,\n",
       "  0.12084172774219941,\n",
       "  0.10535838312666607,\n",
       "  0.3046886751092812],\n",
       " [0.4860041765429284,\n",
       "  0.5123384991223805,\n",
       "  0.2583651126511654,\n",
       "  0.04495114295878001,\n",
       "  0.0908821409552155,\n",
       "  0.04533335666243385],\n",
       " [0.3562946966280529,\n",
       "  0.13813728546523157,\n",
       "  0.015194569568929315,\n",
       "  0.03918538196652972,\n",
       "  0.013164208894871837,\n",
       "  0.12223309749692617],\n",
       " [0.09793120366077049,\n",
       "  0.14622900727472132,\n",
       "  0.01847492735742033,\n",
       "  0.1336541853616111,\n",
       "  0.04009839121307801,\n",
       "  0.12250346346010677],\n",
       " [0.09106536600832713,\n",
       "  0.1238416514943771,\n",
       "  0.049133605603304834,\n",
       "  0.06474683317053015,\n",
       "  0.08880830725883737,\n",
       "  0.015685683589107918],\n",
       " [0.04825030644087904,\n",
       "  0.019305355631910763,\n",
       "  0.1506100042895609,\n",
       "  0.09056384544986523,\n",
       "  0.17596293701287435,\n",
       "  0.10105856932668111],\n",
       " [0.4989577652326822,\n",
       "  0.2852311452999935,\n",
       "  0.2696088063621079,\n",
       "  0.29165171435285003,\n",
       "  0.19082793649378024,\n",
       "  0.3546713935619161],\n",
       " [0.3830609530216551,\n",
       "  0.42707058572200274,\n",
       "  0.008430374082855463,\n",
       "  0.008100389286957124,\n",
       "  0.061614044241771124,\n",
       "  0.04479418964782813],\n",
       " [0.2830712176851169,\n",
       "  0.3964966707506525,\n",
       "  0.506016955710557,\n",
       "  0.4826327723892779,\n",
       "  0.4579675015736565,\n",
       "  0.45541878049637574],\n",
       " [0.46758614108807417,\n",
       "  0.5124253167625559,\n",
       "  0.2313397064641238,\n",
       "  0.2689979429258609,\n",
       "  0.2884458935072883,\n",
       "  0.23419899134823977],\n",
       " [0.49620373188881794,\n",
       "  0.534548792020624,\n",
       "  0.07826846765469488,\n",
       "  0.12747942068464405,\n",
       "  0.015137249320481667,\n",
       "  0.13863363278252638],\n",
       " [0.043857676758525796,\n",
       "  0.16421564140122466,\n",
       "  0.13972992793872857,\n",
       "  0.18408210431589683,\n",
       "  0.12328040455046667,\n",
       "  0.16549161729453793],\n",
       " [0.41702825698978335,\n",
       "  0.23745002254715147,\n",
       "  0.23595060246261707,\n",
       "  0.11681406452355116,\n",
       "  0.17896527924840605,\n",
       "  0.13557124212547242],\n",
       " [0.08893923825626254,\n",
       "  0.18819473885859403,\n",
       "  0.0167338101684857,\n",
       "  0.26179725716183727,\n",
       "  0.11626272037142002,\n",
       "  0.045302200021341194],\n",
       " [0.12835816056452182,\n",
       "  0.026546006013132985,\n",
       "  0.029869982961562364,\n",
       "  0.12190183535501811,\n",
       "  0.09502706911718914,\n",
       "  0.053209830218308524],\n",
       " [0.2641340904152053,\n",
       "  0.2648430406192558,\n",
       "  0.20170415360667607,\n",
       "  0.29984843424974766,\n",
       "  0.27861029400143617,\n",
       "  0.22580708784039943],\n",
       " [0.2242702896534529,\n",
       "  0.3202217049988437,\n",
       "  0.1598444733878735,\n",
       "  0.1679943065417075,\n",
       "  0.11962072556203662,\n",
       "  0.025787310474800755],\n",
       " [0.03582483220754264,\n",
       "  0.020203830893066835,\n",
       "  0.0040109013637081616,\n",
       "  0.030871496389741838,\n",
       "  0.06956129209314761,\n",
       "  0.01675371800495143],\n",
       " [0.1757635280920102,\n",
       "  0.23003172609080008,\n",
       "  0.07036678131288418,\n",
       "  0.04249205697551876,\n",
       "  0.0725948689004095,\n",
       "  0.1275027269293268],\n",
       " [0.44431127253717095,\n",
       "  0.4851753474423775,\n",
       "  0.11445529340424314,\n",
       "  0.17189392721828906,\n",
       "  0.015043963091960003,\n",
       "  0.09016378307247777],\n",
       " [0.3030155796945928,\n",
       "  0.5476999433428644,\n",
       "  0.46063529166705747,\n",
       "  0.3883573763959999,\n",
       "  0.45191106416624516,\n",
       "  0.08025147239481176],\n",
       " [0.2918643576097921,\n",
       "  0.3476577171637422,\n",
       "  0.25707530865414696,\n",
       "  0.16665877380651664,\n",
       "  0.2875498273727396,\n",
       "  0.14122049374677342],\n",
       " [0.32880549370946116,\n",
       "  0.17954407115831353,\n",
       "  0.1283862671835708,\n",
       "  0.15119091228111178,\n",
       "  0.04681871049772646,\n",
       "  0.15596740023319056],\n",
       " [0.22192449553985566,\n",
       "  0.4286025144319979,\n",
       "  0.4238882505355904,\n",
       "  0.47144128246773165,\n",
       "  0.42834302001548924,\n",
       "  0.45267276052876304],\n",
       " [0.06504762826398557,\n",
       "  0.09573303187220389,\n",
       "  0.11980711535448343,\n",
       "  0.05176573039220926,\n",
       "  0.16329281313669375,\n",
       "  0.13641087653772177],\n",
       " [0.6439852763451117,\n",
       "  0.5385880355404301,\n",
       "  0.25106505536390783,\n",
       "  0.48533627108613864,\n",
       "  0.36909635013895375,\n",
       "  0.25311794326150666],\n",
       " [0.1568573390304147,\n",
       "  0.03369361912690749,\n",
       "  0.06159024559512182,\n",
       "  0.1947096363109016,\n",
       "  0.07192630376783299,\n",
       "  0.03379330490602537],\n",
       " [0.5798637925380191,\n",
       "  0.4849203114825569,\n",
       "  0.3371408164412619,\n",
       "  0.2525545630477827,\n",
       "  0.3730430630374635,\n",
       "  0.21667784947686963],\n",
       " [0.1813366035199465,\n",
       "  0.2382996892570079,\n",
       "  0.09702827101380182,\n",
       "  0.1121849480448044,\n",
       "  0.02018092772911942,\n",
       "  0.08120650346711801],\n",
       " [0.4275148460504327,\n",
       "  0.22034313684858636,\n",
       "  0.15772530791434738,\n",
       "  0.2173083539419093,\n",
       "  0.23307092555648037,\n",
       "  0.1068274739069216],\n",
       " [0.08648420142820647,\n",
       "  0.16001930499375006,\n",
       "  0.21692299011950186,\n",
       "  0.05565889652938516,\n",
       "  0.1389173519794272,\n",
       "  0.08907087635856477],\n",
       " [0.4595122883362569,\n",
       "  0.4371850307503733,\n",
       "  0.04296244954696944,\n",
       "  0.12498173688933731,\n",
       "  0.1658370917127565,\n",
       "  0.1602742566892852],\n",
       " [0.48915673631508627,\n",
       "  0.39806836554773634,\n",
       "  0.25220764560957387,\n",
       "  0.15493614852562032,\n",
       "  0.07028732517597874,\n",
       "  0.0015545968068139612],\n",
       " [0.7033479368261609,\n",
       "  0.6741301912823278,\n",
       "  0.44244009760455805,\n",
       "  0.38519932684396463,\n",
       "  0.4083413722456791,\n",
       "  0.34101851291030044],\n",
       " [0.5519265973765918,\n",
       "  0.4418235771686401,\n",
       "  0.1566701561112755,\n",
       "  0.022577084852746683,\n",
       "  0.2961240862498104,\n",
       "  0.23809606327332253],\n",
       " [0.5916666936681807,\n",
       "  0.736015222569243,\n",
       "  0.6414338453220108,\n",
       "  0.5988877272635802,\n",
       "  0.6084606797189355,\n",
       "  0.655221912385447],\n",
       " [0.3451962049885839,\n",
       "  0.11479822318692148,\n",
       "  0.1996105693956967,\n",
       "  0.29438990570618184,\n",
       "  0.22188902202384617,\n",
       "  0.1191106067080979],\n",
       " [0.0790457510356319,\n",
       "  0.07172961899264443,\n",
       "  0.13164794097783833,\n",
       "  0.09044019894006307,\n",
       "  0.1614523850947926,\n",
       "  0.08908414875183024],\n",
       " [0.04262845149177903,\n",
       "  0.026941346466196954,\n",
       "  0.1530676405836484,\n",
       "  0.04177019105000547,\n",
       "  0.08175558100808518,\n",
       "  0.001923991850497242],\n",
       " [0.09420611188058264,\n",
       "  0.22302280653036655,\n",
       "  0.2735068945774781,\n",
       "  0.016471236397476607,\n",
       "  0.15397968437372883,\n",
       "  0.07828950060648962],\n",
       " [0.1513184733225606,\n",
       "  0.15033106122605222,\n",
       "  0.1776385981568718,\n",
       "  0.02627867865979253,\n",
       "  0.2905396875719918,\n",
       "  0.17580222755013553],\n",
       " [0.12235218507071915,\n",
       "  0.10176726766173023,\n",
       "  0.011276323575239174,\n",
       "  0.11657589697276576,\n",
       "  0.07356335948111668,\n",
       "  0.07693292589185632],\n",
       " [0.6876026607578272,\n",
       "  0.7329426468434864,\n",
       "  0.2728649755873257,\n",
       "  0.3239273805639112,\n",
       "  0.3199569241674617,\n",
       "  0.3813732914561977],\n",
       " [0.7324849439113666,\n",
       "  0.7912405423799344,\n",
       "  0.428080750617435,\n",
       "  0.5340460001203704,\n",
       "  0.559805181302359,\n",
       "  0.302370536078979],\n",
       " [0.6282606237220681,\n",
       "  0.5098422040455854,\n",
       "  0.5177790347665996,\n",
       "  0.47326649509839497,\n",
       "  0.48509178146362425,\n",
       "  0.4261456125508353],\n",
       " [0.06744167947261996,\n",
       "  0.16379841148739194,\n",
       "  0.05416934282528844,\n",
       "  0.18515444204152767,\n",
       "  0.14581842664386413,\n",
       "  0.20497714883914855],\n",
       " [0.4417435909014019,\n",
       "  0.43314981411829734,\n",
       "  0.04616845388459169,\n",
       "  0.11822696351560219,\n",
       "  0.1766355635904492,\n",
       "  0.035959358751747356],\n",
       " [0.2818672667529768,\n",
       "  0.3008978450989711,\n",
       "  0.2687417611465145,\n",
       "  0.27260780101484045,\n",
       "  0.2104554213263345,\n",
       "  0.22277953676172862],\n",
       " [0.619046980839115,\n",
       "  0.7469009849169932,\n",
       "  0.5492095724443293,\n",
       "  0.13318293377107984,\n",
       "  0.5639324949186859,\n",
       "  0.5625320690964644],\n",
       " [0.5488209897030818,\n",
       "  0.5551318113927,\n",
       "  0.0498621268604606,\n",
       "  0.14514911900555247,\n",
       "  0.13537923737117685,\n",
       "  0.016999741607947476],\n",
       " [0.10766591400362248,\n",
       "  0.2951181342580146,\n",
       "  0.43816968434143133,\n",
       "  0.4821095809187752,\n",
       "  0.4532233038155483,\n",
       "  0.37946582404134177],\n",
       " [0.7403707935866986,\n",
       "  0.6976839388865638,\n",
       "  0.597348472610046,\n",
       "  0.4718205462489946,\n",
       "  0.5682556846468428,\n",
       "  0.5285272132575081],\n",
       " [0.7424457730789755,\n",
       "  0.7557531430289882,\n",
       "  0.4087165378534474,\n",
       "  0.15955430324606262,\n",
       "  0.3067106949789005,\n",
       "  0.46752629706808035],\n",
       " [0.8544564538278494,\n",
       "  0.8814214929015708,\n",
       "  0.7754745166551535,\n",
       "  0.7731815060443972,\n",
       "  0.8288683120991299,\n",
       "  0.7109138654147036],\n",
       " [0.1732272000949166,\n",
       "  0.09644833931454627,\n",
       "  0.057202769750359865,\n",
       "  0.03023367311690951,\n",
       "  0.11409307308858835,\n",
       "  0.10207082872909332],\n",
       " [0.7220547739506699,\n",
       "  0.6455837704282763,\n",
       "  0.14981586097716895,\n",
       "  0.25216583747045007,\n",
       "  0.3721082412406029,\n",
       "  0.3869562904408063],\n",
       " [0.44331514539867284,\n",
       "  0.28754615145128193,\n",
       "  0.3300952530891549,\n",
       "  0.2710586094292413,\n",
       "  0.18339909717384967,\n",
       "  0.18578881366051617],\n",
       " [0.337869594698706,\n",
       "  0.3861484839682109,\n",
       "  0.09815499795183606,\n",
       "  0.29753634283544256,\n",
       "  0.16336190871920908,\n",
       "  0.30325095140941866],\n",
       " [0.17864691999236557,\n",
       "  0.17881145712312496,\n",
       "  0.0540867098759063,\n",
       "  0.2225471205798616,\n",
       "  0.008820531965578722,\n",
       "  0.05237383697700829],\n",
       " [0.03338000546709627,\n",
       "  0.08867542815275954,\n",
       "  0.010094231781589408,\n",
       "  0.04621871506601683,\n",
       "  0.12263549627943171,\n",
       "  0.2958119667102515],\n",
       " [0.032643198370322354,\n",
       "  0.20082433831302554,\n",
       "  0.27283660702505247,\n",
       "  0.10843673547373982,\n",
       "  0.1706109383547401,\n",
       "  0.0855169569222567],\n",
       " [0.1305720936297754,\n",
       "  0.09737829391937802,\n",
       "  0.12574722623550166,\n",
       "  0.04708112512432665,\n",
       "  0.0389633558807359,\n",
       "  0.0008674032024249129],\n",
       " [0.026123987550722046,\n",
       "  0.1262218901095014,\n",
       "  0.22045717805293946,\n",
       "  0.0341871373001424,\n",
       "  0.026730214883453264,\n",
       "  0.10983965709552879],\n",
       " [0.3190137949240094,\n",
       "  0.4396250043929146,\n",
       "  0.03524304288181402,\n",
       "  0.37622252684244173,\n",
       "  0.24268535985861972,\n",
       "  0.18502063975272862],\n",
       " [0.14221726688756767,\n",
       "  0.2197916881168333,\n",
       "  0.0882853294788146,\n",
       "  0.20806999525785114,\n",
       "  0.05765138562324236,\n",
       "  0.3887662786996604],\n",
       " [0.3437964592252861,\n",
       "  0.20962578681524088,\n",
       "  0.1514036882434408,\n",
       "  0.26398811702949077,\n",
       "  0.15232576644207876,\n",
       "  0.006465188803413548],\n",
       " [0.7084159085886346,\n",
       "  0.6526373296949983,\n",
       "  0.42844429321636923,\n",
       "  0.4573571466591986,\n",
       "  0.4749044695798615,\n",
       "  0.2034578012530095],\n",
       " [0.3322386275585464,\n",
       "  0.5818290136138424,\n",
       "  0.11966410900390381,\n",
       "  0.08380977415387504,\n",
       "  0.12774402241091712,\n",
       "  0.08234767184442734],\n",
       " [0.6732081576684086,\n",
       "  0.5625915059438076,\n",
       "  0.6131511099190138,\n",
       "  0.5790038604454266,\n",
       "  0.4777583067274808,\n",
       "  0.585260872932366],\n",
       " [0.6392122301381565,\n",
       "  0.47964439146596916,\n",
       "  0.19487645954622346,\n",
       "  0.11062848406448252,\n",
       "  0.34171362925689625,\n",
       "  0.28733902289943514],\n",
       " [0.3291866905235471,\n",
       "  0.2376792918695072,\n",
       "  0.2543047990353907,\n",
       "  0.03427201174409649,\n",
       "  0.272197623782106,\n",
       "  0.28846612914040604],\n",
       " [0.14643241019431513,\n",
       "  0.011859238792049812,\n",
       "  0.0052362178734853895,\n",
       "  0.05498796633066794,\n",
       "  0.057879664669901375,\n",
       "  0.07598215896403852],\n",
       " [0.38045018555686994,\n",
       "  0.16578553508704993,\n",
       "  0.09989651043716799,\n",
       "  0.13447646707703168,\n",
       "  0.04480459575718354,\n",
       "  0.025634128588373023],\n",
       " [0.5283514234667597,\n",
       "  0.5292284638914617,\n",
       "  0.053580588627845954,\n",
       "  0.05820495494028227,\n",
       "  0.04058462962986217,\n",
       "  0.1781886028972585],\n",
       " [0.3572958291760876,\n",
       "  0.3778379303662948,\n",
       "  0.05149537696048992,\n",
       "  0.04489523143796794,\n",
       "  0.09997386813259902,\n",
       "  0.03495926267490937],\n",
       " [0.5467502116689851,\n",
       "  0.532933328987836,\n",
       "  0.30095619945457047,\n",
       "  0.24904094101947769,\n",
       "  0.32489729710489423,\n",
       "  0.32037330515693496],\n",
       " [0.6371391752433582,\n",
       "  0.6523896982031503,\n",
       "  0.18368895051084605,\n",
       "  0.13832106382518533,\n",
       "  0.1628263486234602,\n",
       "  0.03253207299623391],\n",
       " [0.19673114070748166,\n",
       "  0.21325493673696277,\n",
       "  0.3022447991792495,\n",
       "  0.4285641271184188,\n",
       "  0.4529941923716749,\n",
       "  0.31977798637015703],\n",
       " [0.8243192070425223,\n",
       "  0.8404248937565123,\n",
       "  0.6889789444255265,\n",
       "  0.6332723911496095,\n",
       "  0.6797611046734169,\n",
       "  0.4383148402993645],\n",
       " [0.8032057917020736,\n",
       "  0.880076950739269,\n",
       "  0.7388129565231012,\n",
       "  0.6724498717122235,\n",
       "  0.5907804481116491,\n",
       "  0.5730926799376165],\n",
       " [0.7057098379973172,\n",
       "  0.6421238475241837,\n",
       "  0.7121303902970291,\n",
       "  0.5528598505202115,\n",
       "  0.5609849181809196,\n",
       "  0.5220747685405384],\n",
       " [0.013205117168385613,\n",
       "  0.038942230497552195,\n",
       "  0.07267875688704331,\n",
       "  0.11857184151835275,\n",
       "  0.07464243504169127,\n",
       "  0.018297892489986922],\n",
       " [0.39176753559819355,\n",
       "  0.2126580884278147,\n",
       "  0.1972797678574355,\n",
       "  0.0874515545895834,\n",
       "  0.23614167477123904,\n",
       "  0.16202957920735528],\n",
       " [0.18270534232002075,\n",
       "  0.3617482813972565,\n",
       "  0.1077739449043063,\n",
       "  0.2403139791605751,\n",
       "  0.052827750158771385,\n",
       "  0.022942872034485737],\n",
       " [0.8395085693530991,\n",
       "  0.8417266361374147,\n",
       "  0.6079112663483716,\n",
       "  0.4970086170987291,\n",
       "  0.32085537948828935,\n",
       "  0.2666735342996457],\n",
       " [0.750370568170686,\n",
       "  0.7852431842938681,\n",
       "  0.14079824159442356,\n",
       "  0.020667683891113175,\n",
       "  0.2285723859484261,\n",
       "  0.04335555668939064],\n",
       " [0.5878373288329224,\n",
       "  0.6415065438382621,\n",
       "  0.7273286991193038,\n",
       "  0.5782431325012739,\n",
       "  0.5637914438231781,\n",
       "  0.4993636359064777],\n",
       " [0.0245119993445007,\n",
       "  0.04004749487553229,\n",
       "  0.25893214865318087,\n",
       "  0.044722915567881355,\n",
       "  0.2443464368829054,\n",
       "  0.04438166469879745],\n",
       " [0.02714938562103031,\n",
       "  0.27630056616494575,\n",
       "  0.032906227492987626,\n",
       "  0.290646890913763,\n",
       "  0.16907402882380587,\n",
       "  0.0930173710808305],\n",
       " [0.019469573941624754,\n",
       "  0.2883779016295819,\n",
       "  0.15372011115807327,\n",
       "  0.1431264288613394,\n",
       "  0.1704736699595046,\n",
       "  0.15145823027620953],\n",
       " [0.3786650652812208,\n",
       "  0.14560304874194074,\n",
       "  0.19996756946321786,\n",
       "  0.003494770370327499,\n",
       "  0.2649003007082464,\n",
       "  0.21766113574404727],\n",
       " [0.17294490852480487,\n",
       "  0.21857282282904758,\n",
       "  0.23960596263311523,\n",
       "  0.08442290591586932,\n",
       "  0.24984361864719812,\n",
       "  0.19241661225600798],\n",
       " [0.010565575057072105,\n",
       "  0.32865978383874295,\n",
       "  0.17141293062878518,\n",
       "  0.039082508908253696,\n",
       "  0.015780201109599868,\n",
       "  0.05604038877669368]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAKDD2010\n",
      "The following are the correlations between the faithfullness metric and... (abs fixed)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank Algorithm</th>\n",
       "      <th>Pearson Correlation</th>\n",
       "      <th>Spearman Correlation</th>\n",
       "      <th>Kendall Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness (itself)</td>\n",
       "      <td>0.911762</td>\n",
       "      <td>0.895730</td>\n",
       "      <td>0.713534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rank_based_fairthfulness:sum</td>\n",
       "      <td>0.617967</td>\n",
       "      <td>0.542093</td>\n",
       "      <td>0.377659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rank_based_fairthfulness:percentile</td>\n",
       "      <td>0.631631</td>\n",
       "      <td>0.577506</td>\n",
       "      <td>0.405708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rank_based_fairthfulness:avg</td>\n",
       "      <td>0.628643</td>\n",
       "      <td>0.572021</td>\n",
       "      <td>0.399599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rank_based_fairthfulness:inverse</td>\n",
       "      <td>0.591073</td>\n",
       "      <td>0.531733</td>\n",
       "      <td>0.369543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Rank Algorithm  Pearson Correlation  \\\n",
       "0                faithfulness (itself)             0.911762   \n",
       "1         rank_based_fairthfulness:sum             0.617967   \n",
       "2  rank_based_fairthfulness:percentile             0.631631   \n",
       "3         rank_based_fairthfulness:avg             0.628643   \n",
       "4     rank_based_fairthfulness:inverse             0.591073   \n",
       "\n",
       "   Spearman Correlation  Kendall Correlation  \n",
       "0              0.895730             0.713534  \n",
       "1              0.542093             0.377659  \n",
       "2              0.577506             0.405708  \n",
       "3              0.572021             0.399599  \n",
       "4              0.531733             0.369543  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "comp_arr = np.array(comp_arr)\n",
    "rank_algs = [\"faithfulness (itself)\", \"rank_based_fairthfulness:sum\", \"rank_based_fairthfulness:percentile\", \"rank_based_fairthfulness:avg\", \"rank_based_fairthfulness:inverse\"]\n",
    "# rank_algs = [\"NRC\"]\n",
    "\n",
    "pearson_corr = [np.corrcoef(comp_arr[:, 0], comp_arr[:, i])[0, 1] for i in range(1, comp_arr.shape[1])]\n",
    "spearman_corr = [spearmanr(comp_arr[:, 0], comp_arr[:, i]).correlation for i in range(1, comp_arr.shape[1])]\n",
    "kendall_corr = [kendalltau(comp_arr[:, 0], comp_arr[:, i]).correlation for i in range(1, comp_arr.shape[1])]\n",
    "\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Rank Algorithm': rank_algs,\n",
    "    'Pearson Correlation': pearson_corr,\n",
    "    'Spearman Correlation': spearman_corr,\n",
    "    'Kendall Correlation': kendall_corr\n",
    "})\n",
    "\n",
    "print(dataset_name)\n",
    "print(\"The following are the correlations between the faithfullness metric and... (abs fixed)\")\n",
    "display(correlation_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
