{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from data_loading import *\n",
    "from xai_agg import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7466666666666667\n",
      "ROC AUC: 0.5158014399393709\n"
     ]
    }
   ],
   "source": [
    "dataset_name, preprocessed_data, categorical_features, X, y, X_train, X_test, y_train, y_test, clf = load_pakdd2010_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2130 - val_loss: 1.1088\n",
      "Epoch 2/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 1.2059 - val_loss: 1.0603\n",
      "Epoch 3/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 1.1921 - val_loss: 1.0120\n",
      "Epoch 4/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 1.3268 - val_loss: 0.9662\n",
      "Epoch 5/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 1.0548 - val_loss: 0.9278\n",
      "Epoch 6/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.9555 - val_loss: 0.8973\n",
      "Epoch 7/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 1.0004 - val_loss: 0.8737\n",
      "Epoch 8/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.9619 - val_loss: 0.8548\n",
      "Epoch 9/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.9319 - val_loss: 0.8393\n",
      "Epoch 10/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.9708 - val_loss: 0.8260\n",
      "Epoch 11/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.9339 - val_loss: 0.8143\n",
      "Epoch 12/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.9562 - val_loss: 0.8042\n",
      "Epoch 13/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 0.9291 - val_loss: 0.7954\n",
      "Epoch 14/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.9730 - val_loss: 0.7879\n",
      "Epoch 15/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.9363 - val_loss: 0.7812\n",
      "Epoch 16/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 0.9127 - val_loss: 0.7753\n",
      "Epoch 17/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 1.0697 - val_loss: 0.7700\n",
      "Epoch 18/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.9097 - val_loss: 0.7654\n",
      "Epoch 19/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 0.8888 - val_loss: 0.7610\n",
      "Epoch 20/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.8605 - val_loss: 0.7572\n",
      "Epoch 21/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.9288 - val_loss: 0.7537\n",
      "Epoch 22/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 1.0651 - val_loss: 0.7505\n",
      "Epoch 23/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 0.8223 - val_loss: 0.7476\n",
      "Epoch 24/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 0.9717 - val_loss: 0.7449\n",
      "Epoch 25/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.9205 - val_loss: 0.7423\n",
      "Epoch 26/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 0.9002 - val_loss: 0.7399\n",
      "Epoch 27/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 1.1591 - val_loss: 0.7378\n",
      "Epoch 28/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.8148 - val_loss: 0.7356\n",
      "Epoch 29/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 0.8578 - val_loss: 0.7334\n",
      "Epoch 30/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.8329 - val_loss: 0.7315\n",
      "Epoch 31/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 0.8683 - val_loss: 0.7297\n",
      "Epoch 32/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.9087 - val_loss: 0.7278\n",
      "Epoch 33/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8518 - val_loss: 0.7260\n",
      "Epoch 34/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 0.9409 - val_loss: 0.7244\n",
      "Epoch 35/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.8659 - val_loss: 0.7227\n",
      "Epoch 36/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.8034 - val_loss: 0.7212\n",
      "Epoch 37/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 1.0145 - val_loss: 0.7196\n",
      "Epoch 38/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.7674 - val_loss: 0.7181\n",
      "Epoch 39/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 0.9722 - val_loss: 0.7167\n",
      "Epoch 40/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 0.7459 - val_loss: 0.7152\n",
      "Epoch 41/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 0.8668 - val_loss: 0.7138\n",
      "Epoch 42/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.8305 - val_loss: 0.7124\n",
      "Epoch 43/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.7885 - val_loss: 0.7110\n",
      "Epoch 44/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.8111 - val_loss: 0.7095\n",
      "Epoch 45/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.7763 - val_loss: 0.7083\n",
      "Epoch 46/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.9819 - val_loss: 0.7068\n",
      "Epoch 47/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.7662 - val_loss: 0.7054\n",
      "Epoch 48/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.8056 - val_loss: 0.7042\n",
      "Epoch 49/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8784 - val_loss: 0.7028\n",
      "Epoch 50/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 0.8626 - val_loss: 0.7015\n",
      "Epoch 51/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 0.8508 - val_loss: 0.7002\n",
      "Epoch 52/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.7272 - val_loss: 0.6990\n",
      "Epoch 53/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.9083 - val_loss: 0.6979\n",
      "Epoch 54/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.8722 - val_loss: 0.6968\n",
      "Epoch 55/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.8162 - val_loss: 0.6958\n",
      "Epoch 56/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.8961 - val_loss: 0.6948\n",
      "Epoch 57/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 0.8101 - val_loss: 0.6938\n",
      "Epoch 58/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 0.8181 - val_loss: 0.6929\n",
      "Epoch 59/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.8806 - val_loss: 0.6920\n",
      "Epoch 60/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.8785 - val_loss: 0.6912\n",
      "Epoch 61/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.8223 - val_loss: 0.6903\n",
      "Epoch 62/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.7782 - val_loss: 0.6897\n",
      "Epoch 63/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.7670 - val_loss: 0.6888\n",
      "Epoch 64/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.7579 - val_loss: 0.6881\n",
      "Epoch 65/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.8412 - val_loss: 0.6873\n",
      "Epoch 66/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 0.7791 - val_loss: 0.6866\n",
      "Epoch 67/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.8431 - val_loss: 0.6858\n",
      "Epoch 68/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.7713 - val_loss: 0.6851\n",
      "Epoch 69/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 0.8924 - val_loss: 0.6843\n",
      "Epoch 70/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.8142 - val_loss: 0.6837\n",
      "Epoch 71/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 0.7471 - val_loss: 0.6830\n",
      "Epoch 72/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.7683 - val_loss: 0.6823\n",
      "Epoch 73/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.8604 - val_loss: 0.6817\n",
      "Epoch 74/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.7425 - val_loss: 0.6811\n",
      "Epoch 75/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.8651 - val_loss: 0.6804\n",
      "Epoch 76/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.7154 - val_loss: 0.6797\n",
      "Epoch 77/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.7766 - val_loss: 0.6790\n",
      "Epoch 78/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 0.8321 - val_loss: 0.6785\n",
      "Epoch 79/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.8463 - val_loss: 0.6777\n",
      "Epoch 80/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.8207 - val_loss: 0.6771\n",
      "Epoch 81/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.7879 - val_loss: 0.6765\n",
      "Epoch 82/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 0.7496 - val_loss: 0.6760\n",
      "Epoch 83/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.7822 - val_loss: 0.6752\n",
      "Epoch 84/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.8583 - val_loss: 0.6746\n",
      "Epoch 85/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.8589 - val_loss: 0.6740\n",
      "Epoch 86/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.8180 - val_loss: 0.6733\n",
      "Epoch 87/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.7675 - val_loss: 0.6728\n",
      "Epoch 88/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.7598 - val_loss: 0.6721\n",
      "Epoch 89/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 0.9267 - val_loss: 0.6716\n",
      "Epoch 90/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.8202 - val_loss: 0.6709\n",
      "Epoch 91/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 0.8447 - val_loss: 0.6702\n",
      "Epoch 92/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.8003 - val_loss: 0.6696\n",
      "Epoch 93/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.6886 - val_loss: 0.6688\n",
      "Epoch 94/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.6768 - val_loss: 0.6680\n",
      "Epoch 95/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.8079 - val_loss: 0.6673\n",
      "Epoch 96/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.8053 - val_loss: 0.6666\n",
      "Epoch 97/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - loss: 0.7792 - val_loss: 0.6658\n",
      "Epoch 98/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - loss: 0.7841 - val_loss: 0.6651\n",
      "Epoch 99/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.8728 - val_loss: 0.6643\n",
      "Epoch 100/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.7866 - val_loss: 0.6637\n",
      "Epoch 101/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.8176 - val_loss: 0.6632\n",
      "Epoch 102/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 0.6948 - val_loss: 0.6625\n",
      "Epoch 103/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.8522 - val_loss: 0.6619\n",
      "Epoch 104/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.7663 - val_loss: 0.6614\n",
      "Epoch 105/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.7650 - val_loss: 0.6608\n",
      "Epoch 106/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.7093 - val_loss: 0.6603\n",
      "Epoch 107/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.7508 - val_loss: 0.6597\n",
      "Epoch 108/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.7698 - val_loss: 0.6593\n",
      "Epoch 109/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7087 - val_loss: 0.6587\n",
      "Epoch 110/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.7325 - val_loss: 0.6583\n",
      "Epoch 111/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.8137 - val_loss: 0.6579\n",
      "Epoch 112/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 0.8068 - val_loss: 0.6574\n",
      "Epoch 113/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 0.7894 - val_loss: 0.6570\n",
      "Epoch 114/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.7725 - val_loss: 0.6566\n",
      "Epoch 115/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.8967 - val_loss: 0.6562\n",
      "Epoch 116/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.8038 - val_loss: 0.6558\n",
      "Epoch 117/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.8807 - val_loss: 0.6556\n",
      "Epoch 118/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 0.7145 - val_loss: 0.6551\n",
      "Epoch 119/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.7916 - val_loss: 0.6548\n",
      "Epoch 120/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.7523 - val_loss: 0.6544\n",
      "Epoch 121/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.7543 - val_loss: 0.6541\n",
      "Epoch 122/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.7728 - val_loss: 0.6538\n",
      "Epoch 123/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.7806 - val_loss: 0.6534\n",
      "Epoch 124/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7585 - val_loss: 0.6532\n",
      "Epoch 125/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.7317 - val_loss: 0.6528\n",
      "Epoch 126/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.8030 - val_loss: 0.6525\n",
      "Epoch 127/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 0.7005 - val_loss: 0.6521\n",
      "Epoch 128/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.8819 - val_loss: 0.6519\n",
      "Epoch 129/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 0.7382 - val_loss: 0.6516\n",
      "Epoch 130/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.7288 - val_loss: 0.6513\n",
      "Epoch 131/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.7629 - val_loss: 0.6510\n",
      "Epoch 132/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7399 - val_loss: 0.6507\n",
      "Epoch 133/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.7091 - val_loss: 0.6504\n",
      "Epoch 134/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.7885 - val_loss: 0.6502\n",
      "Epoch 135/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - loss: 0.8285 - val_loss: 0.6499\n",
      "Epoch 136/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.8743 - val_loss: 0.6496\n",
      "Epoch 137/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.7894 - val_loss: 0.6493\n",
      "Epoch 138/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 0.7962 - val_loss: 0.6490\n",
      "Epoch 139/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.8131 - val_loss: 0.6487\n",
      "Epoch 140/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.7806 - val_loss: 0.6485\n",
      "Epoch 141/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.8176 - val_loss: 0.6482\n",
      "Epoch 142/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 0.7733 - val_loss: 0.6479\n",
      "Epoch 143/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - loss: 0.8615 - val_loss: 0.6476\n",
      "Epoch 144/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 0.8097 - val_loss: 0.6473\n",
      "Epoch 145/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 0.7998 - val_loss: 0.6470\n",
      "Epoch 146/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.7093 - val_loss: 0.6467\n",
      "Epoch 147/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.8156 - val_loss: 0.6465\n",
      "Epoch 148/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.7677 - val_loss: 0.6462\n",
      "Epoch 149/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - loss: 0.7404 - val_loss: 0.6460\n",
      "Epoch 150/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.7451 - val_loss: 0.6457\n",
      "Epoch 151/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 0.8648 - val_loss: 0.6455\n",
      "Epoch 152/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.6818 - val_loss: 0.6452\n",
      "Epoch 153/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 0.7734 - val_loss: 0.6450\n",
      "Epoch 154/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.6836 - val_loss: 0.6446\n",
      "Epoch 155/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.6891 - val_loss: 0.6444\n",
      "Epoch 156/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.7957 - val_loss: 0.6443\n",
      "Epoch 157/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.8279 - val_loss: 0.6439\n",
      "Epoch 158/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 0.8087 - val_loss: 0.6438\n",
      "Epoch 159/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.7396 - val_loss: 0.6435\n",
      "Epoch 160/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.7860 - val_loss: 0.6433\n",
      "Epoch 161/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7515 - val_loss: 0.6431\n",
      "Epoch 162/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.7830 - val_loss: 0.6428\n",
      "Epoch 163/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7624 - val_loss: 0.6426\n",
      "Epoch 164/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 0.8950 - val_loss: 0.6424\n",
      "Epoch 165/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 0.7980 - val_loss: 0.6423\n",
      "Epoch 166/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.7949 - val_loss: 0.6420\n",
      "Epoch 167/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.7294 - val_loss: 0.6419\n",
      "Epoch 168/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.7590 - val_loss: 0.6417\n",
      "Epoch 169/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.7117 - val_loss: 0.6415\n",
      "Epoch 170/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.7603 - val_loss: 0.6413\n",
      "Epoch 171/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.7914 - val_loss: 0.6412\n",
      "Epoch 172/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7165 - val_loss: 0.6410\n",
      "Epoch 173/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.7386 - val_loss: 0.6408\n",
      "Epoch 174/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8196 - val_loss: 0.6406\n",
      "Epoch 175/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.8008 - val_loss: 0.6404\n",
      "Epoch 176/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.8479 - val_loss: 0.6402\n",
      "Epoch 177/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.8566 - val_loss: 0.6401\n",
      "Epoch 178/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.8163 - val_loss: 0.6400\n",
      "Epoch 179/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.7795 - val_loss: 0.6397\n",
      "Epoch 180/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 0.7112 - val_loss: 0.6396\n",
      "Epoch 181/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.7557 - val_loss: 0.6395\n",
      "Epoch 182/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 0.9057 - val_loss: 0.6395\n",
      "Epoch 183/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 0.8307 - val_loss: 0.6393\n",
      "Epoch 184/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.7361 - val_loss: 0.6391\n",
      "Epoch 185/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 0.7420 - val_loss: 0.6391\n",
      "Epoch 186/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7914 - val_loss: 0.6389\n",
      "Epoch 187/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8538 - val_loss: 0.6388\n",
      "Epoch 188/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 0.8456 - val_loss: 0.6387\n",
      "Epoch 189/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.6874 - val_loss: 0.6385\n",
      "Epoch 190/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 0.7611 - val_loss: 0.6384\n",
      "Epoch 191/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.7063 - val_loss: 0.6384\n",
      "Epoch 192/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.7398 - val_loss: 0.6382\n",
      "Epoch 193/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.7636 - val_loss: 0.6382\n",
      "Epoch 194/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.7533 - val_loss: 0.6380\n",
      "Epoch 195/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.7810 - val_loss: 0.6379\n",
      "Epoch 196/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.7266 - val_loss: 0.6378\n",
      "Epoch 197/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.7504 - val_loss: 0.6377\n",
      "Epoch 198/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 0.8211 - val_loss: 0.6376\n",
      "Epoch 199/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.7452 - val_loss: 0.6375\n",
      "Epoch 200/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.8243 - val_loss: 0.6375\n",
      "Epoch 201/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.7691 - val_loss: 0.6373\n",
      "Epoch 202/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.7382 - val_loss: 0.6372\n",
      "Epoch 203/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.7656 - val_loss: 0.6372\n",
      "Epoch 204/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.7656 - val_loss: 0.6371\n",
      "Epoch 205/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.7876 - val_loss: 0.6370\n",
      "Epoch 206/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.8532 - val_loss: 0.6369\n",
      "Epoch 207/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7658 - val_loss: 0.6369\n",
      "Epoch 208/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.7460 - val_loss: 0.6367\n",
      "Epoch 209/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.7891 - val_loss: 0.6367\n",
      "Epoch 210/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.7148 - val_loss: 0.6366\n",
      "Epoch 211/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 0.7787 - val_loss: 0.6365\n",
      "Epoch 212/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.7186 - val_loss: 0.6365\n",
      "Epoch 213/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.8311 - val_loss: 0.6364\n",
      "Epoch 214/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.7724 - val_loss: 0.6364\n",
      "Epoch 215/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.7045 - val_loss: 0.6363\n",
      "Epoch 216/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.7270 - val_loss: 0.6362\n",
      "Epoch 217/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 0.7789 - val_loss: 0.6361\n",
      "Epoch 218/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.8262 - val_loss: 0.6361\n",
      "Epoch 219/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.7707 - val_loss: 0.6360\n",
      "Epoch 220/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.7761 - val_loss: 0.6359\n",
      "Epoch 221/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 0.7071 - val_loss: 0.6359\n",
      "Epoch 222/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.8315 - val_loss: 0.6358\n",
      "Epoch 223/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.8513 - val_loss: 0.6357\n",
      "Epoch 224/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.7896 - val_loss: 0.6357\n",
      "Epoch 225/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 0.7632 - val_loss: 0.6357\n",
      "Epoch 226/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.7891 - val_loss: 0.6356\n",
      "Epoch 227/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.9313 - val_loss: 0.6355\n",
      "Epoch 228/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.7488 - val_loss: 0.6355\n",
      "Epoch 229/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.7351 - val_loss: 0.6355\n",
      "Epoch 230/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.8343 - val_loss: 0.6354\n",
      "Epoch 231/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 0.7760 - val_loss: 0.6353\n",
      "Epoch 232/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.7668 - val_loss: 0.6353\n",
      "Epoch 233/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.7295 - val_loss: 0.6352\n",
      "Epoch 234/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.7519 - val_loss: 0.6352\n",
      "Epoch 235/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.8666 - val_loss: 0.6351\n",
      "Epoch 236/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.7053 - val_loss: 0.6351\n",
      "Epoch 237/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.6566 - val_loss: 0.6350\n",
      "Epoch 238/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.9125 - val_loss: 0.6350\n",
      "Epoch 239/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 0.7848 - val_loss: 0.6350\n",
      "Epoch 240/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.6873 - val_loss: 0.6350\n",
      "Epoch 241/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.7731 - val_loss: 0.6349\n",
      "Epoch 242/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.7986 - val_loss: 0.6348\n",
      "Epoch 243/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7269 - val_loss: 0.6348\n",
      "Epoch 244/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.7295 - val_loss: 0.6348\n",
      "Epoch 245/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 0.7413 - val_loss: 0.6346\n",
      "Epoch 246/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 0.6718 - val_loss: 0.6346\n",
      "Epoch 247/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 0.8273 - val_loss: 0.6345\n",
      "Epoch 248/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7955 - val_loss: 0.6345\n",
      "Epoch 249/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 0.7749 - val_loss: 0.6345\n",
      "Epoch 250/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.7398 - val_loss: 0.6344\n",
      "Epoch 251/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.7817 - val_loss: 0.6344\n",
      "Epoch 252/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 0.7242 - val_loss: 0.6343\n",
      "Epoch 253/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.7898 - val_loss: 0.6344\n",
      "Epoch 254/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.8080 - val_loss: 0.6343\n",
      "Epoch 255/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - loss: 0.7104 - val_loss: 0.6343\n",
      "Epoch 256/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.7673 - val_loss: 0.6341\n",
      "Epoch 257/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.6907 - val_loss: 0.6341\n",
      "Epoch 258/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 0.6447 - val_loss: 0.6340\n",
      "Epoch 259/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.7770 - val_loss: 0.6341\n",
      "Epoch 260/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - loss: 0.7513 - val_loss: 0.6341\n",
      "Epoch 261/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.6837 - val_loss: 0.6340\n",
      "Epoch 262/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.8472 - val_loss: 0.6340\n",
      "Epoch 263/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.6900 - val_loss: 0.6339\n",
      "Epoch 264/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 0.8077 - val_loss: 0.6339\n",
      "Epoch 265/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.7004 - val_loss: 0.6338\n",
      "Epoch 266/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.8672 - val_loss: 0.6338\n",
      "Epoch 267/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 0.8238 - val_loss: 0.6338\n",
      "Epoch 268/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.8100 - val_loss: 0.6337\n",
      "Epoch 269/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.8087 - val_loss: 0.6338\n",
      "Epoch 270/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.7950 - val_loss: 0.6336\n",
      "Epoch 271/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.7635 - val_loss: 0.6336\n",
      "Epoch 272/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.7456 - val_loss: 0.6336\n",
      "Epoch 273/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8061 - val_loss: 0.6335\n",
      "Epoch 274/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.7613 - val_loss: 0.6335\n",
      "Epoch 275/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.7757 - val_loss: 0.6335\n",
      "Epoch 276/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 0.7410 - val_loss: 0.6334\n",
      "Epoch 277/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.7208 - val_loss: 0.6335\n",
      "Epoch 278/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.8098 - val_loss: 0.6334\n",
      "Epoch 279/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.7320 - val_loss: 0.6333\n",
      "Epoch 280/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.8975 - val_loss: 0.6333\n",
      "Epoch 281/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.7533 - val_loss: 0.6332\n",
      "Epoch 282/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.8035 - val_loss: 0.6333\n",
      "Epoch 283/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.8193 - val_loss: 0.6332\n",
      "Epoch 284/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.7636 - val_loss: 0.6332\n",
      "Epoch 285/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.8507 - val_loss: 0.6332\n",
      "Epoch 286/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.7404 - val_loss: 0.6331\n",
      "Epoch 287/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.7019 - val_loss: 0.6331\n",
      "Epoch 288/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.8030 - val_loss: 0.6330\n",
      "Epoch 289/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.7974 - val_loss: 0.6329\n",
      "Epoch 290/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.7072 - val_loss: 0.6331\n",
      "Epoch 291/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.7076 - val_loss: 0.6329\n",
      "Epoch 292/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.8553 - val_loss: 0.6329\n",
      "Epoch 293/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.8862 - val_loss: 0.6329\n",
      "Epoch 294/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 0.7860 - val_loss: 0.6329\n",
      "Epoch 295/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 0.7459 - val_loss: 0.6328\n",
      "Epoch 296/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 0.7876 - val_loss: 0.6327\n",
      "Epoch 297/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.7985 - val_loss: 0.6328\n",
      "Epoch 298/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 0.7898 - val_loss: 0.6327\n",
      "Epoch 299/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7732 - val_loss: 0.6327\n",
      "Epoch 300/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.7133 - val_loss: 0.6327\n",
      "Epoch 301/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.6442 - val_loss: 0.6327\n",
      "Epoch 302/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.8637 - val_loss: 0.6326\n",
      "Epoch 303/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.8364 - val_loss: 0.6326\n",
      "Epoch 304/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.8226 - val_loss: 0.6326\n",
      "Epoch 305/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.7095 - val_loss: 0.6325\n",
      "Epoch 306/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.8170 - val_loss: 0.6326\n",
      "Epoch 307/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.8135 - val_loss: 0.6325\n",
      "Epoch 308/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 0.7013 - val_loss: 0.6324\n",
      "Epoch 309/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.6752 - val_loss: 0.6324\n",
      "Epoch 310/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 0.7410 - val_loss: 0.6324\n",
      "Epoch 311/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.7445 - val_loss: 0.6323\n",
      "Epoch 312/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 0.8191 - val_loss: 0.6323\n",
      "Epoch 313/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 0.7630 - val_loss: 0.6323\n",
      "Epoch 314/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.8411 - val_loss: 0.6322\n",
      "Epoch 315/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.8515 - val_loss: 0.6323\n",
      "Epoch 316/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.7228 - val_loss: 0.6322\n",
      "Epoch 317/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 0.7556 - val_loss: 0.6323\n",
      "Epoch 318/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.7389 - val_loss: 0.6322\n",
      "Epoch 319/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.7611 - val_loss: 0.6322\n",
      "Epoch 320/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.6972 - val_loss: 0.6321\n",
      "Epoch 321/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 0.9247 - val_loss: 0.6321\n",
      "Epoch 322/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 0.7283 - val_loss: 0.6321\n",
      "Epoch 323/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.7040 - val_loss: 0.6321\n",
      "Epoch 324/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7292 - val_loss: 0.6320\n",
      "Epoch 325/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.8651 - val_loss: 0.6320\n",
      "Epoch 326/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 0.8556 - val_loss: 0.6320\n",
      "Epoch 327/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.7247 - val_loss: 0.6320\n",
      "Epoch 328/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - loss: 0.8259 - val_loss: 0.6319\n",
      "Epoch 329/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.6685 - val_loss: 0.6319\n",
      "Epoch 330/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.6984 - val_loss: 0.6319\n",
      "Epoch 331/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.6711 - val_loss: 0.6319\n",
      "Epoch 332/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.6673 - val_loss: 0.6319\n",
      "Epoch 333/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.7402 - val_loss: 0.6318\n",
      "Epoch 334/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.8826 - val_loss: 0.6318\n",
      "Epoch 335/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.8264 - val_loss: 0.6319\n",
      "Epoch 336/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.7234 - val_loss: 0.6318\n",
      "Epoch 337/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 0.7947 - val_loss: 0.6317\n",
      "Epoch 338/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.6997 - val_loss: 0.6317\n",
      "Epoch 339/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.6430 - val_loss: 0.6317\n",
      "Epoch 340/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 0.7718 - val_loss: 0.6316\n",
      "Epoch 341/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.7265 - val_loss: 0.6315\n",
      "Epoch 342/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.7350 - val_loss: 0.6316\n",
      "Epoch 343/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.7959 - val_loss: 0.6315\n",
      "Epoch 344/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.7391 - val_loss: 0.6316\n",
      "Epoch 345/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.7410 - val_loss: 0.6315\n",
      "Epoch 346/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.8956 - val_loss: 0.6316\n",
      "Epoch 347/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 0.8200 - val_loss: 0.6315\n",
      "Epoch 348/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.7699 - val_loss: 0.6314\n",
      "Epoch 349/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.7949 - val_loss: 0.6315\n",
      "Epoch 350/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.6692 - val_loss: 0.6314\n",
      "Epoch 351/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.7788 - val_loss: 0.6314\n",
      "Epoch 352/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8456 - val_loss: 0.6314\n",
      "Epoch 353/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.6320 - val_loss: 0.6314\n",
      "Epoch 354/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 0.7688 - val_loss: 0.6312\n",
      "Epoch 355/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.7419 - val_loss: 0.6313\n",
      "Epoch 356/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7221 - val_loss: 0.6312\n",
      "Epoch 357/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.7831 - val_loss: 0.6312\n",
      "Epoch 358/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.8181 - val_loss: 0.6312\n",
      "Epoch 359/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.7029 - val_loss: 0.6311\n",
      "Epoch 360/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.7809 - val_loss: 0.6312\n",
      "Epoch 361/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.7691 - val_loss: 0.6311\n",
      "Epoch 362/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.8703 - val_loss: 0.6312\n",
      "Epoch 363/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.7836 - val_loss: 0.6311\n",
      "Epoch 364/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 0.8657 - val_loss: 0.6310\n",
      "Epoch 365/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 0.8067 - val_loss: 0.6310\n",
      "Epoch 366/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 0.6650 - val_loss: 0.6310\n",
      "Epoch 367/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 0.7860 - val_loss: 0.6310\n",
      "Epoch 368/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.7145 - val_loss: 0.6309\n",
      "Epoch 369/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 0.8683 - val_loss: 0.6308\n",
      "Epoch 370/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.7164 - val_loss: 0.6309\n",
      "Epoch 371/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7014 - val_loss: 0.6310\n",
      "Epoch 372/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.7929 - val_loss: 0.6308\n",
      "Epoch 373/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.7874 - val_loss: 0.6308\n",
      "Epoch 374/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.6988 - val_loss: 0.6307\n",
      "Epoch 375/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 0.6825 - val_loss: 0.6307\n",
      "Epoch 376/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 0.9090 - val_loss: 0.6307\n",
      "Epoch 377/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.7497 - val_loss: 0.6307\n",
      "Epoch 378/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.6312 - val_loss: 0.6306\n",
      "Epoch 379/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.7727 - val_loss: 0.6306\n",
      "Epoch 380/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.8176 - val_loss: 0.6306\n",
      "Epoch 381/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.7427 - val_loss: 0.6307\n",
      "Epoch 382/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.8261 - val_loss: 0.6306\n",
      "Epoch 383/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.6519 - val_loss: 0.6305\n",
      "Epoch 384/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.8357 - val_loss: 0.6305\n",
      "Epoch 385/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.7641 - val_loss: 0.6306\n",
      "Epoch 386/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.7944 - val_loss: 0.6305\n",
      "Epoch 387/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.6658 - val_loss: 0.6304\n",
      "Epoch 388/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 0.7053 - val_loss: 0.6305\n",
      "Epoch 389/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.7720 - val_loss: 0.6304\n",
      "Epoch 390/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7186 - val_loss: 0.6304\n",
      "Epoch 391/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.6852 - val_loss: 0.6304\n",
      "Epoch 392/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.8030 - val_loss: 0.6304\n",
      "Epoch 393/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.6802 - val_loss: 0.6304\n",
      "Epoch 394/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 0.7969 - val_loss: 0.6303\n",
      "Epoch 395/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 0.7135 - val_loss: 0.6304\n",
      "Epoch 396/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 0.7281 - val_loss: 0.6303\n",
      "Epoch 397/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 0.9065 - val_loss: 0.6302\n",
      "Epoch 398/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.6923 - val_loss: 0.6302\n",
      "Epoch 399/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.7174 - val_loss: 0.6303\n",
      "Epoch 400/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.6789 - val_loss: 0.6302\n",
      "Epoch 401/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.7630 - val_loss: 0.6301\n",
      "Epoch 402/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.6988 - val_loss: 0.6302\n",
      "Epoch 403/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 0.7685 - val_loss: 0.6301\n",
      "Epoch 404/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.8162 - val_loss: 0.6301\n",
      "Epoch 405/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 0.9166 - val_loss: 0.6301\n",
      "Epoch 406/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6901 - val_loss: 0.6301\n",
      "Epoch 407/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.7719 - val_loss: 0.6301\n",
      "Epoch 408/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.6990 - val_loss: 0.6301\n",
      "Epoch 409/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 0.7774 - val_loss: 0.6300\n",
      "Epoch 410/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.9014 - val_loss: 0.6300\n",
      "Epoch 411/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.8477 - val_loss: 0.6300\n",
      "Epoch 412/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.8689 - val_loss: 0.6300\n",
      "Epoch 413/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 0.7466 - val_loss: 0.6299\n",
      "Epoch 414/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8391 - val_loss: 0.6299\n",
      "Epoch 415/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.6807 - val_loss: 0.6299\n",
      "Epoch 416/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 0.7601 - val_loss: 0.6299\n",
      "Epoch 417/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 0.6748 - val_loss: 0.6299\n",
      "Epoch 418/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.7719 - val_loss: 0.6299\n",
      "Epoch 419/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.6921 - val_loss: 0.6299\n",
      "Epoch 420/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 0.6741 - val_loss: 0.6299\n",
      "Epoch 421/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7244 - val_loss: 0.6298\n",
      "Epoch 422/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.7839 - val_loss: 0.6299\n",
      "Epoch 423/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.7578 - val_loss: 0.6298\n",
      "Epoch 424/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.7972 - val_loss: 0.6298\n",
      "Epoch 425/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 0.7325 - val_loss: 0.6297\n",
      "Epoch 426/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.7361 - val_loss: 0.6298\n",
      "Epoch 427/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.7212 - val_loss: 0.6298\n",
      "Epoch 428/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.7699 - val_loss: 0.6297\n",
      "Epoch 429/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.9319 - val_loss: 0.6297\n",
      "Epoch 430/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 0.6738 - val_loss: 0.6297\n",
      "Epoch 431/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 0.7271 - val_loss: 0.6297\n",
      "Epoch 432/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.8710 - val_loss: 0.6296\n",
      "Epoch 433/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 0.6335 - val_loss: 0.6296\n",
      "Epoch 434/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 0.6892 - val_loss: 0.6296\n",
      "Epoch 435/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.7805 - val_loss: 0.6296\n",
      "Epoch 436/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.7504 - val_loss: 0.6296\n",
      "Epoch 437/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.7375 - val_loss: 0.6296\n",
      "Epoch 438/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.6778 - val_loss: 0.6296\n",
      "Epoch 439/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 0.7655 - val_loss: 0.6295\n",
      "Epoch 440/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.8312 - val_loss: 0.6295\n",
      "Epoch 441/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.7807 - val_loss: 0.6296\n",
      "Epoch 442/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 0.7502 - val_loss: 0.6294\n",
      "Epoch 443/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 0.7376 - val_loss: 0.6296\n",
      "Epoch 444/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.8014 - val_loss: 0.6295\n",
      "Epoch 445/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.7822 - val_loss: 0.6295\n",
      "Epoch 446/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0015 - val_loss: 0.6295\n",
      "Epoch 447/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 0.6581 - val_loss: 0.6295\n",
      "Epoch 448/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 0.7474 - val_loss: 0.6295\n",
      "Epoch 449/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.7039 - val_loss: 0.6294\n",
      "Epoch 450/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 0.8160 - val_loss: 0.6294\n",
      "Epoch 451/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 0.7424 - val_loss: 0.6294\n",
      "Epoch 452/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.7531 - val_loss: 0.6294\n",
      "Epoch 453/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.6610 - val_loss: 0.6294\n",
      "Epoch 454/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.7265 - val_loss: 0.6293\n",
      "Epoch 455/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.7601 - val_loss: 0.6293\n",
      "Epoch 456/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.7381 - val_loss: 0.6294\n",
      "Epoch 457/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.8190 - val_loss: 0.6294\n",
      "Epoch 458/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.8250 - val_loss: 0.6293\n",
      "Epoch 459/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 0.7617 - val_loss: 0.6292\n",
      "Epoch 460/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - loss: 0.7182 - val_loss: 0.6293\n",
      "Epoch 461/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 0.8160 - val_loss: 0.6293\n",
      "Epoch 462/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7972 - val_loss: 0.6293\n",
      "Epoch 463/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.7484 - val_loss: 0.6292\n",
      "Epoch 464/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.8455 - val_loss: 0.6293\n",
      "Epoch 465/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.7937 - val_loss: 0.6293\n",
      "Epoch 466/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.8700 - val_loss: 0.6292\n",
      "Epoch 467/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.7117 - val_loss: 0.6293\n",
      "Epoch 468/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 0.7368 - val_loss: 0.6292\n",
      "Epoch 469/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.8137 - val_loss: 0.6292\n",
      "Epoch 470/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 0.8083 - val_loss: 0.6292\n",
      "Epoch 471/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 0.7283 - val_loss: 0.6292\n",
      "Epoch 472/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.7186 - val_loss: 0.6291\n",
      "Epoch 473/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.7357 - val_loss: 0.6292\n",
      "Epoch 474/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.7098 - val_loss: 0.6291\n",
      "Epoch 475/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - loss: 0.7279 - val_loss: 0.6292\n",
      "Epoch 476/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.8317 - val_loss: 0.6291\n",
      "Epoch 477/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7736 - val_loss: 0.6291\n",
      "Epoch 478/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 0.6365 - val_loss: 0.6292\n",
      "Epoch 479/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.8027 - val_loss: 0.6291\n",
      "Epoch 480/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 0.6700 - val_loss: 0.6291\n",
      "Epoch 481/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.7537 - val_loss: 0.6290\n",
      "Epoch 482/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 0.6880 - val_loss: 0.6290\n",
      "Epoch 483/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.8488 - val_loss: 0.6291\n",
      "Epoch 484/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 0.7156 - val_loss: 0.6290\n",
      "Epoch 485/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 0.8760 - val_loss: 0.6291\n",
      "Epoch 486/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.7358 - val_loss: 0.6290\n",
      "Epoch 487/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.7432 - val_loss: 0.6291\n",
      "Epoch 488/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 0.7757 - val_loss: 0.6290\n",
      "Epoch 489/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.7075 - val_loss: 0.6290\n",
      "Epoch 490/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.8038 - val_loss: 0.6289\n",
      "Epoch 491/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.8240 - val_loss: 0.6290\n",
      "Epoch 492/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7160 - val_loss: 0.6290\n",
      "Epoch 493/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 0.7367 - val_loss: 0.6289\n",
      "Epoch 494/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.7132 - val_loss: 0.6290\n",
      "Epoch 495/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 0.7143 - val_loss: 0.6290\n",
      "Epoch 496/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.7218 - val_loss: 0.6289\n",
      "Epoch 497/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 0.7825 - val_loss: 0.6289\n",
      "Epoch 498/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.8162 - val_loss: 0.6289\n",
      "Epoch 499/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.7362 - val_loss: 0.6289\n",
      "Epoch 500/500\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8573 - val_loss: 0.6289\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step\n"
     ]
    }
   ],
   "source": [
    "evaluator = ExplanationModelEvaluator(clf, X_train, categorical_features)\n",
    "evaluator.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6915117682202048"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.faithfullness_correlation(ShapTabularTreeWrapper, X_test.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating metric vectors and calculating correlation between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a few reanknings\n",
    "exps = [LimeWrapper(clf, X_train, categorical_features), ShapTabularTreeWrapper(clf, X_train, categorical_features), AnchorWrapper(clf, X_train, categorical_features)]\n",
    "\n",
    "indexes = np.random.choice(X_test.index, 100, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "comp_arr = [] # original, new1, new2, ...\n",
    "\n",
    "def process_index(idx):\n",
    "    local_comp_arr = []\n",
    "    for exp in exps:\n",
    "        row = []\n",
    "        # explanation = exp.explain_instance(X_test.loc[idx])\n",
    "        \n",
    "        # row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=False, iterations=100, len_subset=1))\n",
    "        # row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=False, iterations=100, len_subset=1))\n",
    "        # row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"sum\", iterations=100, len_subset=1))\n",
    "        # row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"percentile\", iterations=100, len_subset=1))\n",
    "        # row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"avg\", iterations=100, len_subset=1))\n",
    "        # row.append(evaluator.faithfullness_correlation(exp, X_test.loc[idx], explanation=explanation, rank_based=True, rb_alg=\"inverse\", iterations=100, len_subset=1))\n",
    "        \n",
    "        # row.append(evaluator.complexity(exp, X_test.loc[idx], explanation=explanation))\n",
    "        # row.append(evaluator.nrc(exp, X_test.loc[idx], explanation=explanation))\n",
    "        \n",
    "        row.append(evaluator._sensitivity_sequential(exp, X_test.loc[idx], method='mean_absolute', iterations=1))\n",
    "        row.append(evaluator._sensitivity_sequential(exp, X_test.loc[idx], method=\"spearman\", iterations=1))\n",
    "        \n",
    "        local_comp_arr.append(row)\n",
    "    return local_comp_arr\n",
    "    \n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_index, indexes))\n",
    "\n",
    "for result in results:\n",
    "    comp_arr.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0008856545852878739, 0.7800586510263928],\n",
       " [0.0008380033888166596, 0.9933896437752479],\n",
       " [0.03145833333333333, 0.7184212081070995],\n",
       " [0.001175577095221063, 0.8665689149560116],\n",
       " [0.0007403652000692767, 0.9933896437752479],\n",
       " [0.11561197916666666, 0.6330032127588345],\n",
       " [0.0009679633416458217, 0.8676686217008798],\n",
       " [0.0008412419710094097, 0.9827396254131471],\n",
       " [0.047356770833333325, 0.8296959777368934],\n",
       " [0.000943235226089521, 0.8123167155425219],\n",
       " [0.0007939854862945769, 0.9878810135879543],\n",
       " [0.10016927083333332, 0.2328354796127735],\n",
       " [0.0009556669365518614, 0.8082844574780058],\n",
       " [0.0009037461124487224, 0.9944913698127067],\n",
       " [0.05838541666666667, 0.1721244416916042],\n",
       " [0.0010000650151084868, 0.8097507331378297],\n",
       " [0.0007141438283512258, 0.9786999632757988],\n",
       " [0.07940104166666667, 0.4228033098995545],\n",
       " [0.0009607479523114388, 0.8383431085043986],\n",
       " [0.0008343636937755149, 0.9893499816378994],\n",
       " [0.0, 0.7184212081070995],\n",
       " [0.0008898152004622613, 0.8328445747800586],\n",
       " [0.0006898518918332284, 0.9915534337128168],\n",
       " [0.020976562499999997, 0.3185825346357062],\n",
       " [0.0007670096136317183, 0.8167155425219942],\n",
       " [0.0008199092685217302, 0.9651120088138083],\n",
       " [0.12352864583333334, 0.4051444604407426],\n",
       " [0.000870856682922647, 0.8324780058651026],\n",
       " [0.0007651960466519903, 0.9908189496878442],\n",
       " [0.018515625, 1.0],\n",
       " [0.0008502378088720921, 0.8222140762463344],\n",
       " [0.0007541311123324412, 0.9900844656628719],\n",
       " [0.0869921875, 0.6479482676161215],\n",
       " [0.0008626573770336098, 0.8174486803519062],\n",
       " [0.0008221184251691402, 0.9944913698127067],\n",
       " [0.04596354166666666, 0.6776504297994269],\n",
       " [0.0008803619901492065, 0.8225806451612903],\n",
       " [0.0008094784775012445, 0.9533602644142489],\n",
       " [0.023359375, 0.59607118668734],\n",
       " [0.0009529480160446569, 0.8350439882697946],\n",
       " [0.0008055713456589332, 0.9867792875504957],\n",
       " [0.033841145833333336, 0.07918421667319224],\n",
       " [0.0009212299352942134, 0.8200146627565982],\n",
       " [0.000874018079701244, 0.9864120455380095],\n",
       " [0.05291666666666667, 0.4793315430477058],\n",
       " [0.0008491014505420704, 0.8207478005865102],\n",
       " [0.000867374609034748, 0.9761292691883952],\n",
       " [0.0315625, 0.6720714527453512],\n",
       " [0.0011535097563327937, 0.8438416422287388],\n",
       " [0.00043898110390742305, 0.9922879177377891],\n",
       " [0.06119791666666667, 0.5062854457082717],\n",
       " [0.0010932975532689905, 0.811217008797654],\n",
       " [0.0008523742850097093, 0.9618068307014324],\n",
       " [0.023359375000000005, 0.7184212081070995],\n",
       " [0.0008660124790226696, 0.875733137829912],\n",
       " [0.0007614004906016093, 0.9640102827763496],\n",
       " [0.055026041666666664, 0.7184212081070995],\n",
       " [0.0009555475165010697, 0.8101173020527858],\n",
       " [0.0006717268700637644, 0.9419757620271758],\n",
       " [0.032109374999999996, 0.39123196448390685],\n",
       " [0.000918596521049622, 0.849340175953079],\n",
       " [0.0010194478574158518, 0.9898919820162491],\n",
       " [0.0643359375, 0.3733585314148811],\n",
       " [0.0009653455904351256, 0.8552052785923752],\n",
       " [0.0006742510247336849, 0.9768637532133675],\n",
       " [0.04446614583333333, 0.8279692431318217],\n",
       " [4.0841692473118174e-09, 0.9688416422287388],\n",
       " [0.0009504234246854891, 0.9816581071166541],\n",
       " [0.023606770833333332, 0.7184212081070995],\n",
       " [0.0008889353675432257, 0.9043255131964808],\n",
       " [0.0009115250660887849, 0.9728240910760191],\n",
       " [0.10045572916666667, 0.5756617937574081],\n",
       " [0.0010746007253847121, 0.8350439882697946],\n",
       " [0.000853922027671814, 0.9658464928387809],\n",
       " [0.023619791666666664, 0.7184212081070995],\n",
       " [0.0009232339589226989, 0.8643695014662756],\n",
       " [0.0010197114100668613, 0.9533602644142489],\n",
       " [0.02833333333333333, 0.34448355371177986],\n",
       " [0.0009009564120647729, 0.785923753665689],\n",
       " [0.0009733461858093579, 0.9667827184688605],\n",
       " [0.028880208333333334, 0.5],\n",
       " [0.0012400660124640155, 0.8713343108504397],\n",
       " [0.0006543400587101902, 0.9660465427837521],\n",
       " [0.07906250000000001, 0.7122443211069729],\n",
       " [0.0008678298955254324, 0.8530058651026391],\n",
       " [0.0010821205476915455, 0.9842085934630922],\n",
       " [0.008138020833333339, 0.6403142196899277],\n",
       " [0.0008675832613081475, 0.7760263929618766],\n",
       " [0.001095023209059178, 0.9831068674256336],\n",
       " [0.049140625, 0.34448355371177986],\n",
       " [0.0009588410876530478, 0.81341642228739],\n",
       " [0.000805668149542864, 0.9827396254131471],\n",
       " [0.07234375, 0.6332378223495703],\n",
       " [0.0006564101652421581, 0.7415689149560116],\n",
       " [0.0007611338240690202, 0.9640102827763496],\n",
       " [0.020872395833333335, 0.7184212081070995],\n",
       " [0.0010255654425786233, 0.7734604105571846],\n",
       " [0.000647530880383158, 0.9904517076753582],\n",
       " [0.024023437499999998, 0.7184212081070995],\n",
       " [0.0011304966375321698, 0.8339442815249266],\n",
       " [0.0009745626742956402, 0.9570326845391112],\n",
       " [0.04294270833333333, 0.7184212081070995],\n",
       " [0.0008758005551452794, 0.7987536656891494],\n",
       " [0.0005838340942221978, 0.9889827396254132],\n",
       " [0.024205729166666662, 0.39474362368634597],\n",
       " [0.0007825567592128935, 0.8804985337243401],\n",
       " [0.0004916178562942707, 0.984010707707946],\n",
       " [0.02361979166666667, 0.7184212081070995],\n",
       " [0.0007706818560970429, 0.8372434017595306],\n",
       " [0.0007571022508102132, 0.8773411678295996],\n",
       " [0.032213541666666665, 0.43340270551508847],\n",
       " [0.0009008889435250813, 0.787023460410557],\n",
       " [0.0007461732999609083, 0.9834741094381196],\n",
       " [0.06186197916666667, 0.519158775501877],\n",
       " [0.0007510967495370309, 0.7899560117302052],\n",
       " [0.0007731086783768149, 0.9842085934630922],\n",
       " [0.081640625, 0.4659339282421081],\n",
       " [0.0009511187714050262, 0.846774193548387],\n",
       " [0.0006145191453759048, 0.9904517076753582],\n",
       " [0.02541666666666666, 0.3937971147928731],\n",
       " [0.0009181262507976552, 0.7881231671554252],\n",
       " [0.0007144088302095496, 0.9698861549761293],\n",
       " [0.017799479166666667, 0.7150388457269701],\n",
       " [0.0010486742811257697, 0.8361436950146626],\n",
       " [0.0010489429305221166, 0.9886154976129269],\n",
       " [0.04747395833333334, 0.37297467469546086],\n",
       " [0.0009616848039467749, 0.7965542521994134],\n",
       " [0.0007682818366188116, 0.9889827396254132],\n",
       " [0.0712890625, 0.30328738422723056],\n",
       " [0.0008410853358492667, 0.8299120234604105],\n",
       " [0.0010534178669480807, 0.9891568227277111],\n",
       " [0.06096354166666667, 0.19375179930240582],\n",
       " [0.001114782257668425, 0.7657624633431085],\n",
       " [0.0009660178090924274, 0.9853103195005509],\n",
       " [0.04126302083333333, 0.4150186470264551],\n",
       " [0.0012605162767630128, 0.8629032258064515],\n",
       " [0.0009720292100256246, 0.9829079687751391],\n",
       " [0.056940104166666665, 0.7727137357695313],\n",
       " [0.0008757190100211922, 0.8914956011730205],\n",
       " [0.0005913501099044536, 0.9626910883403476],\n",
       " [0.01739583333333333, 0.34448355371177986],\n",
       " [0.0008692968824677926, 0.8123167155425219],\n",
       " [0.0006546068073326695, 0.9742930591259639],\n",
       " [0.02377604166666666, 0.6692553619911364],\n",
       " [0.0010837472486583352, 0.8071847507331378],\n",
       " [0.0008611001520285914, 0.9456481821520383],\n",
       " [0.02373697916666667, 0.59607118668734],\n",
       " [0.0009091244703079887, 0.8936950146627565],\n",
       " [0.0009152467275856679, 0.9599706206390011],\n",
       " [0.015546875000000005, 0.6720714527453512],\n",
       " [0.0009250759218771116, 0.8317448680351905],\n",
       " [0.0007997839487811209, 0.9728240910760191],\n",
       " [0.023567708333333336, 0.7184212081070995],\n",
       " [0.0010315662325412385, 0.8442082111436948],\n",
       " [0.0008385752834679752, 0.9570326845391112],\n",
       " [0.07018229166666667, 0.8029315913582838],\n",
       " [0.0008097814256754819, 0.8328445747800586],\n",
       " [0.0011617649490641216, 0.9731913330885054],\n",
       " [0.04657552083333333, 0.5576149810946085],\n",
       " [0.0009524192195276152, 0.7624633431085043],\n",
       " [0.000531754242643636, 0.940506793977231],\n",
       " [0.027486979166666665, 0.4854791945147811],\n",
       " [0.0008456464953147314, 0.8189149560117301],\n",
       " [0.0007977957100654541, 0.9731913330885054],\n",
       " [0.028841145833333335, 0.7184212081070995],\n",
       " [0.0010056293362552295, 0.9204545454545453],\n",
       " [0.0007657343852082834, 0.9665809768637532],\n",
       " [0.000403645833333334, 0.5],\n",
       " [0.0009411707518509494, 0.8203812316715542],\n",
       " [0.0005938615235771573, 0.9794344473007711],\n",
       " [0.024036458333333333, 0.7184212081070995],\n",
       " [0.0008640569031420259, 0.8310117302052785],\n",
       " [0.0009644283721197941, 0.9875137715754683],\n",
       " [0.07220052083333332, 0.5069159381686805],\n",
       " [0.0008264478328402873, 0.8577712609970674],\n",
       " [0.0010856605476659096, 0.9746603011384501],\n",
       " [0.0596875, 0.5576149810946085],\n",
       " [0.0007202739487015856, 0.8409090909090908],\n",
       " [0.0007061220898085985, 0.9195739992655159],\n",
       " [0.041380208333333335, 0.3185825346357062],\n",
       " [0.000975353449854099, 0.6370967741935483],\n",
       " [0.0011006322426500505, 0.9222573274707646],\n",
       " [0.012955729166666666, 0.3415785643878705],\n",
       " [0.000742153299992902, 0.7620967741935483],\n",
       " [0.0009301029951194551, 0.9818052298423324],\n",
       " [0.0237890625, 0.5],\n",
       " [0.0010538046116307905, 0.8291788856304985],\n",
       " [0.0010081237858718946, 0.980903415350716],\n",
       " [0.023502604166666666, 0.7184212081070995],\n",
       " [0.0008227904756597435, 0.75183284457478],\n",
       " [0.0010083820140400234, 0.9707778405142642],\n",
       " [0.06065104166666666, 0.3721113073929251],\n",
       " [0.0009461011582913033, 0.7426686217008798],\n",
       " [0.0008237964683746756, 0.9876865041506353],\n",
       " [0.04180989583333333, 0.7184212081070995],\n",
       " [0.0009490231063413763, 0.8057184750733137],\n",
       " [0.0010601651301875136, 0.9745125631625011],\n",
       " [0.08067708333333333, 0.15320966787934],\n",
       " [0.0011182883270908605, 0.814149560117302],\n",
       " [0.0010232416118734732, 0.9838413514506059],\n",
       " [0.10790364583333334, 0.6595816103910856],\n",
       " [0.0009716167731095946, 0.7738269794721406],\n",
       " [0.0007135069805354101, 0.977230995225854],\n",
       " [0.07753906249999999, 0.4345084729917858],\n",
       " [0.0010288118105891116, 0.8914956011730205],\n",
       " [0.0009109297060906486, 0.9814376501980633],\n",
       " [0.0473828125, 0.5888226775886302],\n",
       " [0.0009606003399662367, 0.8368768328445746],\n",
       " [0.0010431441502323936, 0.953501597233624],\n",
       " [0.08760416666666666, 0.29357798165137616],\n",
       " [0.0010266280729384427, 0.8262463343108504],\n",
       " [0.0012544290311284456, 0.9490906415023967],\n",
       " [0.04598958333333334, 0.7655365226461628],\n",
       " [0.0009732618486494063, 0.8434750733137828],\n",
       " [0.0007222088023419814, 0.9812706573632023],\n",
       " [0.010507812499999995, 0.8296959777368934],\n",
       " [0.0010352463110406, 0.9039589442815249],\n",
       " [0.0008764627708250569, 0.9768637532133675],\n",
       " [0.08096354166666667, 0.4014657956791419],\n",
       " [0.0008132700088542788, 0.8159824046920819],\n",
       " [0.0008602695115321169, 0.9764965112008812],\n",
       " [0.039466145833333334, 0.18348623853211007],\n",
       " [0.0010964090675999922, 0.7892228739002932],\n",
       " [0.0005423278864000737, 0.9489533602644142],\n",
       " [0.026940104166666666, 0.4656607700312175],\n",
       " [0.0008734900495526701, 0.8533724340175952],\n",
       " [0.0005589915153680276, 0.9880540837949042],\n",
       " [0.0719921875, 0.5],\n",
       " [0.0009884303135534209, 0.7478005865102638],\n",
       " [0.0011344345174860845, 0.9717223650385605],\n",
       " [0.07407552083333334, 0.5402587981719005],\n",
       " [0.0011048328996949054, 0.9314516129032256],\n",
       " [0.0009791096397193755, 0.9821728094866014],\n",
       " [0.063125, 0.12037833684469001],\n",
       " [0.0008213562297471093, 0.8376099706744866],\n",
       " [0.0003666242716523976, 0.9845758354755784],\n",
       " [0.05514322916666667, 0.3290679971218189],\n",
       " [0.0007859718486927705, 0.7917888563049852],\n",
       " [0.0009420748580216201, 0.9259331239134537],\n",
       " [0.12825520833333331, 0.5050244607874541],\n",
       " [0.0010786938008113672, 0.8841642228739002],\n",
       " [0.0011023425658960306, 0.9933896437752479],\n",
       " [0.02348958333333333, 0.7184212081070995],\n",
       " [0.0010010539892910595, 0.814882697947214],\n",
       " [0.0008927081208832989, 0.9360998898273962],\n",
       " [0.059062500000000004, 0.1714939492311954],\n",
       " [0.0010509994583945551, 0.8306451612903225],\n",
       " [0.0009061913566141299, 0.9882482556004406],\n",
       " [0.021653645833333332, 1.0],\n",
       " [0.0011502461272858606, 0.8625366568914956],\n",
       " [0.0012170788997865143, 0.9764965112008812],\n",
       " [0.07533854166666666, 0.043423602691750586],\n",
       " [0.0012474092465310366, 0.8174486803519062],\n",
       " [0.0009157929435465563, 0.9735585751009915],\n",
       " [0.024023437499999998, 0.7184212081070995],\n",
       " [0.0008674922726659352, 0.8555718475073313],\n",
       " [0.0009256806938591922, 0.9790672052882851],\n",
       " [0.030638020833333328, 0.3185825346357062],\n",
       " [0.0011069469570093643, 0.8343108504398826],\n",
       " [0.0005146496782133526, 0.9511568123393316],\n",
       " [0.010169270833333334, 0.6720714527453511],\n",
       " [0.0008638075183206757, 0.8192815249266862],\n",
       " [0.0010629383161207958, 0.9867792875504957],\n",
       " [0.04135416666666667, 0.24431036362780947],\n",
       " [0.0011014050564814048, 0.8933284457478006],\n",
       " [0.0005974130319798033, 0.9867792875504957],\n",
       " [0.026601562499999995, 0.19166970796427724],\n",
       " [0.0008715376515406666, 0.7840909090909091],\n",
       " [0.001116942411361634, 0.9720896070510467],\n",
       " [0.00794270833333333, 0.7184212081070995],\n",
       " [0.0007488699702955153, 0.8310117302052785],\n",
       " [0.0007379011083213528, 0.9151670951156812],\n",
       " [0.027200520833333332, 0.8279692431318217],\n",
       " [0.0007881988922442825, 0.9072580645161289],\n",
       " [0.0008253534272225061, 0.9619559290518098],\n",
       " [0.057122395833333325, 0.2099539893161326],\n",
       " [0.0008793352366774954, 0.8387096774193546],\n",
       " [0.0009595490679048167, 0.9761292691883952],\n",
       " [0.08152343749999999, 0.8305890654552344],\n",
       " [0.0008660381979989505, 0.7877565982404692],\n",
       " [0.000898816959221733, 0.987146529562982],\n",
       " [0.090390625, 0.7187503035920163],\n",
       " [0.0010971319149899704, 0.874266862170088],\n",
       " [0.0008794972184063887, 0.9801689313257437],\n",
       " [3.906249999999917e-05, 0.8029315913582838],\n",
       " [0.0008444599859151539, 0.7609970674486802],\n",
       " [0.0009272752448470278, 0.9891568227277111],\n",
       " [0.05061197916666667, 0.8612527009184298],\n",
       " [0.0008199870649052033, 0.844941348973607],\n",
       " [0.0006993085918982444, 0.9416085200146898],\n",
       " [0.021875, 0.7184212081070995],\n",
       " [0.0008769308245149487, 0.8936950146627565],\n",
       " [0.0010649088339435584, 0.9706206390011016],\n",
       " [0.09561197916666667, 0.35793562708102106],\n",
       " [0.0008093189528032626, 0.8079178885630498],\n",
       " [0.0007133716869235229, 0.9900844656628719],\n",
       " [0.055013020833333336, 0.1721244416916042],\n",
       " [0.00103372985521388, 0.8354105571847507],\n",
       " [0.0008459118539373645, 0.9457824247039766],\n",
       " [0.08338541666666667, 0.32492283004213973]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from scipy.stats import spearmanr\n",
    "\n",
    "# comp_arr = np.array(comp_arr)\n",
    "# rank_algs = [\"faithfulness (itself)\", \"rank_based_fairthfulness:sum\", \"rank_based_fairthfulness:percentile\", \"rank_based_fairthfulness:avg\", \"rank_based_fairthfulness:inverse\"]\n",
    "# # rank_algs = [\"NRC\"]\n",
    "\n",
    "# pearson_corr = [np.corrcoef(comp_arr[:, 0], comp_arr[:, i])[0, 1] for i in range(1, comp_arr.shape[1])]\n",
    "# spearman_corr = [spearmanr(comp_arr[:, 0], comp_arr[:, i]).correlation for i in range(1, comp_arr.shape[1])]\n",
    "# kendall_corr = [kendalltau(comp_arr[:, 0], comp_arr[:, i]).correlation for i in range(1, comp_arr.shape[1])]\n",
    "\n",
    "# correlation_df = pd.DataFrame({\n",
    "#     'Rank Algorithm': rank_algs,\n",
    "#     'Pearson Correlation': pearson_corr,\n",
    "#     'Spearman Correlation': spearman_corr,\n",
    "#     'Kendall Correlation': kendall_corr\n",
    "# })\n",
    "\n",
    "# print(dataset_name)\n",
    "# print(\"The following are the correlations between the faithfullness metric and... (abs fixed, subset_len=1, iterations=100)\")\n",
    "# display(correlation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAKDD2010\n",
      "The following are the correlations between the sensitivity (mean-absolute) metric and...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank Algorithm</th>\n",
       "      <th>Pearson Correlation</th>\n",
       "      <th>Spearman Correlation</th>\n",
       "      <th>Kendall Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stability:spearman</td>\n",
       "      <td>-0.712735</td>\n",
       "      <td>-0.669238</td>\n",
       "      <td>-0.475482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rank Algorithm  Pearson Correlation  Spearman Correlation  \\\n",
       "0  stability:spearman            -0.712735             -0.669238   \n",
       "\n",
       "   Kendall Correlation  \n",
       "0            -0.475482  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "comp_arr = np.array(comp_arr)\n",
    "rank_algs = [\"stability:spearman\"]\n",
    "\n",
    "pearson_corr = [np.corrcoef(comp_arr[:, 0], comp_arr[:, i])[0, 1] for i in range(1, comp_arr.shape[1])]\n",
    "spearman_corr = [spearmanr(comp_arr[:, 0], comp_arr[:, i]).correlation for i in range(1, comp_arr.shape[1])]\n",
    "kendall_corr = [kendalltau(comp_arr[:, 0], comp_arr[:, i]).correlation for i in range(1, comp_arr.shape[1])]\n",
    "\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Rank Algorithm': rank_algs,\n",
    "    'Pearson Correlation': pearson_corr,\n",
    "    'Spearman Correlation': spearman_corr,\n",
    "    'Kendall Correlation': kendall_corr\n",
    "})\n",
    "\n",
    "print(dataset_name)\n",
    "print(\"The following are the correlations between the sensitivity (mean-absolute) metric and...\")\n",
    "display(correlation_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
