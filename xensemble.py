import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tools.topsis import Topsis

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import NearestNeighbors

# Explainable AI tools:
import shap
from lime.lime_tabular import LimeTabularExplainer
from alibi.explainers import AnchorTabular # why not used the original anchor package?

from scipy.stats import spearmanr, pearsonr

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam  # Import the Adam optimizer

class FeatureImportanceCalculator():
    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, predict_proba_function: callable = None):
        self.X_train = X_train
        self.clf = clf
        
        if predict_proba_function is not None:
            self.predict_proba_function = predict_proba_function
        elif hasattr(clf, 'predict_proba') and predict_proba_function is None:
            self.predict_proba_function = clf.predict_proba
        else:
            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')
        
        self.anchor_explainer = AnchorTabular(predictor=self.predict_proba_function, feature_names=self.X_train.columns) # TODO: fix parameters
        self.anchor_explainer.fit(self.X_train.values)


    def get_lime_ranking(self, instance_data_row) -> pd.DataFrame:

        """
        Returns a DataFrame with the feature importance ranking using LIME, ordered by abs(importance).
        """
        
        explainer = LimeTabularExplainer(self.X_train.values, feature_names=self.X_train.columns, discretize_continuous=False)
        lime_exp = explainer.explain_instance(instance_data_row, self.predict_proba_function, num_features=len(self.X_train.columns))
        
        ranking = pd.DataFrame(lime_exp.as_list(), columns=['feature', 'score'])

        return ranking
    
    def get_shap_ranking(self, instance_data_row, explainer_type: shap.Explainer = shap.KernelExplainer, **additional_explainer_args) -> pd.DataFrame:
        """
        Returns a DataFrame with the feature importance ranking using SHAP, ordered by abs(importance).
        """
        explainer = explainer_type(self.clf, self.X_train, **additional_explainer_args)
        shap_values = explainer.shap_values(instance_data_row)

        ranking = pd.DataFrame(list(zip(self.X_train.columns, shap_values[:, 0])), columns=['feature', 'score'])
        ranking = ranking.sort_values(by='score', ascending=False, key=lambda x: abs(x)).reset_index(drop=True)
        
        return ranking
    
    def get_anchor_ranking(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:
        """
        Returns a DataFrame with the feature importance ranking using Anchor, ordered by abs(importance).
        Feature importance is not directly available in the AnchorTabular class. In order to obtain it, we can
        calculate the percentage of rows in the training data that are not covered by the anchor rule. The more
        rows that are not covered, the more important the feature is.
        """

        if isinstance(instance_data_row, pd.Series):
            instance_data_row = instance_data_row.to_numpy()

        feature_importances = {feature: 0 for feature in self.X_train.columns}
        explanation = self.anchor_explainer.explain(instance_data_row)
        
        for rule in explanation.anchor:
            # Extract the feature name from the rule string
            # This method won't work for column names that have spaces in them or that don't contain any letters
            for expression_element in rule.split():
                if any(c.isalpha() for c in expression_element):
                    referenced_feature = expression_element
                    break

            rule_coverage = self.X_train.query(rule).shape[0] / self.X_train.shape[0]
            feature_importances[referenced_feature] = 1 - rule_coverage
        
        return pd.DataFrame(list(feature_importances.items()), columns=['feature', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)

def get_noisy_data(X: pd.DataFrame, categorical_features_names: list[str], encoding_dim: int = 5, num_features_to_replace: int = 2, epochs=500) -> pd.DataFrame:

    """
    Returns a DataFrame containing a noisy variation of the data.

    The noise is generated by swapping the values of a small number of features between a sample and a random close neighbor.
    To determine the neighbors, we use an autoencoder to reduce the dimensionality of the data and then calculate the use the NearestNeightbors algorithm in the reduced space.
    """

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    input_dim = X_scaled.shape[1]

    input_layer = Input(shape=(input_dim,))
    encoded = Dense(encoding_dim, activation='relu')(input_layer)
    decoded = Dense(input_dim, activation='sigmoid')(encoded)

    autoencoder = Model(inputs=input_layer, outputs=decoded)
    encoder = Model(inputs=input_layer, outputs=encoded)
    autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')
    autoencoder.fit(X_scaled, X_scaled, epochs=epochs, batch_size=32, shuffle=True, validation_split=0.2)
    # Extract hidden layer representation:
    hidden_representation = encoder.predict(X_scaled)

    # Compute Nearest Neighbors using hidden_representation
    nbrs = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(hidden_representation) # TODO: here, hidden_representation is just the autoencoder fit to the scaled X data; see if this is the way to do this
    distances, indices = nbrs.kneighbors(hidden_representation)

    X_noisy = X.copy()

    # Get id's of columns that belong to the same categorical feature (after being one-hot-encodeded);
    # Columns that belong to the same categorical feature start with the same name, and will be treated as a single feature when adding noise.
    categorical_features_indices = [
        [X.columns.get_loc(col_name) for col_name in X.columns if col_name.startswith(feature)]
        for feature in categorical_features_names
    ]

    # Replace features with random neighbor's features
    for i in range(X.shape[0]):  # Iterate over each sample
        available_features_to_replace = list(range(X.shape[1]))
        for j in range(num_features_to_replace):
            # Select features to replace; if the feture selected belong to one of the lists in categorical_features_indices, we will replace all the features in that list
            features_to_replace = np.random.choice(available_features_to_replace, 1)
            for feature_indices in categorical_features_indices:
                if features_to_replace in feature_indices:
                    features_to_replace = feature_indices
                    break
            
            # Remove the selected features from the list of available features to replace
            available_features_to_replace = [f for f in available_features_to_replace if f not in features_to_replace]

            # Choose a random neighbor from the nearest neighbors
            neighbor_idx = np.random.choice(indices[i][1:])

            # Replace the selected features with the neighbor's features
            X_noisy.iloc[i, features_to_replace] = X.iloc[neighbor_idx, features_to_replace]

    return X_noisy


class EnsembleExplanation:
    def __init__(self, results: pd.DataFrame, weights: dict[str, float]):
        self.results = results
        self.weights = weights
    
    def __str__(self):
        return self.results.__str__()
    
    def __repr__(self):
        return self.results.__repr__()

class EnsembleExplainer:
    def __init__(self, X_train_preprocessed: pd.DataFrame | np.ndarray, categorical_features_names: list[str],
                 clf: RandomForestClassifier, predict_proba_function: callable = None, **kwargs):
        self.X_train = X_train_preprocessed
        self.categorical_features_names = categorical_features_names
        self.clf = clf
        self.predict_proba_function = predict_proba_function
        self._get_noisy_data_params = kwargs.get('get_noisy_data_params', {}) # optional parameters to be passed to get_noisy_data()

        self.original_fic = FeatureImportanceCalculator(self.clf, self.X_train, self.predict_proba_function)
        self.noisy_fic = None

        self.metrics_context = {
            "lime": [],
            "shap": [],
            "anchor": []
        }
    
    @staticmethod
    def _robustness_metrics(ranking_original: pd.DataFrame, ranking_noisy: pd.DataFrame) -> pd.DataFrame:
        """
        Returns a DataFrame with 4 robustness metrics of a given feature importance ranking:
            1. "mean_squared_differece": the mean squared difference between the scores of the original and noisy rankings; (previously called "stability") | Lower is better
            2. "mean_absolute_difference": the mean absolute difference between the scores of the original and noisy rankings; (previously called "sensitivity") | Lower is better
            3. "spearman_correlation": the Spearman correlation | Higher is better
            4. "pearson_correlation": the Pearson correlation | Higher is better
        """
        
        # Align dataframes:
        ranking_original = ranking_original.set_index('feature')
        ranking_noisy = ranking_noisy.set_index('feature')
        ranking_original = ranking_original.reindex(ranking_noisy.index)

        # Compute metrics:
        mean_squared_difference = ((ranking_original['score'] - ranking_noisy['score']) ** 2).mean()
        mean_absolute_difference = np.abs(ranking_original['score'] - ranking_noisy['score']).mean()
        spearman_correlation = spearmanr(ranking_original['score'], ranking_noisy['score']).correlation
        pearson_correlation = pearsonr(ranking_original['score'], ranking_noisy['score'])[0]

        robustness_metrics = pd.DataFrame({
            'mean_squared_difference': [mean_squared_difference],
            'mean_absolute_difference': [mean_absolute_difference],
            'spearman_correlation': [spearman_correlation],
            'pearson_correlation': [pearson_correlation]
        })

        return robustness_metrics
    
    def init(self):
        # Obtain noisy variation of needed tools:
        self.X_train_noisy = get_noisy_data(self.X_train, self.categorical_features_names, **self._get_noisy_data_params)
        self.noisy_fic = FeatureImportanceCalculator(self.clf, self.X_train_noisy, self.predict_proba_function)
    
    @staticmethod
    def _compute_weights(lime_instance_metrics: pd.DataFrame, shap_instance_metrics: pd.DataFrame, anchor_instance_metrics: pd.DataFrame) -> tuple[float, float, float]:
        """
        Returns the weights for each explanation method based on the robustness metrics. The weights are calculated using the TOPSIS method.
        reference: https://en.wikipedia.org/wiki/TOPSIS

        Parameters:
        lime_instance_metrics (pd.DataFrame): DataFrame containing robustness metrics for LIME explanations (obtained from FeatureImportanceCalculator).
        shap_instance_metrics (pd.DataFrame): DataFrame containing robustness metrics for SHAP explanations (obtained from FeatureImportanceCalculator).
        anchor_instance_metrics (pd.DataFrame): DataFrame containing robustness metrics for Anchor explanations (obtained from FeatureImportanceCalculator).
        Returns:
        tuple[float, float, float]: A tuple containing the weights for LIME, SHAP, and Anchor explanations respectively.
        """
        
        evaluation_matrix = np.array([
            lime_instance_metrics.values.flatten(),
            shap_instance_metrics.values.flatten(),
            anchor_instance_metrics.values.flatten()
        ])

        robustness_metrics_weights = [
            0.25, # mean squared difference
            0.25, # mean absolute difference
            0.25, # spearman correlation
            0.25  # pearson correlation
        ]

        # if higher value is preferred - True
        # if lower value is preferred - False
        criterias = np.array([
            False,  # For mean_squared_difference, lower is better
            False,  # For mean_absolute_difference, lower is better
            True,   # For spearman_correlation, higher is better
            True    # For pearson_correlation, higher is better
        ])

        t = Topsis(evaluation_matrix, robustness_metrics_weights, criterias, debug=False)
        t.calc()

        lime_weight, shap_weight, anchor_weight = t.best_similarity # redundant code, but it's easier to understand
        return lime_weight, shap_weight, anchor_weight

    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> EnsembleExplanation:
        # Getting weights for each explanation method:

        lime_ranking = self.original_fic.get_lime_ranking(instance_data_row)
        lime_ranking_noisy = self.noisy_fic.get_lime_ranking(instance_data_row)
        lime_instance_metrics = self._robustness_metrics(lime_ranking, lime_ranking_noisy)

        shap_ranking = self.original_fic.get_shap_ranking(instance_data_row, explainer_type=shap.TreeExplainer)
        shap_ranking_noisy = self.noisy_fic.get_shap_ranking(instance_data_row, explainer_type=shap.TreeExplainer)
        shap_instance_metrics = self._robustness_metrics(shap_ranking, shap_ranking_noisy)

        anchor_ranking = self.original_fic.get_anchor_ranking(instance_data_row)
        anchor_ranking_noisy = self.noisy_fic.get_anchor_ranking(instance_data_row)
        anchor_instance_metrics = self._robustness_metrics(anchor_ranking, anchor_ranking_noisy)

        lime_weight, shap_weight, anchor_weight = self._compute_weights(lime_instance_metrics, shap_instance_metrics, anchor_instance_metrics)

        result = lime_ranking.merge(shap_ranking, on='feature', how='outer', suffixes=('_lime', '_shap'))
        result = result.merge(anchor_ranking, on='feature', how='outer', suffixes=('_lime', '_shap'))
        result = result.rename(columns={'score': 'score_anchor'})
        result['score_ensemble'] = lime_weight * result['score_lime'] + shap_weight * result['score_shap'] + anchor_weight * result['score_anchor']
        result = result.sort_values(by='score_ensemble', ascending=False).reset_index(drop=True)

        explanation = EnsembleExplanation(result, {'lime': lime_weight, 'shap': shap_weight, 'anchor': anchor_weight})
        return explanation
