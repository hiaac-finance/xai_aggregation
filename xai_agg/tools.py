import time
from typing import Literal, Type, Callable

import numpy as np
import pandas as pd
import math

from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors

# Explainable AI tools:
from .explainers import *

from scipy.stats import spearmanr, pearsonr

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Concunrrency:
import concurrent.futures
from pathos.multiprocessing import ProcessingPool as Pool
# from .mp import NoDaemonProcessPool as Pool
# from pathos.multiprocessing import ThreadPool as Pool
# from multiprocessing import Pool as ProcessPool

# import multiprocess.context as ctx
# ctx._force_start_method('spawn')


class AutoencoderNoisyDataGenerator():
    """
    This class is used to generate a noisy variation of a given tabular dataset by swapping the values of a small number of features
    between a sample and a random close neighbor. The neighbors are determined using an autoencoder to reduce the dimensionality of 
        the data and then calculate the use the NearestNeightbors algorithm in the reduced space.
    """

    def __init__(self, X: pd.DataFrame, ohe_categorical_features_names: list[str], encoding_dim: int = 5, epochs=500, percent_features_to_replace: float = 0.1):
        self.X = X
        self.categorical_features_names = ohe_categorical_features_names
        self.encoding_dim = encoding_dim
        self.epochs = epochs
        self.replace_features_percent = percent_features_to_replace

        scaler = StandardScaler()
        self.X_scaled = scaler.fit_transform(self.X)
        
        input_dim = self.X_scaled.shape[1]

        input_layer = Input(shape=(input_dim,))
        encoded = Dense(self.encoding_dim, activation='relu')(input_layer)
        decoded = Dense(input_dim, activation='sigmoid')(encoded)

        self.autoencoder = Model(inputs=input_layer, outputs=decoded)
        self.encoder = Model(inputs=input_layer, outputs=encoded)
        self.was_fit = False
        
    
    def fit(self):
        self.autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')
        self.autoencoder.fit(self.X_scaled, self.X_scaled, epochs=self.epochs, batch_size=32, shuffle=True, validation_split=0.2)
        # Extract hidden layer representation:
        self.hidden_representation = self.encoder.predict(self.X_scaled)
        self.was_fit = True


    def generate_noisy_data(self, num_features_to_replace: int = None) -> pd.DataFrame:
        """
        Returns a DataFrame containing a noisy variation of the data.

        The noise is generated by swapping the values of a small number of fea tures between a sample and a random close neighbor.
        To determine the neighbors, we use an autoencoder to reduce the dimensionality of the data and then calculate the use the NearestNeightbors algorithm in the reduced space.
        """

        if not self.was_fit:
            raise ValueError('The autoencoder has not been fitted yet. Call the fit() method before generating noisy data.')
        
        if not num_features_to_replace:
            num_features_to_replace = math.ceil(self.X.shape[1] * self.replace_features_percent)

        # Compute Nearest Neighbors using hidden_representation
        nbrs = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(self.hidden_representation)
        distances, indices = nbrs.kneighbors(self.hidden_representation)

        X_noisy = self.X.copy()

        # Get id's of columns that belong to the same categorical feature (after being one-hot-encodeded);
        # Columns that belong to the same categorical feature start with the same name, and will be treated as a single feature when adding noise.
        categorical_features_indices = [
            [self.X.columns.get_loc(col_name) for col_name in self.X.columns if col_name.startswith(feature)]
            for feature in self.categorical_features_names
        ]

        # Replace features with random neighbor's features
        for i in range(self.X.shape[0]):  # Iterate over each sample
            available_features_to_replace = list(range(self.X.shape[1]))
            for j in range(num_features_to_replace):
                # Select features to replace; if the feture selected belong to one of the lists in categorical_features_indices, we will replace all the features in that list
                features_to_replace = np.random.choice(available_features_to_replace, 1)
                for feature_indices in categorical_features_indices:
                    if features_to_replace in feature_indices:
                        features_to_replace = feature_indices
                        break
                
                # Remove the selected features from the list of available features to replace
                available_features_to_replace = [f for f in available_features_to_replace if f not in features_to_replace]

                # Choose a random neighbor from the nearest neighbors
                neighbor_idx = np.random.choice(indices[i][1:])

                # Replace the selected features with the neighbor's features
                X_noisy.iloc[i, features_to_replace] = self.X.iloc[neighbor_idx, features_to_replace]

        return X_noisy

class ExplanationModelEvaluator:
    """
    This class defines a set of metrics to evaluate an explantion model's performance on a given data instance. THIS CLASS MUST BE INITIALIZED BEFORE USE.

    The metrics are:
    - Faithfullness correlation: The correlation between the importance of the features in the explanation and the change in the model's output when the features are perturbed.
    - Sensitivity: The relationship (average difference or correlation) between the explanation of the instance and the explanation of a noisy version of the instance.
    - Complexity: The complexity of the explanation, calculated as the entropy of the explanation's feature importance distribution.

    The class can be used to evaluate the performance of different explanation methods, or to evaluate the performance of an aggregated explainer.

    Attributes:
        - clf (object): The classifier model that will be explained.
        - X_train (pd.DataFrame | np.ndarray): The training data used to train the classifier.
        - ohe_categorical_feature_names (list[str]): The names of the categorical features that were one-hot-encoded.
        - predict_proba (callable): A function that receives a data row and returns the model's prediction probabilities.
        - noise_gen_args (dict): A dictionary containing the arguments to be passed to the AutoencoderNoisyDataGenerator class.
    """

    IS_METRIC_HIGHER_BETTER = {
        "complexity": False,
        "sensitivity_spearman": True,
        "faithfulness_corr": True,
        "nrc": False,
        "nrc_old": False,
        "rb_faithfulness_corr": True
    }

    def __init__(self, model, X_train: pd.DataFrame | np.ndarray, ohe_categorical_feature_names: list[str] = [], predict_fn: callable = None,
                 noise_gen_args: dict = {}, debug=False, jobs = None, **kwargs):
        self.model = model
        if predict_fn is None:
            if hasattr(self.model, 'predict_proba'):
                self.predict_fn = self.model.predict_proba
            elif hasattr(self.model, 'predict'):
                self.predict_fn = self.model.predict
            else:
                raise ValueError('Could not find a predict or predict_proba method in the model. Please provide a value for the predict_fn parameter.')
        else:
            self.predict_fn = predict_fn

        self.X_train = X_train
        self.ohe_categorical_feature_names = ohe_categorical_feature_names

        self.categorical_features_indices = [
            [self.X_train.columns.get_loc(col_name) for col_name in self.X_train.columns if col_name.startswith(feature)]
            for feature in self.ohe_categorical_feature_names
        ]

        self.noisy_data_generator = AutoencoderNoisyDataGenerator(X_train, ohe_categorical_feature_names, **noise_gen_args)
        self.jobs = jobs

        self.debug = debug
        self.was_initialized = False
        
    
    # Initialization opeations that take a long time to run
    def init(self):
        self.noisy_data_generator.fit()
        self.was_initialized = True
            
    def faithfullness_correlation(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, len_subset: int = None,
                                  iterations: int = 10, baseline_strategy: Literal["zeros", "mean"] = "zeros", rank_based = False,
                                  rb_alg: Literal["sum", "percentile", "avg", "inverse"] = "percentile", explanation: DataFrame[ExplanationModel] = None) -> float:
        """
        This metric measures the correlation between the importance of the features in the explanation and the change in the model's output when the features are perturbed.
        Referenced from: https://arxiv.org/abs/2005.00631

        Parameters:
            explainer (ExplainerWrapper | Type[ExplainerWrapper]): The explainer object or class to be evaluated. Must be passed if explanation is None.
            instance_data_row (pd.Series): The instance to be explained. Must be passed if explanation is None.
            explanation: The explanation of the instance. If None, the explainer will be used to generate the explanation, and both the explainer and the instance_data_row parameters must be passed.
            len_subset (int): The number of features to perturb in each iteration. If None, the default value is len(instance_data_row)//4 (25% of the features).
            iterations (int): The number of iterations to run the metric calculation. The higher the number of iterations, the more accurate the result.
            baseline_strategy (str): The strategy to be used to generate the baseline values for the perturbed features. Options are "zeros" (all zeros) or "mean" (mean of the training data).
                                     "mean" usually provides hihger correlation values, but "zeros" is more conservative.
        """

        if explanation is None:
            if not isinstance(explainer, ExplainerWrapper):
                explainer = explainer(self.model, self.X_train, self.ohe_categorical_feature_names, predict_fn=self.predict_fn)
            
            explanation = explanation if explanation is not None else explainer.explain_instance(instance_data_row)

        importance_sums = []
        delta_fs = []
        
        # Done this way so it'll work both on regression and on classification
        prediction = self.predict_fn(np.array(instance_data_row).reshape(1, -1))[0]
        predicted_index = np.argmax(prediction)
        f_x = prediction[predicted_index] if isinstance(prediction, (list, np.ndarray)) else prediction

        for _ in range(iterations):
            evaluation = self._evaluate_faithfullness_iteration(instance_data_row, explanation, f_x, predicted_index, len_subset, baseline_strategy, rank_based, rb_alg)
            importance_sums.append(evaluation[0])
            delta_fs.append(evaluation[1])
        
        return abs(pearsonr(importance_sums, delta_fs)[0])

    def _evaluate_faithfullness_iteration(self, instance_data_row, g_x, f_x, predicted_index, len_subset, baseline_strategy, rank_based: bool = False,
                                          rb_alg: Literal["sum", "percentile", "avg", "inverse"] = "sum") -> tuple[float, float]:
        subset = np.random.choice(instance_data_row.index.values, len_subset if len_subset else len(instance_data_row) // 4, replace=False)
        perturbed_instance = instance_data_row.copy()

        if baseline_strategy == "zeros":
            baseline = np.zeros(len(instance_data_row))
        elif baseline_strategy == "mean":
            baseline = np.mean(self.X_train, axis=0)
            for feature_index in self.categorical_features_indices:
                baseline[feature_index] = 0

        perturbed_instance[subset] = baseline[instance_data_row.index.get_indexer(subset)]

        subset_g_x = g_x[g_x['feature'].isin(subset)]
        subset_feature_importances = subset_g_x['score'].values

        if not rank_based:
            combined_importance = sum(subset_feature_importances)
        else:
            sfi_ranking = get_ranked_explanation(subset_g_x)
            if rb_alg == "sum":
                combined_importance = -sfi_ranking['rank'].sum()
            elif rb_alg == "percentile":
                percentiles = 1 - (sfi_ranking['rank'] / sfi_ranking['rank'].values[-1])
                combined_importance = percentiles.sum()
            elif rb_alg == "avg":
                combined_importance = -sfi_ranking['rank'].mean()
            elif rb_alg == "inverse":
                combined_importance = (1 / sfi_ranking['rank']).sum()

        # Done this way so it'll work both on regression and on classification
        prediction = self.predict_fn(perturbed_instance.to_numpy().reshape(1, -1))[0]
        f_x_perturbed = prediction[predicted_index] if isinstance(prediction, (list, np.ndarray)) else prediction
        delta_f = np.abs(f_x - f_x_perturbed)

        return combined_importance, delta_f

    def sensitivity(self, ExplainerType: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, iterations: int = 10, method: Literal['mean_squared', 'spearman', 'pearson'] = 'spearman',
                    custom_method: Callable[[pd.DataFrame, pd.DataFrame], float] = None, extra_explainer_params: dict = {}) -> float:
        """
        Concurrent variation of the sensitivity method. This metric measures the relationship (average difference or correlation) between the explanation of the instance and the explanation of a noisy version of the instance.
        The explainer is instantiated twice: once to explain the original instance and once to explain the noisy instance, since it may need to fit or train itself with the data.

        Beware: depending on the method used, the metric can either be a cost function (the lower the better: mean_squared) or a reward function (the higher the better: spearman, person).

        Parameters:
            - ExplainerType (ExplainerWrapper | Type[ExplainerWrapper]): The explainer object or class to be evaluated.
            - instance_data_row (pd.Series): The instance to be explained.
            - iterations (int): The number of iterations to run the metric calculation. The higher the number of iterations, the more accurate the result.
            - method (str): The method to be used to calculate the sensitivity. Options are "mean_squared", "spearman", and "pearson".
            - custom_method (Callable[[pd.DataFrame, pd.DataFrame], float]): A custom method to calculate the sensitivity. If provided, the method parameter will be ignored.
            - extra_explainer_params (dict): A dictionary containing the parameters to be passed to the explainer class, in case it requires additional parameters.
        """

        if not self.was_initialized:
            raise ValueError('The XaiEvaluator has not been initialized yet. Call the init() method before evaluating sensitivity.')

        if isinstance(ExplainerType, ExplainerWrapper):
            ExplainerType = ExplainerType.__class__

        original_explainer = ExplainerType(model=self.model, X_train=self.X_train, categorical_feature_names=self.ohe_categorical_feature_names,
                                           predict_fn=self.predict_fn, **extra_explainer_params)

        with Pool(processes = self.jobs) as executor:
            results = executor.map(
                self._evaluate_sensitivity_iteration,
                [original_explainer] * iterations,
                [instance_data_row] * iterations,
                [ExplainerType] * iterations,
                [method] * iterations,
                [custom_method] * iterations,
                [extra_explainer_params] * iterations
            )

        return np.mean(results)

    def _evaluate_sensitivity_iteration(self, original_explainer, instance_data_row, ExplainerType: Type[ExplainerWrapper], method, custom_method, extra_explainer_params):
        # Obtain the original explanation:
        original_explanation = original_explainer.explain_instance(instance_data_row)

        # Obtain the noisy explanation:
        noisy_data = self.noisy_data_generator.generate_noisy_data()
        noisy_explainer = ExplainerType(model=self.model, X_train=noisy_data, categorical_feature_names=self.ohe_categorical_feature_names, predict_fn=self.predict_fn, **extra_explainer_params)
        noisy_explanation = noisy_explainer.explain_instance(instance_data_row)

        # Align the two explanations
        noisy_explanation = noisy_explanation.set_index('feature').loc[original_explanation['feature']].reset_index()

        if custom_method is not None:
            return custom_method(original_explanation, noisy_explanation)
        elif method == 'mean_squared':
            mean_squared_difference = ((original_explanation['score'] - noisy_explanation['score']) ** 2).mean()
            return mean_squared_difference
        elif method == 'spearman':
            spearman_correlation = spearmanr(original_explanation['score'], noisy_explanation['score']).correlation
            return abs(spearman_correlation)
        elif method == 'pearson':
            pearson_correlation = pearsonr(original_explanation['score'], noisy_explanation['score']).correlation
            return abs(pearson_correlation)
    
    def _sensitivity_sequential(self, ExplainerType: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, iterations: int = 10, method: Literal['mean_squared', 'spearman', 'pearson'] = 'spearman',
                    custom_method: Callable[[pd.DataFrame, pd.DataFrame], float]=None, extra_explainer_params: dict = {}) -> float:
        """
        Sequential version of sensitivity()
        """

        if not self.was_initialized:
            raise ValueError('The XaiEvaluator has not been initialized yet. Call the init() method before evaluating sensitivity.')
        
        if isinstance(ExplainerType, ExplainerWrapper):
            ExplainerType = ExplainerType.__class__
        
        original_explainer = ExplainerType(model=self.model, X_train=self.X_train, categorical_feature_names=self.ohe_categorical_feature_names, predict_fn=self.predict_fn, **extra_explainer_params)

        results: list[float] = []
        for _ in range(iterations):
            results.append(
                self._evaluate_sensitivity_iteration(
                    original_explainer, instance_data_row, ExplainerType, method, custom_method, extra_explainer_params
                )
            )
        
        return np.mean(results)

    def complexity(self, explainer: ExplainerWrapper | Type[ExplainerWrapper] = None, instance_data_row: pd.Series = None,
                   explanation: DataFrame[ExplanationModel] = None, **kwargs) -> float:
        """
        This metric is calculated as the entropy of the explanation's feature importance distribution.
        Referenced from: https://arxiv.org/abs/2005.00631
        
        Parameters:
            - explainer (ExplainerWrapper | Type[ExplainerWrapper]): The explainer object or class to be evaluated. Must be passed if explanation is None.
            - instance_data_row (pd.Series): The instance to be explained. Must be passed if explanation is None.
            - explanation: The explanation of the instance. If None, the explainer will be used to generate the explanation, and both the explainer and the instance_data_row parameters must be passed.
        """
        
        assert explanation is not None or (explainer is not None and instance_data_row is not None), "Either an explanation or both an explainer and an instance_data_row must be provided."

        if explanation is None:
            if not isinstance(explainer, ExplainerWrapper):
                explainer = explainer(self.model, self.X_train, self.ohe_categorical_feature_names, predict_fn=self.predict_fn)
            
            explanation = explanation if explanation is not None else explainer.explain_instance(instance_data_row)
        
        def frac_contribution(explanation: pd.DataFrame, i: int) -> float:
            abs_score_sum = explanation['score'].abs().sum()
            return explanation['score'].abs()[i] / abs_score_sum

        sum = 0
        for i in range(explanation.shape[0]):
            fc = frac_contribution(explanation, i)
            sum += fc * np.log(fc) if fc > 0 else 0
            
        return -sum
    
    def nrc_old(self, explainer: ExplainerWrapper | Type[ExplainerWrapper] = None, instance_data_row: pd.Series = None, alpha: float = 0.5,
                explanation: DataFrame[ExplanationModel] = None) -> float:
        assert explanation is not None or (explainer is not None and instance_data_row is not None), "Either an explanation or both an explainer and an instance_data_row must be provided."
        
        if explanation is None:
            if not isinstance(explainer, ExplainerWrapper):
                explainer = explainer(self.model, self.X_train, self.ohe_categorical_feature_names, predict_fn=self.predict_fn)
                
            explanation = explainer.explain_instance(instance_data_row)

        attributions = explanation['score'].values
        attributions = np.abs(attributions) / np.sum(np.abs(attributions))

        ranks = get_ranked_explanation(explanation)['rank'].values
        reciprocal_ranks = 1 / ranks
        sum_reciprocal_ranks = sum(reciprocal_ranks)
        sum_attributions = sum(attributions)
        k = np.count_nonzero(attributions)

        if k == 0:
            return 0

        log_weight = math.log(k + 1)
        rank_dispersion = np.std(ranks)
        revised_nrc_value = (sum_reciprocal_ranks / sum_attributions) * log_weight * (1 + alpha * rank_dispersion)
        
        return revised_nrc_value

    def nrc(self, explainer: ExplainerWrapper | Type[ExplainerWrapper] = None, instance_data_row: pd.Series = None, alpha: float = 0.5,
            explanation: DataFrame[ExplanationModel] = None) -> float:
        """
        New proposed metric: NRC (Normalized Ratio of Complexity) with dispersion penalty.
        
        Parameters:
            - explainer (ExplainerWrapper | Type[ExplainerWrapper]): The explainer object or class to be evaluated. Must be passed if explanation is None.
            - instance_data_row (pd.Series): The instance to be explained. Must be passed if explanation is None.
            - alpha (float): The dispersion penalty factor. The higher the value, the more the metric will penalize explanations with high rank dispersion.
            - explanation: The explanation of the instance. If None, the explainer will be used to generate the explanation, and both the explainer and the instance_data_row parameters must be passed.
        """
        
        assert explanation is not None or (explainer is not None and instance_data_row is not None), "Either an explanation or both an explainer and an instance_data_row must be provided."

        if explanation is None:
            if not isinstance(explainer, ExplainerWrapper):
                explainer = explainer(self.model, self.X_train, self.ohe_categorical_feature_names, predict_fn=self.predict_fn)
            
            explanation = explainer.explain_instance(instance_data_row)

        ranks = get_ranked_explanation(explanation)['rank'].tolist()
        ranks = np.array(ranks, dtype=int)
        reciprocal_ranks = 1 / ranks
        sum_reciprocal_ranks = np.sum(reciprocal_ranks)
        n = len(ranks)

        if n == 0:
            return 0

        log_weight = math.log(n + 1)
        rank_dispersion = np.std(ranks)
        revised_nrc_value = sum_reciprocal_ranks * log_weight * (1 + alpha * rank_dispersion)

        return revised_nrc_value  


import pandera as pa
from pandera.typing import DataFrame

class RankedExplanationModel(pa.DataFrameModel):
    feature: str
    rank: int = pa.Field(ge=0)

def get_ranked_explanation(scored_explanation: DataFrame[ExplanationModel], fraction: float = None, method: Literal["std", "spread"] = "std", epsilon: float = None) -> DataFrame[RankedExplanationModel]:
    """
    Assigns a rank to each feature based on the score in the ranking dataframe. Features with similar scores are assigned the same rank.
    
    Parameters:
    scored_explanation (DataFrame[ExplanationModel]): DataFrame containing the feature importance scores (two columns: 'feature' and 'score').
    fraction (float): The fraction of the score range to use as epsilon. If epsilon is not provided, it is calculated based on this fraction.
    method (str): The method to use to calculate epsilon. Can be 'std' (standard deviation) or 'spread' (score range).
    epsilon (float): The epsilon value to use. If provided, the fraction parameter is ignored.

    Returns:
    A pd.DataFrame containing the features ('feature' column) and their respective ranks ('rank' column).
    """

    # Calculating epsilon
    if epsilon is None:
        if method == "std":
            if fraction is None:
                fraction = 0.05

            epsilon = scored_explanation["score"].std() * fraction
        elif method == "spread":
            if fraction is None:
                fraction = 0.025
            epsilon = (scored_explanation["score"].max() - scored_explanation["score"].min()) * fraction

    # Sort the ranking dataframe by score in descending order
    scored_explanation = scored_explanation.sort_values(by='score', ascending=False).reset_index(drop=True)
    
    # If two features have a score difference smaller than epsilon, they are considered to have the same rank
    ranked_explanation = pd.DataFrame(columns=['feature', 'rank'])
    ranked_explanation['feature'] = scored_explanation["feature"]
    
    current_rank = 1
    ranked_explanation['rank'] = 0
    ranked_explanation.at[0, 'rank'] = current_rank
    max_score_in_rank = scored_explanation.at[0, 'score']
    
    for i in range(1, len(scored_explanation)):
        if abs(scored_explanation.at[i, 'score'] - max_score_in_rank) < epsilon:
            ranked_explanation.at[i, 'rank'] = current_rank
        else:
            current_rank += 1
            ranked_explanation.at[i, 'rank'] = current_rank
            max_score_in_rank = scored_explanation.at[i, 'score']
    
    return DataFrame[RankedExplanationModel](ranked_explanation)
