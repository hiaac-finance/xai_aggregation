import time
from typing import Literal, Type, Callable

import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors

# Explainable AI tools:
from .explainers import *

from scipy.stats import spearmanr, pearsonr

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Concunrrency:
import concurrent.futures
from pathos.multiprocessing import ProcessingPool as Pool
# from .mp import NoDaemonProcessPool as Pool
# from pathos.multiprocessing import ThreadPool as Pool
# from multiprocessing import Pool as ProcessPool

# import multiprocess.context as ctx
# ctx._force_start_method('spawn')


class AutoencoderNoisyDataGenerator():
    """
    This class generates noisy data by swapping the values of a small number of features between a sample and a random close neighbor.
    The neighbors are determined using an autoencoder to reduce the dimensionality of the data and then calculate the use the NearestNeightbors algorithm in the reduced space.
    """

    def __init__(self, X: pd.DataFrame, ohe_categorical_features_names: list[str], encoding_dim: int = 5, epochs=500):
        self.X = X
        self.categorical_features_names = ohe_categorical_features_names
        self.encoding_dim = encoding_dim
        self.epochs = epochs

        scaler = StandardScaler()
        self.X_scaled = scaler.fit_transform(self.X)
        
        input_dim = self.X_scaled.shape[1]

        input_layer = Input(shape=(input_dim,))
        encoded = Dense(self.encoding_dim, activation='relu')(input_layer)
        decoded = Dense(input_dim, activation='sigmoid')(encoded)

        self.autoencoder = Model(inputs=input_layer, outputs=decoded)
        self.encoder = Model(inputs=input_layer, outputs=encoded)
        self.was_fit = False
        
    
    def fit(self):
        self.autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')
        self.autoencoder.fit(self.X_scaled, self.X_scaled, epochs=self.epochs, batch_size=32, shuffle=True, validation_split=0.2)
        # Extract hidden layer representation:
        self.hidden_representation = self.encoder.predict(self.X_scaled)
        self.was_fit = True


    def generate_noisy_data(self, num_features_to_replace: int = 2) -> pd.DataFrame:
        """
        Returns a DataFrame containing a noisy variation of the data.

        The noise is generated by swapping the values of a small number of features between a sample and a random close neighbor.
        To determine the neighbors, we use an autoencoder to reduce the dimensionality of the data and then calculate the use the NearestNeightbors algorithm in the reduced space.
        """

        if not self.was_fit:
            raise ValueError('The autoencoder has not been fitted yet. Call the fit() method before generating noisy data.')

        # Compute Nearest Neighbors using hidden_representation
        nbrs = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(self.hidden_representation)
        distances, indices = nbrs.kneighbors(self.hidden_representation)

        X_noisy = self.X.copy()

        # Get id's of columns that belong to the same categorical feature (after being one-hot-encodeded);
        # Columns that belong to the same categorical feature start with the same name, and will be treated as a single feature when adding noise.
        categorical_features_indices = [
            [self.X.columns.get_loc(col_name) for col_name in self.X.columns if col_name.startswith(feature)]
            for feature in self.categorical_features_names
        ]

        # Replace features with random neighbor's features
        for i in range(self.X.shape[0]):  # Iterate over each sample
            available_features_to_replace = list(range(self.X.shape[1]))
            for j in range(num_features_to_replace):
                # Select features to replace; if the feture selected belong to one of the lists in categorical_features_indices, we will replace all the features in that list
                features_to_replace = np.random.choice(available_features_to_replace, 1)
                for feature_indices in categorical_features_indices:
                    if features_to_replace in feature_indices:
                        features_to_replace = feature_indices
                        break
                
                # Remove the selected features from the list of available features to replace
                available_features_to_replace = [f for f in available_features_to_replace if f not in features_to_replace]

                # Choose a random neighbor from the nearest neighbors
                neighbor_idx = np.random.choice(indices[i][1:])

                # Replace the selected features with the neighbor's features
                X_noisy.iloc[i, features_to_replace] = self.X.iloc[neighbor_idx, features_to_replace]

        return X_noisy

class ExplanationModelEvaluator:
    """
    This class defines a set of metrics to evaluate an explantion model's performance on a given data instance. THIS CLASS MUST BE INITIALIZED BEFORE USE.

    The metrics are:
    - Faithfullness correlation: The correlation between the importance of the features in the explanation and the change in the model's output when the features are perturbed.
    - Sensitivity: The relationship (average difference or correlation) between the explanation of the instance and the explanation of a noisy version of the instance.
    - Complexity: The complexity of the explanation, calculated as the entropy of the explanation's feature importance distribution.

    The class can be used to evaluate the performance of different explanation methods, or to evaluate the performance of an aggregated explainer.

    Attributes:
        - clf (object): The classifier model that will be explained.
        - X_train (pd.DataFrame | np.ndarray): The training data used to train the classifier.
        - ohe_categorical_feature_names (list[str]): The names of the categorical features that were one-hot-encoded.
        - predict_proba (callable): A function that receives a data row and returns the model's prediction probabilities.
        - noise_gen_args (dict): A dictionary containing the arguments to be passed to the AutoencoderNoisyDataGenerator class.
    """

    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, ohe_categorical_feature_names: list[str], predict_proba: callable = None, noise_gen_args: dict = {}, debug=False, **kwargs):
        self.clf = clf
        if hasattr(clf, 'predict_proba') and predict_proba is None:
            self.predict_proba = clf.predict_proba
        elif predict_proba is not None:
            self.predict_proba = predict_proba
        else:
            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')

        self.X_train = X_train
        self.ohe_categorical_feature_names = ohe_categorical_feature_names

        self.categorical_features_indices = [
            [self.X_train.columns.get_loc(col_name) for col_name in self.X_train.columns if col_name.startswith(feature)]
            for feature in self.ohe_categorical_feature_names
        ]

        self.noisy_data_generator = AutoencoderNoisyDataGenerator(X_train, ohe_categorical_feature_names, **noise_gen_args)

        self.debug = debug
        self.was_initialized = False
        
    
    # Initialization opeations that take a long time to run
    def init(self):
        self.noisy_data_generator.fit()
        self.was_initialized = True
            
    def faithfullness_correlation(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, len_subset: int = None,
                                  iterations: int = 100, baseline_strategy: Literal["zeros", "mean"] = "zeros") -> float:
        """
        This metric measures the correlation between the importance of the features in the explanation and the change in the model's output when the features are perturbed.
        Referenced from: https://arxiv.org/abs/2005.00631

        Parameters:
            explainer (ExplainerWrapper | Type[ExplainerWrapper]): The explainer object or class to be evaluated.
            instance_data_row (pd.Series): The instance to be explained.
            len_subset (int): The number of features to perturb in each iteration. If None, the default value is len(instance_data_row)//4 (25% of the features).
            iterations (int): The number of iterations to run the metric calculation. The higher the number of iterations, the more accurate the result.
            baseline_strategy (str): The strategy to be used to generate the baseline values for the perturbed features. Options are "zeros" (all zeros) or "mean" (mean of the training data).
                                     "mean" usually provides hihger correlation values, but "zeros" is more conservative.
        """

        if not isinstance(explainer, ExplainerWrapper):
            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)
        
        dimension = len(instance_data_row)  

        importance_sums = []
        delta_fs = []

        f_x = self.predict_proba(instance_data_row.to_numpy().reshape(1, -1))[0][1]
        g_x = explainer.explain_instance(instance_data_row)

        for _ in range(iterations):
            if self.debug:
                print(f"[faithfullness_correlation()]: Iteration {_}")

            # Select a subset of features to perturb
            subset = np.random.choice(instance_data_row.index.values, len_subset if len_subset else dimension//4, replace=False)

            perturbed_instance = instance_data_row.copy()

            if baseline_strategy == "zeros":
                baseline = np.zeros(dimension)  # either mean on all zeros
            elif baseline_strategy == "mean":
                baseline = np.mean(self.X_train, axis=0)
                for feature_index in self.categorical_features_indices:
                    baseline[feature_index] = 0
                
            perturbed_instance[subset] = baseline[instance_data_row.index.get_indexer(subset)]

            importance_sum = 0
            for feature in subset:
                importance_sum += g_x[g_x['feature'] == feature]['score'].values[0] # should I take the abs value here?
            importance_sums.append(importance_sum)

            f_x_perturbed = self.predict_proba(perturbed_instance.to_numpy().reshape(1, -1))[0][1]
            delta_f = np.abs(f_x - f_x_perturbed)
            delta_fs.append(delta_f)
        
        return abs(pearsonr(importance_sums, delta_fs).statistic)

    def _faithfullness_correlation_concurrent(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, len_subset: int = None,
                                  iterations: int = 100, baseline_strategy: Literal["zeros", "mean"] = "zeros") -> float:
        """
        This metric measures the correlation between the importance of the features in the explanation and the change in the model's output when the features are perturbed.
        Referenced from: https://arxiv.org/abs/2005.00631

        Parameters:
            explainer (ExplainerWrapper | Type[ExplainerWrapper]): The explainer object or class to be evaluated.
            instance_data_row (pd.Series): The instance to be explained.
            len_subset (int): The number of features to perturb in each iteration. If None, the default value is len(instance_data_row)//4 (25% of the features).
            iterations (int): The number of iterations to run the metric calculation. The higher the number of iterations, the more accurate the result.
            baseline_strategy (str): The strategy to be used to generate the baseline values for the perturbed features. Options are "zeros" (all zeros) or "mean" (mean of the training data).
                                     "mean" usually provides higher correlation values, but "zeros" is more conservative.
        """

        if not isinstance(explainer, ExplainerWrapper):
            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)

        f_x = self.predict_proba(instance_data_row.to_numpy().reshape(1, -1))[0][1]
        g_x = explainer.explain_instance(instance_data_row)

        with concurrent.futures.ProcessPoolExecutor() as executor:
            futures = [
                executor.submit(
                    self._evaluate_faithfullness_iteration,
                    instance_data_row,
                    g_x,
                    f_x,
                    len_subset,
                    baseline_strategy
                )
                for _ in range(iterations)
            ]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]

        importance_sums, delta_fs = zip(*results)
        return abs(pearsonr(importance_sums, delta_fs).statistic)

    def _evaluate_faithfullness_iteration(self, instance_data_row, g_x, f_x, len_subset, baseline_strategy):
        subset = np.random.choice(instance_data_row.index.values, len_subset if len_subset else len(instance_data_row) // 4, replace=False)
        perturbed_instance = instance_data_row.copy()

        if baseline_strategy == "zeros":
            baseline = np.zeros(len(instance_data_row))
        elif baseline_strategy == "mean":
            baseline = np.mean(self.X_train, axis=0)
            for feature_index in self.categorical_features_indices:
                baseline[feature_index] = 0

        perturbed_instance[subset] = baseline[instance_data_row.index.get_indexer(subset)]

        importance_sum = sum(g_x[g_x['feature'] == feature]['score'].values[0] for feature in subset)
        f_x_perturbed = self.predict_proba(perturbed_instance.to_numpy().reshape(1, -1))[0][1]
        delta_f = np.abs(f_x - f_x_perturbed)

        return importance_sum, delta_f
    
    def _sensitivity_sequential(self, ExplainerType: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, iterations: int = 10, method: Literal['mean_squared', 'spearman', 'pearson'] = 'spearman',
                    custom_method: Callable[[pd.DataFrame, pd.DataFrame], float]=None, extra_explainer_params: dict = {}) -> float:
        """
        Sequential version of sensitivity()
        """

        if not self.was_initialized:
            raise ValueError('The XaiEvaluator has not been initialized yet. Call the init() method before evaluating sensitivity.')
        
        if isinstance(ExplainerType, ExplainerWrapper):
            ExplainerType = ExplainerType.__class__
        
        original_explainer = ExplainerType(clf=self.clf, X_train=self.X_train, categorical_feature_names=self.ohe_categorical_feature_names, predict_proba=self.predict_proba, **extra_explainer_params)

        results: list[float] = []
        for _ in range(iterations):
            if self.debug:
                print(f"[sensitivity()]: Iteration {_}")

            # Obtain the original explanation:
            original_explanation = original_explainer.explain_instance(instance_data_row)

            # Obtain the noisy explanation:
            noisy_data = self.noisy_data_generator.generate_noisy_data(num_features_to_replace=2)
            noisy_explainer = ExplainerType(clf=self.clf, X_train=noisy_data, categorical_feature_names=self.ohe_categorical_feature_names, predict_proba=self.predict_proba, **extra_explainer_params)
            noisy_explanation = noisy_explainer.explain_instance(instance_data_row)

            if custom_method is not None:
                results.append(custom_method(original_explanation, noisy_explanation))
            elif method == 'mean_squared':
                mean_squared_difference = ((original_explanation['score'] - noisy_explanation['score']) ** 2).mean()
                results.append(mean_squared_difference)
            elif method == 'spearman':
                spearman_correlation = spearmanr(original_explanation['score'], noisy_explanation['score']).correlation
                results.append(abs(spearman_correlation))
            elif method == 'pearson':
                pearson_correlation = pearsonr(original_explanation['score'], noisy_explanation['score']).correlation
                results.append(abs(pearson_correlation))
        
        return np.mean(results)

    def sensitivity(self, ExplainerType: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, iterations: int = 10, method: Literal['mean_squared', 'spearman', 'pearson'] = 'spearman',
                               custom_method: Callable[[pd.DataFrame, pd.DataFrame], float] = None, extra_explainer_params: dict = {}) -> float:
        """
        Concurrent variation of the sensitivity method. This metric measures the relationship (average difference or correlation) between the explanation of the instance and the explanation of a noisy version of the instance.
        The explainer is instantiated twice: once to explain the original instance and once to explain the noisy instance, since it may need to fit or train itself with the data.

        Beware: depending on the method used, the metric can either be a cost function (the lower the better: mean_squared) or a reward function (the higher the better: spearman, person).

        Parameters:
            - ExplainerType (ExplainerWrapper | Type[ExplainerWrapper]): The explainer object or class to be evaluated.
            - instance_data_row (pd.Series): The instance to be explained.
            - iterations (int): The number of iterations to run the metric calculation. The higher the number of iterations, the more accurate the result.
            - method (str): The method to be used to calculate the sensitivity. Options are "mean_squared", "spearman", and "pearson".
            - custom_method (Callable[[pd.DataFrame, pd.DataFrame], float]): A custom method to calculate the sensitivity. If provided, the method parameter will be ignored.
            - extra_explainer_params (dict): A dictionary containing the parameters to be passed to the explainer class, in case it requires additional parameters.
        """

        if not self.was_initialized:
            raise ValueError('The XaiEvaluator has not been initialized yet. Call the init() method before evaluating sensitivity.')

        if isinstance(ExplainerType, ExplainerWrapper):
            ExplainerType = ExplainerType.__class__

        original_explainer = ExplainerType(clf=self.clf, X_train=self.X_train, categorical_feature_names=self.ohe_categorical_feature_names, predict_proba=self.predict_proba, **extra_explainer_params)

        # with concurrent.futures.ProcessPoolExecutor() as executor:
        #     futures = [
        #         executor.submit(
        #             self._evaluate_sensitivity_iteration,
        #             original_explainer,
        #             instance_data_row,
        #             ExplainerType,
        #             method,
        #             custom_method,
        #             extra_explainer_params
        #         )
        #         for _ in range(iterations)
        #     ]
        #     results = [future.result() for future in concurrent.futures.as_completed(futures)]

        with Pool() as executor:
            results = executor.map(
                self._evaluate_sensitivity_iteration,
                [original_explainer] * iterations,
                [instance_data_row] * iterations,
                [ExplainerType] * iterations,
                [method] * iterations,
                [custom_method] * iterations,
                [extra_explainer_params] * iterations
            )

        return np.mean(results)

    def _evaluate_sensitivity_iteration(self, original_explainer, instance_data_row, ExplainerType, method, custom_method, extra_explainer_params):
        if self.debug:
            print(f"[sensitivity_concurrent()]: Iteration")

        # Obtain the original explanation:
        original_explanation = original_explainer.explain_instance(instance_data_row)

        # Obtain the noisy explanation:
        noisy_data = self.noisy_data_generator.generate_noisy_data(num_features_to_replace=2)
        noisy_explainer = ExplainerType(clf=self.clf, X_train=noisy_data, categorical_feature_names=self.ohe_categorical_feature_names, predict_proba=self.predict_proba, **extra_explainer_params)
        noisy_explanation = noisy_explainer.explain_instance(instance_data_row)

        if custom_method is not None:
            return custom_method(original_explanation, noisy_explanation)
        elif method == 'mean_squared':
            mean_squared_difference = ((original_explanation['score'] - noisy_explanation['score']) ** 2).mean()
            return mean_squared_difference
        elif method == 'spearman':
            spearman_correlation = spearmanr(original_explanation['score'], noisy_explanation['score']).correlation
            return abs(spearman_correlation)
        elif method == 'pearson':
            pearson_correlation = pearsonr(original_explanation['score'], noisy_explanation['score']).correlation
            return abs(pearson_correlation)

    def complexity(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, **kwargs) -> float:
        """
        This metric is calculated as the entropy of the explanation's feature importance distribution.
        Referenced from: https://arxiv.org/abs/2005.00631
        """

        if not kwargs.get("bypass_check", False) and not isinstance(explainer, ExplainerWrapper):
            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)

        explanation = explainer.explain_instance(instance_data_row)

        def frac_contribution(explanation: pd.DataFrame, i: int) -> float:
            abs_score_sum = explanation['score'].abs().sum()
            return explanation['score'].abs()[i] / abs_score_sum

        sum = 0
        for i in range(explanation.shape[0]):
            fc = frac_contribution(explanation, i)
            sum += fc * np.log(fc) if fc > 0 else 0
            
        return -sum