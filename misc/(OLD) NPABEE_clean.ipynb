{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensemble of Local Model-Agnostic Explanation Models for Robust Local Feature Importance Ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 15:40:26.717971: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-20 15:40:26.726143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732128026.735861   22784 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732128026.738935   22784 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 15:40:26.749252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Explainable AI tools:\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from alibi.explainers import AnchorTabular # why not used the original anchor package?\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam  # Import the Adam optimizer\n",
    "\n",
    "from tools.topsis import Topsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Job</th>\n",
       "      <th>Housing</th>\n",
       "      <th>Saving accounts</th>\n",
       "      <th>Checking account</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Credit Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>NaN</td>\n",
       "      <td>little</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>moderate</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>radio/TV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>own</td>\n",
       "      <td>little</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>education</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>furniture/equipment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>free</td>\n",
       "      <td>little</td>\n",
       "      <td>little</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>car</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
       "0           0   67    male    2     own             NaN           little   \n",
       "1           1   22  female    2     own          little         moderate   \n",
       "2           2   49    male    1     own          little              NaN   \n",
       "3           3   45    male    2    free          little           little   \n",
       "4           4   53    male    2    free          little           little   \n",
       "\n",
       "   Credit amount  Duration              Purpose  Credit Risk  \n",
       "0           1169         6             radio/TV            1  \n",
       "1           5951        48             radio/TV            2  \n",
       "2           2096        12            education            1  \n",
       "3           7882        42  furniture/equipment            1  \n",
       "4           4870        24                  car            2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Job</th>\n",
       "      <th>Credit amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Credit Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>476.500000</td>\n",
       "      <td>35.501048</td>\n",
       "      <td>1.909853</td>\n",
       "      <td>3279.112159</td>\n",
       "      <td>20.780922</td>\n",
       "      <td>1.302935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>275.540378</td>\n",
       "      <td>11.379668</td>\n",
       "      <td>0.649681</td>\n",
       "      <td>2853.315158</td>\n",
       "      <td>12.046483</td>\n",
       "      <td>0.459768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>238.250000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1360.250000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>476.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2302.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>714.750000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3975.250000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>953.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>18424.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         Age         Job  Credit amount    Duration  \\\n",
       "count  954.000000  954.000000  954.000000     954.000000  954.000000   \n",
       "mean   476.500000   35.501048    1.909853    3279.112159   20.780922   \n",
       "std    275.540378   11.379668    0.649681    2853.315158   12.046483   \n",
       "min      0.000000   19.000000    0.000000     250.000000    4.000000   \n",
       "25%    238.250000   27.000000    2.000000    1360.250000   12.000000   \n",
       "50%    476.500000   33.000000    2.000000    2302.500000   18.000000   \n",
       "75%    714.750000   42.000000    2.000000    3975.250000   24.000000   \n",
       "max    953.000000   75.000000    3.000000   18424.000000   72.000000   \n",
       "\n",
       "       Credit Risk  \n",
       "count   954.000000  \n",
       "mean      1.302935  \n",
       "std       0.459768  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       2.000000  \n",
       "max       2.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        954 non-null    int64 \n",
      " 1   Age               954 non-null    int64 \n",
      " 2   Sex               954 non-null    object\n",
      " 3   Job               954 non-null    int64 \n",
      " 4   Housing           954 non-null    object\n",
      " 5   Saving accounts   779 non-null    object\n",
      " 6   Checking account  576 non-null    object\n",
      " 7   Credit amount     954 non-null    int64 \n",
      " 8   Duration          954 non-null    int64 \n",
      " 9   Purpose           954 non-null    object\n",
      " 10  Credit Risk       954 non-null    int64 \n",
      "dtypes: int64(6), object(5)\n",
      "memory usage: 82.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of the categorical features:\n",
      "\t- Sex: ['male' 'female']\n",
      "\t- Housing: ['own' 'free' 'rent']\n",
      "\t- Saving accounts: [nan 'little' 'quite rich' 'rich' 'moderate']\n",
      "\t- Checking account: ['little' 'moderate' nan 'rich']\n",
      "\t- Purpose: ['radio/TV' 'education' 'furniture/equipment' 'car' 'business'\n",
      " 'domestic appliances' 'repairs' 'vacation/others']\n"
     ]
    }
   ],
   "source": [
    "original_data = pd.read_csv('german_credit_data_updated.csv')\n",
    "\n",
    "# Dataset overview - German Credit Risk (from Kaggle):\n",
    "# 1. Age (numeric)\n",
    "# 2. Sex (text: male, female)\n",
    "# 3. Job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\n",
    "# 4. Housing (text: own, rent, or free)\n",
    "# 5. Saving accounts (text - little, moderate, quite rich, rich)\n",
    "# 6. Checking account (numeric, in DM - Deutsch Mark)\n",
    "# 7. Credit amount (numeric, in DM)\n",
    "# 8. Duration (numeric, in month)\n",
    "# 9. Purpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)\n",
    "\n",
    "display(original_data.head())\n",
    "display(original_data.describe())\n",
    "display(original_data.info())\n",
    "\n",
    "# Display the unique values of thprecision=3, e categorical features:\n",
    "print('Unique values of the categorical features:')\n",
    "for col in original_data.select_dtypes(include='object'):\n",
    "    print(f'\\t- {col}: {original_data[col].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: Index(['Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account',\n",
      "       'Purpose'],\n",
      "      dtype='object')\n",
      "Numerical features: Index(['Age', 'Credit amount', 'Duration'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Credit_amount</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Credit_Risk</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Job_highlyskilled</th>\n",
       "      <th>Job_skilled</th>\n",
       "      <th>Job_unskilled_nonresident</th>\n",
       "      <th>Job_unskilled_resident</th>\n",
       "      <th>Housing_free</th>\n",
       "      <th>Housing_own</th>\n",
       "      <th>Housing_rent</th>\n",
       "      <th>Saving_accounts_little</th>\n",
       "      <th>Saving_accounts_moderate</th>\n",
       "      <th>Saving_accounts_none</th>\n",
       "      <th>Saving_accounts_quite_rich</th>\n",
       "      <th>Saving_accounts_rich</th>\n",
       "      <th>Checking_account_little</th>\n",
       "      <th>Checking_account_moderate</th>\n",
       "      <th>Checking_account_none</th>\n",
       "      <th>Checking_account_rich</th>\n",
       "      <th>Purpose_business</th>\n",
       "      <th>Purpose_car</th>\n",
       "      <th>Purpose_domestic_appliances</th>\n",
       "      <th>Purpose_education</th>\n",
       "      <th>Purpose_furniture_equipment</th>\n",
       "      <th>Purpose_radio_TV</th>\n",
       "      <th>Purpose_repairs</th>\n",
       "      <th>Purpose_vacation_others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>1169</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>5951</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>2096</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>7882</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>4870</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Credit_amount  Duration  Credit_Risk  Sex_female  Sex_male  \\\n",
       "0   67           1169         6            0           0         1   \n",
       "1   22           5951        48            1           1         0   \n",
       "2   49           2096        12            0           0         1   \n",
       "3   45           7882        42            0           0         1   \n",
       "4   53           4870        24            1           0         1   \n",
       "\n",
       "   Job_highlyskilled  Job_skilled  Job_unskilled_nonresident  \\\n",
       "0                  0            1                          0   \n",
       "1                  0            1                          0   \n",
       "2                  0            0                          0   \n",
       "3                  0            1                          0   \n",
       "4                  0            1                          0   \n",
       "\n",
       "   Job_unskilled_resident  Housing_free  Housing_own  Housing_rent  \\\n",
       "0                       0             0            1             0   \n",
       "1                       0             0            1             0   \n",
       "2                       1             0            1             0   \n",
       "3                       0             1            0             0   \n",
       "4                       0             1            0             0   \n",
       "\n",
       "   Saving_accounts_little  Saving_accounts_moderate  Saving_accounts_none  \\\n",
       "0                       0                         0                     1   \n",
       "1                       1                         0                     0   \n",
       "2                       1                         0                     0   \n",
       "3                       1                         0                     0   \n",
       "4                       1                         0                     0   \n",
       "\n",
       "   Saving_accounts_quite_rich  Saving_accounts_rich  Checking_account_little  \\\n",
       "0                           0                     0                        1   \n",
       "1                           0                     0                        0   \n",
       "2                           0                     0                        0   \n",
       "3                           0                     0                        1   \n",
       "4                           0                     0                        1   \n",
       "\n",
       "   Checking_account_moderate  Checking_account_none  Checking_account_rich  \\\n",
       "0                          0                      0                      0   \n",
       "1                          1                      0                      0   \n",
       "2                          0                      1                      0   \n",
       "3                          0                      0                      0   \n",
       "4                          0                      0                      0   \n",
       "\n",
       "   Purpose_business  Purpose_car  Purpose_domestic_appliances  \\\n",
       "0                 0            0                            0   \n",
       "1                 0            0                            0   \n",
       "2                 0            0                            0   \n",
       "3                 0            0                            0   \n",
       "4                 0            1                            0   \n",
       "\n",
       "   Purpose_education  Purpose_furniture_equipment  Purpose_radio_TV  \\\n",
       "0                  0                            0                 1   \n",
       "1                  0                            0                 1   \n",
       "2                  1                            0                 0   \n",
       "3                  0                            1                 0   \n",
       "4                  0                            0                 0   \n",
       "\n",
       "   Purpose_repairs  Purpose_vacation_others  \n",
       "0                0                        0  \n",
       "1                0                        0  \n",
       "2                0                        0  \n",
       "3                0                        0  \n",
       "4                0                        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 30 columns):\n",
      " #   Column                       Non-Null Count  Dtype\n",
      "---  ------                       --------------  -----\n",
      " 0   Age                          954 non-null    int64\n",
      " 1   Credit_amount                954 non-null    int64\n",
      " 2   Duration                     954 non-null    int64\n",
      " 3   Credit_Risk                  954 non-null    int64\n",
      " 4   Sex_female                   954 non-null    int64\n",
      " 5   Sex_male                     954 non-null    int64\n",
      " 6   Job_highlyskilled            954 non-null    int64\n",
      " 7   Job_skilled                  954 non-null    int64\n",
      " 8   Job_unskilled_nonresident    954 non-null    int64\n",
      " 9   Job_unskilled_resident       954 non-null    int64\n",
      " 10  Housing_free                 954 non-null    int64\n",
      " 11  Housing_own                  954 non-null    int64\n",
      " 12  Housing_rent                 954 non-null    int64\n",
      " 13  Saving_accounts_little       954 non-null    int64\n",
      " 14  Saving_accounts_moderate     954 non-null    int64\n",
      " 15  Saving_accounts_none         954 non-null    int64\n",
      " 16  Saving_accounts_quite_rich   954 non-null    int64\n",
      " 17  Saving_accounts_rich         954 non-null    int64\n",
      " 18  Checking_account_little      954 non-null    int64\n",
      " 19  Checking_account_moderate    954 non-null    int64\n",
      " 20  Checking_account_none        954 non-null    int64\n",
      " 21  Checking_account_rich        954 non-null    int64\n",
      " 22  Purpose_business             954 non-null    int64\n",
      " 23  Purpose_car                  954 non-null    int64\n",
      " 24  Purpose_domestic_appliances  954 non-null    int64\n",
      " 25  Purpose_education            954 non-null    int64\n",
      " 26  Purpose_furniture_equipment  954 non-null    int64\n",
      " 27  Purpose_radio_TV             954 non-null    int64\n",
      " 28  Purpose_repairs              954 non-null    int64\n",
      " 29  Purpose_vacation_others      954 non-null    int64\n",
      "dtypes: int64(30)\n",
      "memory usage: 223.7 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.7694545 , -0.7399179 , -1.22763429, ...,  1.62518349,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-1.18704073,  0.93690642,  2.26068929, ...,  1.62518349,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [ 1.18685641, -0.41486224, -0.72930235, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       ...,\n",
       "       [-1.0111965 , -0.39768023,  1.26402541, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-0.65950803,  0.29240557,  0.26736153, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653],\n",
       "       [-0.83535227,  2.69823821,  1.26402541, ..., -0.61531514,\n",
       "        -0.14633276, -0.11286653]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_data = original_data.copy()\n",
    "\n",
    "# For savings and checking accounts, we will replace the missing values with 'none':\n",
    "preprocessed_data['Saving accounts'].fillna('none', inplace=True)\n",
    "preprocessed_data['Checking account'].fillna('none', inplace=True)\n",
    "\n",
    "# Dropping index column:\n",
    "preprocessed_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Using pd.dummies to one-hot-encode the categorical features\n",
    "preprocessed_data[\"Job\"] = preprocessed_data[\"Job\"].map({0: 'unskilled_nonresident', 1: 'unskilled_resident',\n",
    "                                                         2: 'skilled', 3: 'highlyskilled'})\n",
    "\n",
    "categorical_features = preprocessed_data.select_dtypes(include='object').columns\n",
    "numerical_features = preprocessed_data.select_dtypes(include='number').columns.drop('Credit Risk')\n",
    "print(f'Categorical features: {categorical_features}')\n",
    "print(f'Numerical features: {numerical_features}')\n",
    "\n",
    "preprocessed_data = pd.get_dummies(preprocessed_data, columns=categorical_features, dtype='int64')\n",
    "\n",
    "# Remapping the target variable to 0 and 1:\n",
    "preprocessed_data['Credit Risk'] = preprocessed_data['Credit Risk'].map({1: 0, 2: 1})\n",
    "\n",
    "# Make sure all column names are valid python identifiers (important for pd.query() calls):\n",
    "preprocessed_data.columns = preprocessed_data.columns.str.replace(' ', '_')\n",
    "preprocessed_data.columns = preprocessed_data.columns.str.replace('/', '_')\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "scaled_preprocessed_data = scaler.fit_transform(preprocessed_data)\n",
    "\n",
    "display(preprocessed_data.head())\n",
    "display(preprocessed_data.info())\n",
    "\n",
    "display(scaled_preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = preprocessed_data['Credit_Risk']\n",
    "X = preprocessed_data.drop(columns='Credit_Risk')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7696335078534031\n",
      "ROC AUC: 0.6830357142857143\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'ROC AUC: {roc_auc_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implementation\n",
    "Given a dataset, an ML model trained on this dataset (whose predictions we want to explain), and a set of explanation tools from which the aggregate explanation model will be built:\n",
    "\n",
    "1. Apply each explanation model to the instance whose prediction is to be explained, obtaining a feature importance ranking for each explanation model.\n",
    "2. Generate a variation of the original dataset by introducing noise.\n",
    "3. Repeat step 1 on this noisy dataset to obtain a new set of feature importance rankings.\n",
    "4. Assess the robustness of each explanation model by comparing its feature importance ranking on the original data with the ranking on the noisy data.\n",
    "5. Finally, compute an aggregate explanation ranking by taking the weighted average of the rankings obtained in step 1, with the weights based on each model's stability (determined in step 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Apply each explanation model to the instance whose prediction is to be explained, obtaining a feature importance ranking for each explanation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImportanceCalculator():\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, predict_proba_function: callable = None):\n",
    "        self.X_train = X_train\n",
    "        self.clf = clf\n",
    "        \n",
    "        if predict_proba_function is not None:\n",
    "            self.predict_proba_function = predict_proba_function\n",
    "        elif hasattr(clf, 'predict_proba') and predict_proba_function is None:\n",
    "            self.predict_proba_function = clf.predict_proba\n",
    "        else:\n",
    "            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')\n",
    "        \n",
    "        self.anchor_explainer = AnchorTabular(predictor=self.predict_proba_function, feature_names=self.X_train.columns) # TODO: fix parameters\n",
    "        self.anchor_explainer.fit(self.X_train.values)\n",
    "\n",
    "\n",
    "    def get_lime_ranking(self, instance_data_row) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame with the feature importance ranking using LIME, ordered by abs(importance).\n",
    "        \"\"\"\n",
    "        \n",
    "        explainer = LimeTabularExplainer(self.X_train.values, feature_names=self.X_train.columns, discretize_continuous=False)\n",
    "        lime_exp = explainer.explain_instance(instance_data_row, self.predict_proba_function, num_features=len(self.X_train.columns))\n",
    "        \n",
    "        ranking = pd.DataFrame(lime_exp.as_list(), columns=['feature', 'score'])\n",
    "\n",
    "        return ranking\n",
    "    \n",
    "    def get_shap_ranking(self, instance_data_row, explainer_type: shap.Explainer = shap.KernelExplainer, **additional_explainer_args) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame with the feature importance ranking using SHAP, ordered by abs(importance).\n",
    "        \"\"\"\n",
    "        explainer = explainer_type(self.clf, self.X_train, **additional_explainer_args)\n",
    "        shap_values = explainer.shap_values(instance_data_row)\n",
    "\n",
    "        ranking = pd.DataFrame(list(zip(self.X_train.columns, shap_values[:, 0])), columns=['feature', 'score'])\n",
    "        ranking = ranking.sort_values(by='score', ascending=False, key=lambda x: abs(x)).reset_index(drop=True)\n",
    "        \n",
    "        return ranking\n",
    "    \n",
    "    def get_anchor_ranking(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame with the feature importance ranking using Anchor, ordered by abs(importance).\n",
    "        Feature importance is not directly available in the AnchorTabular class. In order to obtain it, we can\n",
    "        calculate the percentage of rows in the training data that are not covered by the anchor rule. The more\n",
    "        rows that are not covered, the more important the feature is.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(instance_data_row, pd.Series):\n",
    "            instance_data_row = instance_data_row.to_numpy()\n",
    "\n",
    "        feature_importances = {feature: 0 for feature in self.X_train.columns}\n",
    "        explanation = self.anchor_explainer.explain(instance_data_row)\n",
    "        \n",
    "        for rule in explanation.anchor:\n",
    "            # Extract the feature name from the rule string\n",
    "            # This method won't work for column names that have spaces in them or that don't contain any letters\n",
    "            for expression_element in rule.split():\n",
    "                if any(c.isalpha() for c in expression_element):\n",
    "                    referenced_feature = expression_element\n",
    "                    break\n",
    "\n",
    "            rule_coverage = X_train.query(rule).shape[0] / X_train.shape[0]\n",
    "            feature_importances[referenced_feature] = 1 - rule_coverage\n",
    "        \n",
    "        return pd.DataFrame(list(feature_importances.items()), columns=['feature', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME ranking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Checking_account_none</td>\n",
       "      <td>-0.058835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Duration</td>\n",
       "      <td>0.053430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Checking_account_little</td>\n",
       "      <td>0.036393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Age</td>\n",
       "      <td>-0.024741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Checking_account_moderate</td>\n",
       "      <td>0.015632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Housing_own</td>\n",
       "      <td>-0.012648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Saving_accounts_little</td>\n",
       "      <td>0.010625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>-0.010128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Purpose_radio_TV</td>\n",
       "      <td>-0.008845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Credit_amount</td>\n",
       "      <td>0.007636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sex_female</td>\n",
       "      <td>0.007120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Housing_rent</td>\n",
       "      <td>0.006117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Saving_accounts_none</td>\n",
       "      <td>-0.005523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Housing_free</td>\n",
       "      <td>0.004319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Purpose_furniture_equipment</td>\n",
       "      <td>-0.004029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Purpose_car</td>\n",
       "      <td>0.003633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Purpose_business</td>\n",
       "      <td>0.003231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Purpose_vacation_others</td>\n",
       "      <td>0.003171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Saving_accounts_moderate</td>\n",
       "      <td>0.003137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Checking_account_rich</td>\n",
       "      <td>-0.002914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Job_highlyskilled</td>\n",
       "      <td>-0.002131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Purpose_education</td>\n",
       "      <td>0.001994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Purpose_domestic_appliances</td>\n",
       "      <td>-0.001688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Job_skilled</td>\n",
       "      <td>0.001023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Purpose_repairs</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Saving_accounts_rich</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Saving_accounts_quite_rich</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Job_unskilled_resident</td>\n",
       "      <td>-0.000328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Job_unskilled_nonresident</td>\n",
       "      <td>-0.000162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature     score\n",
       "0         Checking_account_none -0.058835\n",
       "1                      Duration  0.053430\n",
       "2       Checking_account_little  0.036393\n",
       "3                           Age -0.024741\n",
       "4     Checking_account_moderate  0.015632\n",
       "5                   Housing_own -0.012648\n",
       "6        Saving_accounts_little  0.010625\n",
       "7                      Sex_male -0.010128\n",
       "8              Purpose_radio_TV -0.008845\n",
       "9                 Credit_amount  0.007636\n",
       "10                   Sex_female  0.007120\n",
       "11                 Housing_rent  0.006117\n",
       "12         Saving_accounts_none -0.005523\n",
       "13                 Housing_free  0.004319\n",
       "14  Purpose_furniture_equipment -0.004029\n",
       "15                  Purpose_car  0.003633\n",
       "16             Purpose_business  0.003231\n",
       "17      Purpose_vacation_others  0.003171\n",
       "18     Saving_accounts_moderate  0.003137\n",
       "19        Checking_account_rich -0.002914\n",
       "20            Job_highlyskilled -0.002131\n",
       "21            Purpose_education  0.001994\n",
       "22  Purpose_domestic_appliances -0.001688\n",
       "23                  Job_skilled  0.001023\n",
       "24              Purpose_repairs  0.000930\n",
       "25         Saving_accounts_rich  0.000476\n",
       "26   Saving_accounts_quite_rich  0.000439\n",
       "27       Job_unskilled_resident -0.000328\n",
       "28    Job_unskilled_nonresident -0.000162"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP ranking:\n",
      "Anchor ranking:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Purpose_furniture_equipment</td>\n",
       "      <td>0.813893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Duration</td>\n",
       "      <td>0.804718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.503277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Checking_account_little</td>\n",
       "      <td>0.283093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saving_accounts_quite_rich</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Purpose_repairs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Purpose_radio_TV</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Purpose_education</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Purpose_domestic_appliances</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Purpose_car</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Purpose_business</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Checking_account_rich</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Checking_account_none</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Checking_account_moderate</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Saving_accounts_rich</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Saving_accounts_none</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Credit_amount</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Saving_accounts_moderate</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Saving_accounts_little</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Housing_rent</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Housing_own</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Housing_free</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Job_unskilled_resident</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Job_unskilled_nonresident</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Job_skilled</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Job_highlyskilled</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sex_female</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Purpose_vacation_others</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature     score\n",
       "0   Purpose_furniture_equipment  0.813893\n",
       "1                      Duration  0.804718\n",
       "2                           Age  0.503277\n",
       "3       Checking_account_little  0.283093\n",
       "4    Saving_accounts_quite_rich  0.000000\n",
       "5               Purpose_repairs  0.000000\n",
       "6              Purpose_radio_TV  0.000000\n",
       "7             Purpose_education  0.000000\n",
       "8   Purpose_domestic_appliances  0.000000\n",
       "9                   Purpose_car  0.000000\n",
       "10             Purpose_business  0.000000\n",
       "11        Checking_account_rich  0.000000\n",
       "12        Checking_account_none  0.000000\n",
       "13    Checking_account_moderate  0.000000\n",
       "14         Saving_accounts_rich  0.000000\n",
       "15         Saving_accounts_none  0.000000\n",
       "16                Credit_amount  0.000000\n",
       "17     Saving_accounts_moderate  0.000000\n",
       "18       Saving_accounts_little  0.000000\n",
       "19                 Housing_rent  0.000000\n",
       "20                  Housing_own  0.000000\n",
       "21                 Housing_free  0.000000\n",
       "22       Job_unskilled_resident  0.000000\n",
       "23    Job_unskilled_nonresident  0.000000\n",
       "24                  Job_skilled  0.000000\n",
       "25            Job_highlyskilled  0.000000\n",
       "26                     Sex_male  0.000000\n",
       "27                   Sex_female  0.000000\n",
       "28      Purpose_vacation_others  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "feature_importances = FeatureImportanceCalculator(clf, X_train)\n",
    "    \n",
    "sample_idx = 0\n",
    "lime_ranking = feature_importances.get_lime_ranking(X_test.iloc[sample_idx])\n",
    "shap_ranking = feature_importances.get_shap_ranking(X_test.iloc[sample_idx], explainer_type=shap.TreeExplainer)\n",
    "anchor_ranking = feature_importances.get_anchor_ranking(X_test.iloc[sample_idx])\n",
    "\n",
    "print('LIME ranking:')\n",
    "display(lime_ranking)\n",
    "print('SHAP ranking:')\n",
    "# display(shap_ranking)\n",
    "print('Anchor ranking:')\n",
    "display(anchor_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Generate a variation of the original dataset by introducing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_data_autoencoder(X: pd.DataFrame, categorical_features_names: list[str], encoding_dim: int = 5, num_features_to_replace: int = 2, epochs=500) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing a noisy variation of the data.\n",
    "\n",
    "    The noise is generated by swapping the values of a small number of features between a sample and a random close neighbor.\n",
    "    To determine the neighbors, we use an autoencoder to reduce the dimensionality of the data and then calculate the use the NearestNeightbors algorithm in the reduced space.\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    input_dim = X_scaled.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "    autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "    autoencoder.fit(X_scaled, X_scaled, epochs=epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "    # Extract hidden layer representation:\n",
    "    hidden_representation = encoder.predict(X_scaled)\n",
    "\n",
    "    # Compute Nearest Neighbors using hidden_representation\n",
    "    nbrs = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(hidden_representation) # TODO: here, hidden_representation is just the autoencoder fit to the scaled X data; see if this is the way to do this\n",
    "    distances, indices = nbrs.kneighbors(hidden_representation)\n",
    "\n",
    "    X_noisy = X.copy()\n",
    "\n",
    "    # Get id's of columns that belong to the same categorical feature (after being one-hot-encodeded);\n",
    "    # Columns that belong to the same categorical feature start with the same name, and will be treated as a single feature when adding noise.\n",
    "    categorical_features_indices = [\n",
    "        [X.columns.get_loc(col_name) for col_name in X.columns if col_name.startswith(feature)]\n",
    "        for feature in categorical_features_names\n",
    "    ]\n",
    "\n",
    "    # Replace features with random neighbor's features\n",
    "    for i in range(X.shape[0]):  # Iterate over each sample\n",
    "        available_features_to_replace = list(range(X.shape[1]))\n",
    "        for j in range(num_features_to_replace):\n",
    "            # Select features to replace; if the feture selected belong to one of the lists in categorical_features_indices, we will replace all the features in that list\n",
    "            features_to_replace = np.random.choice(available_features_to_replace, 1)\n",
    "            for feature_indices in categorical_features_indices:\n",
    "                if features_to_replace in feature_indices:\n",
    "                    features_to_replace = feature_indices\n",
    "                    break\n",
    "            \n",
    "            # Remove the selected features from the list of available features to replace\n",
    "            available_features_to_replace = [f for f in available_features_to_replace if f not in features_to_replace]\n",
    "\n",
    "            # Choose a random neighbor from the nearest neighbors\n",
    "            neighbor_idx = np.random.choice(indices[i][1:])\n",
    "\n",
    "            # Replace the selected features with the neighbor's features\n",
    "            X_noisy.iloc[i, features_to_replace] = X.iloc[neighbor_idx, features_to_replace]\n",
    "\n",
    "    return X_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954, 29)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732128028.908517   22784 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2707 - val_loss: 1.2044\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2288 - val_loss: 1.1834\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2396 - val_loss: 1.1636\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2376 - val_loss: 1.1439\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2091 - val_loss: 1.1237\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1681 - val_loss: 1.1032\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1567 - val_loss: 1.0828\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 1.1498 - val_loss: 1.0629\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0952 - val_loss: 1.0437\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0792 - val_loss: 1.0257\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mean Absolute Difference: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.094954095279404"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of usage:\n",
    "display(X.shape)\n",
    "noisy_data = get_noisy_data_autoencoder(X, categorical_features_names=categorical_features, encoding_dim=5, num_features_to_replace=2, epochs=10)\n",
    "\n",
    "# Split the noisy data the same way as the original data\n",
    "# X_train_noisy, X_test_noisy = train_test_split(noisy_data, test_size=0.2, random_state=41) # Split it with the same random_state as the original data\n",
    "X_train_noisy = noisy_data.loc[X_train.index]\n",
    "X_test_noisy = noisy_data.loc[X_test.index]\n",
    "display(\"Mean Absolute Difference: \", np.mean(np.abs(X - noisy_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Repeat step 1 on this noisy dataset to obtain a new set of feature importance rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Checking_account_none</td>\n",
       "      <td>-0.057467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Duration</td>\n",
       "      <td>0.057258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Checking_account_little</td>\n",
       "      <td>0.035452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Age</td>\n",
       "      <td>-0.027223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Checking_account_moderate</td>\n",
       "      <td>0.019969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Housing_own</td>\n",
       "      <td>-0.013258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Purpose_radio_TV</td>\n",
       "      <td>-0.010295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Saving_accounts_little</td>\n",
       "      <td>0.008335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Housing_free</td>\n",
       "      <td>0.007493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>-0.006986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Purpose_car</td>\n",
       "      <td>0.006425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Purpose_education</td>\n",
       "      <td>0.005970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Saving_accounts_none</td>\n",
       "      <td>-0.005365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Credit_amount</td>\n",
       "      <td>0.004824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sex_female</td>\n",
       "      <td>0.004746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Housing_rent</td>\n",
       "      <td>0.004709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Saving_accounts_moderate</td>\n",
       "      <td>0.003547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Purpose_business</td>\n",
       "      <td>0.002966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Job_unskilled_resident</td>\n",
       "      <td>0.002762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Purpose_repairs</td>\n",
       "      <td>-0.002486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Job_skilled</td>\n",
       "      <td>0.002335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Saving_accounts_quite_rich</td>\n",
       "      <td>-0.001835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Job_highlyskilled</td>\n",
       "      <td>-0.001480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Purpose_vacation_others</td>\n",
       "      <td>0.001116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Job_unskilled_nonresident</td>\n",
       "      <td>0.001046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Saving_accounts_rich</td>\n",
       "      <td>-0.000891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Purpose_furniture_equipment</td>\n",
       "      <td>-0.000851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Checking_account_rich</td>\n",
       "      <td>-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Purpose_domestic_appliances</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature     score\n",
       "0         Checking_account_none -0.057467\n",
       "1                      Duration  0.057258\n",
       "2       Checking_account_little  0.035452\n",
       "3                           Age -0.027223\n",
       "4     Checking_account_moderate  0.019969\n",
       "5                   Housing_own -0.013258\n",
       "6              Purpose_radio_TV -0.010295\n",
       "7        Saving_accounts_little  0.008335\n",
       "8                  Housing_free  0.007493\n",
       "9                      Sex_male -0.006986\n",
       "10                  Purpose_car  0.006425\n",
       "11            Purpose_education  0.005970\n",
       "12         Saving_accounts_none -0.005365\n",
       "13                Credit_amount  0.004824\n",
       "14                   Sex_female  0.004746\n",
       "15                 Housing_rent  0.004709\n",
       "16     Saving_accounts_moderate  0.003547\n",
       "17             Purpose_business  0.002966\n",
       "18       Job_unskilled_resident  0.002762\n",
       "19              Purpose_repairs -0.002486\n",
       "20                  Job_skilled  0.002335\n",
       "21   Saving_accounts_quite_rich -0.001835\n",
       "22            Job_highlyskilled -0.001480\n",
       "23      Purpose_vacation_others  0.001116\n",
       "24    Job_unskilled_nonresident  0.001046\n",
       "25         Saving_accounts_rich -0.000891\n",
       "26  Purpose_furniture_equipment -0.000851\n",
       "27        Checking_account_rich -0.000300\n",
       "28  Purpose_domestic_appliances  0.000299"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Checking_account_none</td>\n",
       "      <td>-0.050534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.039863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Duration</td>\n",
       "      <td>0.037818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Checking_account_little</td>\n",
       "      <td>0.028099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Checking_account_moderate</td>\n",
       "      <td>-0.027543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Credit_amount</td>\n",
       "      <td>0.019034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Purpose_car</td>\n",
       "      <td>-0.013071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Saving_accounts_moderate</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>0.008038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Saving_accounts_little</td>\n",
       "      <td>-0.006773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Saving_accounts_none</td>\n",
       "      <td>-0.006566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Job_highlyskilled</td>\n",
       "      <td>0.005123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Housing_free</td>\n",
       "      <td>0.004959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sex_female</td>\n",
       "      <td>0.004557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Purpose_radio_TV</td>\n",
       "      <td>-0.004111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Purpose_business</td>\n",
       "      <td>-0.002618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Housing_own</td>\n",
       "      <td>-0.002597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Saving_accounts_quite_rich</td>\n",
       "      <td>-0.002426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Housing_rent</td>\n",
       "      <td>-0.002390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Job_unskilled_resident</td>\n",
       "      <td>-0.002198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Purpose_furniture_equipment</td>\n",
       "      <td>-0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Purpose_repairs</td>\n",
       "      <td>0.001081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Job_skilled</td>\n",
       "      <td>-0.000928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Purpose_vacation_others</td>\n",
       "      <td>0.000707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Checking_account_rich</td>\n",
       "      <td>-0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Job_unskilled_nonresident</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Saving_accounts_rich</td>\n",
       "      <td>-0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Purpose_education</td>\n",
       "      <td>0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Purpose_domestic_appliances</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature     score\n",
       "0         Checking_account_none -0.050534\n",
       "1                           Age  0.039863\n",
       "2                      Duration  0.037818\n",
       "3       Checking_account_little  0.028099\n",
       "4     Checking_account_moderate -0.027543\n",
       "5                 Credit_amount  0.019034\n",
       "6                   Purpose_car -0.013071\n",
       "7      Saving_accounts_moderate  0.010728\n",
       "8                      Sex_male  0.008038\n",
       "9        Saving_accounts_little -0.006773\n",
       "10         Saving_accounts_none -0.006566\n",
       "11            Job_highlyskilled  0.005123\n",
       "12                 Housing_free  0.004959\n",
       "13                   Sex_female  0.004557\n",
       "14             Purpose_radio_TV -0.004111\n",
       "15             Purpose_business -0.002618\n",
       "16                  Housing_own -0.002597\n",
       "17   Saving_accounts_quite_rich -0.002426\n",
       "18                 Housing_rent -0.002390\n",
       "19       Job_unskilled_resident -0.002198\n",
       "20  Purpose_furniture_equipment -0.001650\n",
       "21              Purpose_repairs  0.001081\n",
       "22                  Job_skilled -0.000928\n",
       "23      Purpose_vacation_others  0.000707\n",
       "24        Checking_account_rich -0.000705\n",
       "25    Job_unskilled_nonresident  0.000210\n",
       "26         Saving_accounts_rich -0.000168\n",
       "27            Purpose_education  0.000160\n",
       "28  Purpose_domestic_appliances  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Duration</td>\n",
       "      <td>0.433814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex_female</td>\n",
       "      <td>0.313237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Housing_own</td>\n",
       "      <td>0.296199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Checking_account_little</td>\n",
       "      <td>0.283093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saving_accounts_rich</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Purpose_repairs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Purpose_radio_TV</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Purpose_furniture_equipment</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Purpose_education</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Purpose_domestic_appliances</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Purpose_car</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Purpose_business</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Checking_account_rich</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Checking_account_none</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Checking_account_moderate</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Saving_accounts_none</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Saving_accounts_quite_rich</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Credit_amount</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Saving_accounts_moderate</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Saving_accounts_little</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Housing_rent</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Housing_free</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Job_unskilled_resident</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Job_unskilled_nonresident</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Job_skilled</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Job_highlyskilled</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Purpose_vacation_others</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature     score\n",
       "0                      Duration  0.433814\n",
       "1                    Sex_female  0.313237\n",
       "2                   Housing_own  0.296199\n",
       "3       Checking_account_little  0.283093\n",
       "4                           Age  0.000000\n",
       "5          Saving_accounts_rich  0.000000\n",
       "6               Purpose_repairs  0.000000\n",
       "7              Purpose_radio_TV  0.000000\n",
       "8   Purpose_furniture_equipment  0.000000\n",
       "9             Purpose_education  0.000000\n",
       "10  Purpose_domestic_appliances  0.000000\n",
       "11                  Purpose_car  0.000000\n",
       "12             Purpose_business  0.000000\n",
       "13        Checking_account_rich  0.000000\n",
       "14        Checking_account_none  0.000000\n",
       "15    Checking_account_moderate  0.000000\n",
       "16         Saving_accounts_none  0.000000\n",
       "17   Saving_accounts_quite_rich  0.000000\n",
       "18                Credit_amount  0.000000\n",
       "19     Saving_accounts_moderate  0.000000\n",
       "20       Saving_accounts_little  0.000000\n",
       "21                 Housing_rent  0.000000\n",
       "22                 Housing_free  0.000000\n",
       "23       Job_unskilled_resident  0.000000\n",
       "24    Job_unskilled_nonresident  0.000000\n",
       "25                  Job_skilled  0.000000\n",
       "26            Job_highlyskilled  0.000000\n",
       "27                     Sex_male  0.000000\n",
       "28      Purpose_vacation_others  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Given a single instance, get rankings for original and noisy data, then compare them (next steps).\n",
    "feature_importances_noisy = FeatureImportanceCalculator(clf, X_train_noisy)\n",
    "\n",
    "lime_ranking_noisy = feature_importances_noisy.get_lime_ranking(X_test_noisy.iloc[sample_idx])\n",
    "shap_ranking_noisy = feature_importances_noisy.get_shap_ranking(X_test_noisy.iloc[sample_idx], explainer_type=shap.TreeExplainer)\n",
    "anchor_ranking_noisy = feature_importances_noisy.get_anchor_ranking(X_test_noisy.iloc[sample_idx])\n",
    "\n",
    "display(lime_ranking_noisy, shap_ranking_noisy, anchor_ranking_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Assess the robustness of each explanation model by comparing its feature importance ranking on the original data with the ranking on the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_metrics(ranking_original: pd.DataFrame, ranking_noisy: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with 4 robustness metrics of a given feature importance ranking:\n",
    "        1. \"mean_squared_differece\": the mean squared difference between the scores of the original and noisy rankings; (previously called \"stability\") | Lower is better\n",
    "        2. \"mean_absolute_difference\": the mean absolute difference between the scores of the original and noisy rankings; (previously called \"sensitivity\") | Lower is better\n",
    "        3. \"spearman_correlation\": the Spearman correlation | Higher is better\n",
    "        4. \"pearson_correlation\": the Pearson correlation | Higher is better\n",
    "    \"\"\"\n",
    "    \n",
    "    # Align dataframes:\n",
    "    ranking_original = ranking_original.set_index('feature')\n",
    "    ranking_noisy = ranking_noisy.set_index('feature')\n",
    "    ranking_original = ranking_original.reindex(ranking_noisy.index)\n",
    "\n",
    "    # Compute metrics:\n",
    "    mean_squared_difference = ((ranking_original['score'] - ranking_noisy['score']) ** 2).mean()\n",
    "    mean_absolute_difference = np.abs(ranking_original['score'] - ranking_noisy['score']).mean()\n",
    "    spearman_correlation = spearmanr(ranking_original['score'], ranking_noisy['score']).correlation\n",
    "    pearson_correlation = pearsonr(ranking_original['score'], ranking_noisy['score'])[0]\n",
    "\n",
    "    robustness_metrics = pd.DataFrame({\n",
    "        'mean_squared_difference': [mean_squared_difference],\n",
    "        'mean_absolute_difference': [mean_absolute_difference],\n",
    "        'spearman_correlation': [spearman_correlation],\n",
    "        'pearson_correlation': [pearson_correlation]\n",
    "    })\n",
    "\n",
    "    return robustness_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness metrics for LIME:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_squared_difference</th>\n",
       "      <th>mean_absolute_difference</th>\n",
       "      <th>spearman_correlation</th>\n",
       "      <th>pearson_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.991705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_squared_difference  mean_absolute_difference  spearman_correlation  \\\n",
       "0                 0.000006                  0.002102              0.928571   \n",
       "\n",
       "   pearson_correlation  \n",
       "0             0.991705  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness metrics for SHAP:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_squared_difference</th>\n",
       "      <th>mean_absolute_difference</th>\n",
       "      <th>spearman_correlation</th>\n",
       "      <th>pearson_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.768568</td>\n",
       "      <td>0.962927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_squared_difference  mean_absolute_difference  spearman_correlation  \\\n",
       "0                  0.00003                  0.003304              0.768568   \n",
       "\n",
       "   pearson_correlation  \n",
       "0             0.962927  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness metrics for Anchor:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_squared_difference</th>\n",
       "      <th>mean_absolute_difference</th>\n",
       "      <th>spearman_correlation</th>\n",
       "      <th>pearson_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042729</td>\n",
       "      <td>0.079224</td>\n",
       "      <td>0.40137</td>\n",
       "      <td>0.423505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_squared_difference  mean_absolute_difference  spearman_correlation  \\\n",
       "0                 0.042729                  0.079224               0.40137   \n",
       "\n",
       "   pearson_correlation  \n",
       "0             0.423505  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Robustness metrics for LIME:\")\n",
    "display(robustness_metrics(lime_ranking, lime_ranking_noisy))\n",
    "\n",
    "print(\"Robustness metrics for SHAP:\")\n",
    "display(robustness_metrics(shap_ranking, shap_ranking_noisy))\n",
    "\n",
    "print(\"Robustness metrics for Anchor:\")\n",
    "display(robustness_metrics(anchor_ranking, anchor_ranking_noisy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instance(instance_data_row):\n",
    "    lime_ranking = feature_importances.get_lime_ranking(instance_data_row)\n",
    "    lime_ranking_noisy = feature_importances_noisy.get_lime_ranking(instance_data_row)\n",
    "    lime_instance_metrics = robustness_metrics(lime_ranking, lime_ranking_noisy)\n",
    "\n",
    "    shap_ranking = feature_importances.get_shap_ranking(instance_data_row, explainer_type=shap.TreeExplainer)\n",
    "    shap_ranking_noisy = feature_importances_noisy.get_shap_ranking(instance_data_row, explainer_type=shap.TreeExplainer)\n",
    "    shap_instance_metrics = robustness_metrics(shap_ranking, shap_ranking_noisy)\n",
    "\n",
    "    anchor_ranking = feature_importances.get_anchor_ranking(instance_data_row)\n",
    "    anchor_ranking_noisy = feature_importances_noisy.get_anchor_ranking(instance_data_row)\n",
    "    anchor_instance_metrics = robustness_metrics(anchor_ranking, anchor_ranking_noisy)\n",
    "\n",
    "    ### Use toposis to assign weights to the explanation models:\n",
    "    evaluation_matrix = np.array([\n",
    "        lime_instance_metrics.values.flatten(),\n",
    "        shap_instance_metrics.values.flatten(),\n",
    "        anchor_instance_metrics.values.flatten()\n",
    "    ])\n",
    "\n",
    "    display(evaluation_matrix)\n",
    "\n",
    "    robustness_metrics_weights = [\n",
    "        0.25, # mean squared difference\n",
    "        0.25, # mean absolute difference\n",
    "        0.25, # spearman correlation\n",
    "        0.25  # pearson correlation\n",
    "    ]\n",
    "\n",
    "    # if higher value is preferred - True\n",
    "    # if lower value is preferred - False\n",
    "    criterias = np.array([\n",
    "        False,  # For mean_squared_difference, lower is better\n",
    "        False,  # For mean_absolute_difference, lower is better\n",
    "        True,   # For spearman_correlation, higher is better\n",
    "        True    # For pearson_correlation, higher is better\n",
    "    ])\n",
    "\n",
    "    t = Topsis(evaluation_matrix, robustness_metrics_weights, criterias, debug=True)\n",
    "    t.calc()\n",
    "\n",
    "    display(t.best_similarity)\n",
    "    # Sujoy suggested using the best_similarity as weights for the explanations\n",
    "\n",
    "    lime_weight, shap_weight, anchor_weight = t.best_similarity\n",
    "    result = lime_ranking.merge(shap_ranking, on='feature', how='outer', suffixes=('_lime', '_shap'))\n",
    "    result = result.merge(anchor_ranking, on='feature', how='outer', suffixes=('_lime', '_shap'))\n",
    "    result = result.rename(columns={'score': 'score_anchor'})\n",
    "    result['score_ensemble'] = lime_weight * result['score_lime'] + shap_weight * result['score_shap'] + anchor_weight * result['score_anchor']\n",
    "    result = result.sort_values(by='score_ensemble', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000497, 0.00163388, 0.91182266, 0.99251495],\n",
       "       [0.00000336, 0.00111344, 0.97746029, 0.99537514],\n",
       "       [0.02586741, 0.03827903, 0.6890411 , 0.55892702]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Evaluation Matrix:\n",
      " [[0.00000497 0.00163388 0.91182266 0.99251495]\n",
      " [0.00000336 0.00111344 0.97746029 0.99537514]\n",
      " [0.02586741 0.03827903 0.6890411  0.55892702]]\n",
      "\n",
      "Step 2 - Normalized Evaluation Matrix:\n",
      " [[0.000192   0.04262671 0.60631723 0.65612208]\n",
      " [0.00013    0.02904875 0.64996302 0.65801287]\n",
      " [0.99999997 0.99866868 0.45817844 0.36949001]]\n",
      "\n",
      "Step 3 - Weighted Normalized Evaluation Matrix\n",
      " [[0.000048   0.01065668 0.15157931 0.16403052]\n",
      " [0.0000325  0.00726219 0.16249076 0.16450322]\n",
      " [0.24999999 0.24966717 0.11454461 0.0923725 ]]\n",
      "\n",
      "Step 4 - worst_alternatives | best_alternatives \n",
      " [0.24999999 0.24966717 0.11454461 0.0923725 ]  |  [0.0000325  0.00726219 0.16249076 0.16450322]\n",
      "\n",
      "Step 5 - Distances to Worst Alternative | Distances to Best Alternative\n",
      " [0.35511752 0.35881136 0.        ] [0.01143704 0.         0.35881136]\n",
      "\n",
      "Step 6 - Similarites to Worst Alternative | Similarities to Best Alternative\n",
      " [0.96879853 1.         0.        ] [0.03120147 0.         1.        ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.03120147, 0.        , 1.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score_lime</th>\n",
       "      <th>score_shap</th>\n",
       "      <th>score_anchor</th>\n",
       "      <th>score_ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Purpose_furniture_equipment</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.009065</td>\n",
       "      <td>0.813893</td>\n",
       "      <td>0.813893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Age</td>\n",
       "      <td>-0.027337</td>\n",
       "      <td>0.044427</td>\n",
       "      <td>0.503277</td>\n",
       "      <td>0.502424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Duration</td>\n",
       "      <td>0.054827</td>\n",
       "      <td>0.051965</td>\n",
       "      <td>0.433814</td>\n",
       "      <td>0.435525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Checking_account_little</td>\n",
       "      <td>0.038031</td>\n",
       "      <td>0.030740</td>\n",
       "      <td>0.283093</td>\n",
       "      <td>0.284280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Checking_account_moderate</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>-0.025005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Credit_amount</td>\n",
       "      <td>0.008780</td>\n",
       "      <td>0.018809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Saving_accounts_little</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>-0.003987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Purpose_education</td>\n",
       "      <td>0.006638</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sex_female</td>\n",
       "      <td>0.005948</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Purpose_business</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Purpose_car</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Housing_free</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Housing_rent</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Saving_accounts_moderate</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Purpose_repairs</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Purpose_domestic_appliances</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Job_skilled</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.001720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Saving_accounts_quite_rich</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.001809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Purpose_vacation_others</td>\n",
       "      <td>-0.000782</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Saving_accounts_rich</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Job_highlyskilled</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Job_unskilled_resident</td>\n",
       "      <td>-0.002261</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Checking_account_rich</td>\n",
       "      <td>-0.002773</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Job_unskilled_nonresident</td>\n",
       "      <td>-0.002872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Saving_accounts_none</td>\n",
       "      <td>-0.005079</td>\n",
       "      <td>-0.004482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>-0.007554</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Purpose_radio_TV</td>\n",
       "      <td>-0.009799</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Housing_own</td>\n",
       "      <td>-0.014710</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Checking_account_none</td>\n",
       "      <td>-0.057115</td>\n",
       "      <td>-0.048818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature  score_lime  score_shap  score_anchor  \\\n",
       "0   Purpose_furniture_equipment    0.000012    0.009065      0.813893   \n",
       "1                           Age   -0.027337    0.044427      0.503277   \n",
       "2                      Duration    0.054827    0.051965      0.433814   \n",
       "3       Checking_account_little    0.038031    0.030740      0.283093   \n",
       "4     Checking_account_moderate    0.016900   -0.025005      0.000000   \n",
       "5                 Credit_amount    0.008780    0.018809      0.000000   \n",
       "6        Saving_accounts_little    0.008481   -0.003987      0.000000   \n",
       "7             Purpose_education    0.006638    0.001744      0.000000   \n",
       "8                    Sex_female    0.005948    0.007021      0.000000   \n",
       "9              Purpose_business    0.005631   -0.000601      0.000000   \n",
       "10                  Purpose_car    0.005609    0.006882      0.000000   \n",
       "11                 Housing_free    0.004372    0.006844      0.000000   \n",
       "12                 Housing_rent    0.004186    0.000178      0.000000   \n",
       "13     Saving_accounts_moderate    0.003105    0.011132      0.000000   \n",
       "14              Purpose_repairs    0.001892    0.001397      0.000000   \n",
       "15  Purpose_domestic_appliances    0.000880    0.000000      0.000000   \n",
       "16                  Job_skilled    0.000589   -0.001720      0.000000   \n",
       "17   Saving_accounts_quite_rich   -0.000178   -0.001809      0.000000   \n",
       "18      Purpose_vacation_others   -0.000782    0.000760      0.000000   \n",
       "19         Saving_accounts_rich   -0.000865    0.000569      0.000000   \n",
       "20            Job_highlyskilled   -0.001111    0.002708      0.000000   \n",
       "21       Job_unskilled_resident   -0.002261    0.004134      0.000000   \n",
       "22        Checking_account_rich   -0.002773   -0.000486      0.000000   \n",
       "23    Job_unskilled_nonresident   -0.002872    0.000000      0.000000   \n",
       "24         Saving_accounts_none   -0.005079   -0.004482      0.000000   \n",
       "25                     Sex_male   -0.007554    0.004218      0.000000   \n",
       "26             Purpose_radio_TV   -0.009799   -0.000356      0.000000   \n",
       "27                  Housing_own   -0.014710    0.001668      0.000000   \n",
       "28        Checking_account_none   -0.057115   -0.048818      0.000000   \n",
       "\n",
       "    score_ensemble  \n",
       "0         0.813893  \n",
       "1         0.502424  \n",
       "2         0.435525  \n",
       "3         0.284280  \n",
       "4         0.000527  \n",
       "5         0.000274  \n",
       "6         0.000265  \n",
       "7         0.000207  \n",
       "8         0.000186  \n",
       "9         0.000176  \n",
       "10        0.000175  \n",
       "11        0.000136  \n",
       "12        0.000131  \n",
       "13        0.000097  \n",
       "14        0.000059  \n",
       "15        0.000027  \n",
       "16        0.000018  \n",
       "17       -0.000006  \n",
       "18       -0.000024  \n",
       "19       -0.000027  \n",
       "20       -0.000035  \n",
       "21       -0.000071  \n",
       "22       -0.000087  \n",
       "23       -0.000090  \n",
       "24       -0.000158  \n",
       "25       -0.000236  \n",
       "26       -0.000306  \n",
       "27       -0.000459  \n",
       "28       -0.001782  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "explain_instance(X_test.iloc[sample_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pevious attempt at step 5 (IGNORE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Calculate the robustness metrics for all samples in the set, so that we're able to normalize test instance metrics later\n",
    "\n",
    "feature_importances = FeatureImportanceCalculator(clf, X_train)\n",
    "feature_importances_noisy = FeatureImportanceCalculator(clf, X_train_noisy)\n",
    "\n",
    "lime_metrics = []\n",
    "shap_metrics = []\n",
    "anchor_metrics = []\n",
    "\n",
    "for i in range(len(X_test)):    # TODO: i think this has to come from X_train, not X_test\n",
    "    lime_ranking = feature_importances.get_lime_ranking(X_test.iloc[i])\n",
    "    lime_ranking_noisy = feature_importances_noisy.get_lime_ranking(X_test_noisy.iloc[i])\n",
    "\n",
    "    shap_ranking = feature_importances.get_shap_ranking(X_test.iloc[i], explainer_type=shap.TreeExplainer)\n",
    "    shap_ranking_noisy = feature_importances_noisy.get_shap_ranking(X_test_noisy.iloc[i], explainer_type=shap.TreeExplainer)\n",
    "\n",
    "    anchor_ranking = feature_importances.get_anchor_ranking(X_test.iloc[i])\n",
    "    anchor_ranking_noisy = feature_importances_noisy.get_anchor_ranking(X_test_noisy.iloc[i])\n",
    "\n",
    "    lime_metrics.append(robustness_metrics(lime_ranking, lime_ranking_noisy))\n",
    "    shap_metrics.append(robustness_metrics(shap_ranking, shap_ranking_noisy))\n",
    "    anchor_metrics.append(robustness_metrics(anchor_ranking, anchor_ranking_noisy))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# print min max and average of the metrics\n",
    "LIME_metrics_df = pd.concat(lime_metrics)\n",
    "SHAP_metrics_df = pd.concat(shap_metrics)\n",
    "anchor_metrics_df = pd.concat(anchor_metrics)\n",
    "\n",
    "print(\"LIME metrics:\")\n",
    "display(LIME_metrics_df.describe())\n",
    "print(\"SHAP metrics:\")\n",
    "display(SHAP_metrics_df.describe())\n",
    "print(\"Anchor metrics:\")\n",
    "display(anchor_metrics_df.describe())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# For a given instance, normalize the metrics:\n",
    "def normalize_metrics(instance_metrics: pd.Series, training_metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the normalized metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_metrics = (instance_metrics - training_metrics_df.min()) / (training_metrics_df.max() - training_metrics_df.min())\n",
    "\n",
    "    return normalized_metrics\n",
    "\n",
    "# Example of weight calculation for the first instance:\n",
    "normalized_metrics = normalize_metrics(LIME_metrics_df.iloc[0], LIME_metrics_df)\n",
    "weight = normalized_metrics.mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def explain_instance(instance_data_row):\n",
    "    lime_ranking = feature_importances.get_lime_ranking(instance_data_row)\n",
    "    lime_ranking_noisy = feature_importances_noisy.get_lime_ranking(instance_data_row)\n",
    "    lime_instance_metrics = robustness_metrics(lime_ranking, lime_ranking_noisy)\n",
    "    lime_normalized_metrics = normalize_metrics(lime_instance_metrics, LIME_metrics_df)\n",
    "    lime_weight = np.mean(lime_normalized_metrics)\n",
    "\n",
    "    shap_ranking = feature_importances.get_shap_ranking(instance_data_row, explainer_type=shap.TreeExplainer)\n",
    "    shap_ranking_noisy = feature_importances_noisy.get_shap_ranking(instance_data_row, explainer_type=shap.TreeExplainer)\n",
    "    shap_instance_metrics = robustness_metrics(shap_ranking, shap_ranking_noisy)\n",
    "    shap_normalized_metrics = normalize_metrics(shap_instance_metrics, SHAP_metrics_df)\n",
    "    shap_weight = np.mean(shap_normalized_metrics)\n",
    "\n",
    "    anchor_ranking = feature_importances.get_anchor_ranking(instance_data_row)\n",
    "    anchor_ranking_noisy = feature_importances_noisy.get_anchor_ranking(instance_data_row)\n",
    "    anchor_instance_metrics = robustness_metrics(anchor_ranking, anchor_ranking_noisy)\n",
    "    anchor_normalized_metrics = normalize_metrics(anchor_instance_metrics, anchor_metrics_df)\n",
    "    anchor_weight = np.mean(anchor_normalized_metrics)\n",
    "\n",
    "    # Normalize the weights:\n",
    "    weights = np.array([lime_weight, shap_weight, anchor_weight])\n",
    "    normalized_weights = weights / weights.sum()\n",
    "\n",
    "    print(f'LIME weight: {lime_weight}')\n",
    "    print(f'SHAP weight: {shap_weight}')\n",
    "    print(f'Anchor weight: {anchor_weight}')\n",
    "    print(f'Normalized weights: {normalized_weights}')\n",
    "\n",
    "    # Combine the explanations, sum of the rankings weighted by the normalized metrics:\n",
    "    result = lime_ranking.merge(shap_ranking, on='feature', how='outer', suffixes=('_lime', '_shap'))\n",
    "    result = result.merge(anchor_ranking, on='feature', how='outer', suffixes=('_lime', '_shap'))\n",
    "    result = result.rename(columns={'score': 'score_anchor'})\n",
    "    result['score'] = lime_weight * result['score_lime'] + shap_weight * result['score_shap'] + anchor_weight * result['score_anchor']\n",
    "    result = result.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "explain_instance(X_test.iloc[sample_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring different XAI Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faithfullness(predict_proba, explain_instance: callable, instance_data_row: pd.Series, len_subset: int = None, iterations: int = 10, baseline_strategy: str = \"zeros\") -> float:\n",
    "    dimension = len(instance_data_row)\n",
    "\n",
    "    importance_sums = []\n",
    "    delta_fs = []\n",
    "\n",
    "    f_x = predict_proba(instance_data_row.to_numpy().reshape(1, -1))[0][1]\n",
    "    display(f_x)\n",
    "    g_x = explain_instance(instance_data_row)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Select a subset of features to perturb\n",
    "        subset = np.random.choice(instance_data_row.index.values, len_subset if len_subset else dimension/4, replace=False)\n",
    "\n",
    "        perturbed_instance = instance_data_row.copy()\n",
    "\n",
    "        if baseline_strategy == \"zeros\":\n",
    "            baseline = np.zeros(dimension)  # either mean on all zeros\n",
    "        elif baseline_strategy == \"mean\":\n",
    "            pass # yet to be defined\n",
    "        perturbed_instance[subset] = baseline[instance_data_row.index.get_indexer(subset)]\n",
    "\n",
    "        importance_sum = 0\n",
    "        for feature in subset:\n",
    "            importance_sum += g_x[g_x['feature'] == feature]['score'].values[0] # should I take the abs value here?\n",
    "        importance_sums.append(importance_sum)\n",
    "\n",
    "        f_x_perturbed = predict_proba(perturbed_instance.to_numpy().reshape(1, -1))[0][1]\n",
    "        delta_f = np.abs(f_x - f_x_perturbed)\n",
    "        delta_fs.append(delta_f)\n",
    "    \n",
    "    # display(importance_sums, delta_fs)\n",
    "    return pearsonr(importance_sums, delta_fs).statistic\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.399866252455617"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = FeatureImportanceCalculator(clf, X_train)\n",
    "faithfullness(clf.predict_proba, lambda instance: g.get_shap_ranking(instance, explainer_type=shap.TreeExplainer), X_test.iloc[sample_idx], len_subset=10, iterations=1000, baseline_strategy=\"zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderNoisyDataGenerator():\n",
    "    def __init__(self, X: pd.DataFrame, ohe_categorical_features_names: list[str], encoding_dim: int = 5, epochs=500):\n",
    "        self.X = X\n",
    "        self.categorical_features_names = ohe_categorical_features_names\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.epochs = epochs\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        self.X_scaled = scaler.fit_transform(self.X)\n",
    "        \n",
    "        input_dim = self.X_scaled.shape[1]\n",
    "\n",
    "        input_layer = Input(shape=(input_dim,))\n",
    "        encoded = Dense(self.encoding_dim, activation='relu')(input_layer)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "        self.autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "        self.encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "        self.was_fit = False\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        self.autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "        self.autoencoder.fit(self.X_scaled, self.X_scaled, epochs=self.epochs, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "        # Extract hidden layer representation:\n",
    "        self.hidden_representation = self.encoder.predict(self.X_scaled)\n",
    "        self.was_fit = True\n",
    "\n",
    "\n",
    "    def generate_noisy_data(self, num_features_to_replace: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame containing a noisy variation of the data.\n",
    "\n",
    "        The noise is generated by swapping the values of a small number of features between a sample and a random close neighbor.\n",
    "        To determine the neighbors, we use an autoencoder to reduce the dimensionality of the data and then calculate the use the NearestNeightbors algorithm in the reduced space.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.was_fit:\n",
    "            raise ValueError('The autoencoder has not been fitted yet. Call the fit() method before generating noisy data.')\n",
    "\n",
    "        # Compute Nearest Neighbors using hidden_representation\n",
    "        nbrs = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(self.hidden_representation)\n",
    "        distances, indices = nbrs.kneighbors(self.hidden_representation)\n",
    "\n",
    "        X_noisy = self.X.copy()\n",
    "\n",
    "        # Get id's of columns that belong to the same categorical feature (after being one-hot-encodeded);\n",
    "        # Columns that belong to the same categorical feature start with the same name, and will be treated as a single feature when adding noise.\n",
    "        categorical_features_indices = [\n",
    "            [self.X.columns.get_loc(col_name) for col_name in self.X.columns if col_name.startswith(feature)]\n",
    "            for feature in self.categorical_features_names\n",
    "        ]\n",
    "\n",
    "        # Replace features with random neighbor's features\n",
    "        for i in range(self.X.shape[0]):  # Iterate over each sample\n",
    "            available_features_to_replace = list(range(self.X.shape[1]))\n",
    "            for j in range(num_features_to_replace):\n",
    "                # Select features to replace; if the feture selected belong to one of the lists in categorical_features_indices, we will replace all the features in that list\n",
    "                features_to_replace = np.random.choice(available_features_to_replace, 1)\n",
    "                for feature_indices in categorical_features_indices:\n",
    "                    if features_to_replace in feature_indices:\n",
    "                        features_to_replace = feature_indices\n",
    "                        break\n",
    "                \n",
    "                # Remove the selected features from the list of available features to replace\n",
    "                available_features_to_replace = [f for f in available_features_to_replace if f not in features_to_replace]\n",
    "\n",
    "                # Choose a random neighbor from the nearest neighbors\n",
    "                neighbor_idx = np.random.choice(indices[i][1:])\n",
    "\n",
    "                # Replace the selected features with the neighbor's features\n",
    "                X_noisy.iloc[i, features_to_replace] = self.X.iloc[neighbor_idx, features_to_replace]\n",
    "\n",
    "        return X_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2329 - val_loss: 1.2192\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2264 - val_loss: 1.2028\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2100 - val_loss: 1.1872\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1810 - val_loss: 1.1717\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2186 - val_loss: 1.1556\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1608 - val_loss: 1.1391\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1769 - val_loss: 1.1231\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1139 - val_loss: 1.1075\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0973 - val_loss: 1.0924\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0747 - val_loss: 1.0774\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mean Absolute Difference: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.946716680977991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage Example:\n",
    "autoencoder_noisy_data_generator = AutoencoderNoisyDataGenerator(X_train, categorical_features, encoding_dim=5, epochs=10)\n",
    "autoencoder_noisy_data_generator.fit()\n",
    "noisy_data = autoencoder_noisy_data_generator.generate_noisy_data(num_features_to_replace=2)\n",
    "display(\"Mean Absolute Difference: \", np.mean(np.abs(X_train - noisy_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainerWrapper:\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        self.clf = clf\n",
    "\n",
    "        if hasattr(clf, 'predict_proba') and predict_proba is None:\n",
    "            self.predict_proba = clf.predict_proba\n",
    "        elif predict_proba is not None:\n",
    "            self.predict_proba = predict_proba\n",
    "        else:\n",
    "            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.categorical_feature_names = categorical_feature_names\n",
    "\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "class LimeWrapper(ExplainerWrapper):\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "        \n",
    "        self.explainer = LimeTabularExplainer(self.X_train.values, feature_names=self.X_train.columns, discretize_continuous=False)\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        lime_exp = self.explainer.explain_instance(instance_data_row, self.predict_proba, num_features=len(self.X_train.columns))\n",
    "        \n",
    "        ranking = pd.DataFrame(lime_exp.as_list(), columns=['feature', 'score'])\n",
    "        return ranking\n",
    "\n",
    "class ShapTabularTreeWrapper(ExplainerWrapper):\n",
    "    \n",
    "        def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None, **additional_explainer_args):\n",
    "            super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "            \n",
    "            self.explainer = shap.TreeExplainer(clf, self.X_train, **additional_explainer_args)\n",
    "        \n",
    "        def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "            shap_values = self.explainer.shap_values(instance_data_row)\n",
    "    \n",
    "            ranking = pd.DataFrame(list(zip(self.X_train.columns, shap_values[:, 0])), columns=['feature', 'score'])\n",
    "            ranking = ranking.sort_values(by='score', ascending=False, key=lambda x: abs(x)).reset_index(drop=True)\n",
    "            \n",
    "            return ranking\n",
    "\n",
    "class AnchorWrapper(ExplainerWrapper):\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, categorical_feature_names: list[str], predict_proba: callable = None):\n",
    "        super().__init__(clf, X_train, categorical_feature_names, predict_proba)\n",
    "        \n",
    "        self.explainer = AnchorTabular(predictor=self.predict_proba, feature_names=self.X_train.columns) # TODO: fix parameters\n",
    "        self.explainer.fit(self.X_train.values)\n",
    "    \n",
    "    def explain_instance(self, instance_data_row: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "        if isinstance(instance_data_row, pd.Series):\n",
    "            instance_data_row = instance_data_row.to_numpy()\n",
    "\n",
    "        feature_importances = {feature: 0 for feature in self.X_train.columns}\n",
    "        explanation = self.explainer.explain(instance_data_row)\n",
    "        \n",
    "        for rule in explanation.anchor:\n",
    "            # Extract the feature name from the rule string\n",
    "            # This method won't work for column names that have spaces in them or that don't contain any letters\n",
    "            for expression_element in rule.split():\n",
    "                if any(c.isalpha() for c in expression_element):\n",
    "                    referenced_feature = expression_element\n",
    "                    break\n",
    "\n",
    "            rule_coverage = self.X_train.query(rule).shape[0] / self.X_train.shape[0]\n",
    "            feature_importances[referenced_feature] = 1 - rule_coverage\n",
    "        \n",
    "        return pd.DataFrame(list(feature_importances.items()), columns=['feature', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XaiEvaluator:\n",
    "\n",
    "    def __init__(self, clf, X_train: pd.DataFrame | np.ndarray, ohe_categorical_feature_names: list[str], predict_proba: callable = None, noise_gen_args: dict = {}):\n",
    "        self.clf = clf\n",
    "        if hasattr(clf, 'predict_proba') and predict_proba is None:\n",
    "            self.predict_proba = clf.predict_proba\n",
    "        elif predict_proba is not None:\n",
    "            self.predict_proba = predict_proba\n",
    "        else:\n",
    "            raise ValueError('The classifier does not have a predict_proba method and no predict_proba_function was provided.')\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.ohe_categorical_feature_names = ohe_categorical_feature_names\n",
    "\n",
    "        self.categorical_features_indices = [\n",
    "            [self.X_train.columns.get_loc(col_name) for col_name in self.X_train.columns if col_name.startswith(feature)]\n",
    "            for feature in self.ohe_categorical_feature_names\n",
    "        ]\n",
    "\n",
    "        self.noisy_data_generator = AutoencoderNoisyDataGenerator(X_train, ohe_categorical_feature_names, **noise_gen_args)\n",
    "\n",
    "        self.was_initialized = False\n",
    "    \n",
    "    # Initialization opeations that take a long time to run\n",
    "    def init(self):\n",
    "        self.noisy_data_generator.fit()\n",
    "        self.was_initialized = True\n",
    "            \n",
    "    def faithfullness_correlation(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series, len_subset: int = None, iterations: int = 100, baseline_strategy: str = \"zeros\") -> float:\n",
    "        if not isinstance(explainer, ExplainerWrapper):\n",
    "            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "        \n",
    "        dimension = len(instance_data_row)\n",
    "\n",
    "        importance_sums = []\n",
    "        delta_fs = []\n",
    "\n",
    "        f_x = self.predict_proba(instance_data_row.to_numpy().reshape(1, -1))[0][1]\n",
    "        g_x = explainer.explain_instance(instance_data_row)\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            # Select a subset of features to perturb\n",
    "            subset = np.random.choice(instance_data_row.index.values, len_subset if len_subset else dimension/4, replace=False)\n",
    "\n",
    "            perturbed_instance = instance_data_row.copy()\n",
    "\n",
    "            if baseline_strategy == \"zeros\":\n",
    "                baseline = np.zeros(dimension)  # either mean on all zeros\n",
    "            elif baseline_strategy == \"mean\":\n",
    "                baseline = np.mean(self.X_train, axis=0)\n",
    "                for feature_index in self.categorical_features_indices:\n",
    "                    baseline[feature_index] = 0\n",
    "                \n",
    "            perturbed_instance[subset] = baseline[instance_data_row.index.get_indexer(subset)]\n",
    "\n",
    "            importance_sum = 0\n",
    "            for feature in subset:\n",
    "                importance_sum += g_x[g_x['feature'] == feature]['score'].values[0] # should I take the abs value here?\n",
    "            importance_sums.append(importance_sum)\n",
    "\n",
    "            f_x_perturbed = self.predict_proba(perturbed_instance.to_numpy().reshape(1, -1))[0][1]\n",
    "            delta_f = np.abs(f_x - f_x_perturbed)\n",
    "            delta_fs.append(delta_f)\n",
    "        \n",
    "        return pearsonr(importance_sums, delta_fs).statistic\n",
    "    \n",
    "    def sensitivity(self, ExplainerType: Type[ExplainerWrapper], instance_data_row: pd.Series, iterations: int = 10):\n",
    "        if not self.was_initialized:\n",
    "            raise ValueError('The XaiEvaluator has not been initialized yet. Call the init() method before evaluating sensitivity.')\n",
    "        \n",
    "        original_explainer = ExplainerType(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "\n",
    "        mean_squared_differences: list[float] = []\n",
    "        for _ in range(iterations):\n",
    "            # Obtain the original explanation:\n",
    "            original_explanation = original_explainer.explain_instance(instance_data_row)\n",
    "\n",
    "            # Obtain the noisy explanation:\n",
    "            noisy_data = self.noisy_data_generator.generate_noisy_data(num_features_to_replace=2)\n",
    "            noisy_explainer = ExplainerType(self.clf, noisy_data, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "            noisy_explanation = noisy_explainer.explain_instance(instance_data_row)\n",
    "\n",
    "            # Compute the mean squared difference between the scores of the original and noisy explanations:\n",
    "            mean_squared_difference = ((original_explanation['score'] - noisy_explanation['score']) ** 2).mean()\n",
    "            mean_squared_differences.append(mean_squared_difference)\n",
    "        \n",
    "        return np.mean(mean_squared_differences)\n",
    "\n",
    "    def complexity(self, explainer: ExplainerWrapper | Type[ExplainerWrapper], instance_data_row: pd.Series) -> float:\n",
    "        if not isinstance(explainer, ExplainerWrapper):\n",
    "            explainer = explainer(self.clf, self.X_train, self.ohe_categorical_feature_names, predict_proba=self.predict_proba)\n",
    "\n",
    "        explanation = explainer.explain_instance(instance_data_row)\n",
    "\n",
    "        def frac_contribution(explanation: pd.DataFrame, i: int) -> float:\n",
    "            abs_score_sum = explanation['score'].abs().sum()\n",
    "            return explanation['score'].abs()[i] / abs_score_sum\n",
    "\n",
    "        sum = 0\n",
    "        for i in range(explanation.shape[0]):\n",
    "            fc = frac_contribution(explanation, i)\n",
    "            sum += fc * np.log(fc) if fc > 0 else 0\n",
    "            \n",
    "        return -sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2447 - val_loss: 1.2493\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2534 - val_loss: 1.2316\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2402 - val_loss: 1.2142\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2218 - val_loss: 1.1972\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1781 - val_loss: 1.1805\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1531 - val_loss: 1.1641\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1452 - val_loss: 1.1476\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1532 - val_loss: 1.1316\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1181 - val_loss: 1.1161\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1036 - val_loss: 1.1012\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "g = FeatureImportanceCalculator(clf, X_train)\n",
    "xai_eval = XaiEvaluator(clf, X_train, categorical_features, noise_gen_args={'encoding_dim': 5, 'epochs': 10})\n",
    "xai_eval.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4220889429385705"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0035197446220883487"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faithfullness =  xai_eval.faithfullness_correlation(AnchorWrapper,\n",
    "                                                    X_test.iloc[sample_idx], len_subset=10, iterations=100, baseline_strategy=\"mean\")\n",
    "display(faithfullness)\n",
    "sensitivity = xai_eval.sensitivity(AnchorWrapper, X_test.iloc[sample_idx], iterations=10)\n",
    "display(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.516308873975537"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "complexity = xai_eval.complexity(ShapTabularTreeWrapper, X_test.iloc[sample_idx])\n",
    "display(complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xai Evaluation Metrics Trends for Selected Explanation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n",
      "Could not find an anchor satisfying the 0.95 precision constraint. Now returning the best non-eligible result. The desired precision threshold might not be achieved due to the quantile-based discretisation of the numerical features. The resolution of the bins may be too large to find an anchor of required precision. Consider increasing the number of bins in `disc_perc`, but note that for some numerical distribution (e.g. skewed distribution) it may not help.\n"
     ]
    }
   ],
   "source": [
    "shap_sensitivities = []\n",
    "shap_faithfullnesses = []\n",
    "shap_complexities = []\n",
    "\n",
    "lime_sensitivities = []\n",
    "lime_faithfullnesses = []\n",
    "lime_complexities = []\n",
    "\n",
    "anchor_sensitivities = []\n",
    "anchor_faithfullnesses = []\n",
    "anchor_complexities = []\n",
    "\n",
    "shap_exp = ShapTabularTreeWrapper(clf, X_train, categorical_features)\n",
    "lime_exp = LimeWrapper(clf, X_train, categorical_features)\n",
    "anchor_exp = AnchorWrapper(clf, X_train, categorical_features)\n",
    "\n",
    "for i in range(50):\n",
    "    sample_idx = i\n",
    "    shap_sensitivities.append(xai_eval.sensitivity(ShapTabularTreeWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "    lime_sensitivities.append(xai_eval.sensitivity(LimeWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "    anchor_sensitivities.append(xai_eval.sensitivity(AnchorWrapper, X_test.iloc[sample_idx], iterations=10))\n",
    "\n",
    "    shap_faithfullnesses.append(xai_eval.faithfullness_correlation(shap_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "    lime_faithfullnesses.append(xai_eval.faithfullness_correlation(lime_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "    anchor_faithfullnesses.append(xai_eval.faithfullness_correlation(anchor_exp, X_test.iloc[sample_idx], len_subset=10, iterations=10, baseline_strategy=\"mean\"))\n",
    "\n",
    "    shap_complexities.append(xai_eval.complexity(shap_exp, X_test.iloc[sample_idx]))\n",
    "    lime_complexities.append(xai_eval.complexity(lime_exp, X_test.iloc[sample_idx]))\n",
    "    anchor_complexities.append(xai_eval.complexity(anchor_exp, X_test.iloc[sample_idx]))\n",
    "\n",
    "shap_metrics = pd.DataFrame({\n",
    "    'faithfullness': shap_faithfullnesses,\n",
    "    'complexity': shap_complexities\n",
    "})\n",
    "\n",
    "lime_metrics = pd.DataFrame({\n",
    "    'faithfullness': lime_faithfullnesses,\n",
    "    'complexity': lime_complexities\n",
    "})\n",
    "\n",
    "anchor_metrics = pd.DataFrame({\n",
    "    'faithfullness': anchor_faithfullnesses,\n",
    "    'complexity': anchor_complexities\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.024205</td>\n",
       "      <td>2.470693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.587730</td>\n",
       "      <td>0.146090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.827780</td>\n",
       "      <td>2.071901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.562122</td>\n",
       "      <td>2.383871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.032993</td>\n",
       "      <td>2.485882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.582136</td>\n",
       "      <td>2.559405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.834696</td>\n",
       "      <td>2.731101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      50.000000   50.000000\n",
       "mean        0.024205    2.470693\n",
       "std         0.587730    0.146090\n",
       "min        -0.827780    2.071901\n",
       "25%        -0.562122    2.383871\n",
       "50%         0.032993    2.485882\n",
       "75%         0.582136    2.559405\n",
       "max         0.834696    2.731101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.023739</td>\n",
       "      <td>2.589746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.410476</td>\n",
       "      <td>0.048431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.801083</td>\n",
       "      <td>2.478361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.374738</td>\n",
       "      <td>2.554466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.045940</td>\n",
       "      <td>2.581921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.312178</td>\n",
       "      <td>2.625277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.694030</td>\n",
       "      <td>2.709051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      50.000000   50.000000\n",
       "mean       -0.023739    2.589746\n",
       "std         0.410476    0.048431\n",
       "min        -0.801083    2.478361\n",
       "25%        -0.374738    2.554466\n",
       "50%         0.045940    2.581921\n",
       "75%         0.312178    2.625277\n",
       "max         0.694030    2.709051"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfullness</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.212699</td>\n",
       "      <td>1.373141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.450593</td>\n",
       "      <td>0.555356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.734714</td>\n",
       "      <td>0.604767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.156941</td>\n",
       "      <td>1.049478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.304438</td>\n",
       "      <td>1.306196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.628737</td>\n",
       "      <td>1.535797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.939272</td>\n",
       "      <td>2.661137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       faithfullness  complexity\n",
       "count      50.000000   50.000000\n",
       "mean        0.212699    1.373141\n",
       "std         0.450593    0.555356\n",
       "min        -0.734714    0.604767\n",
       "25%        -0.156941    1.049478\n",
       "50%         0.304438    1.306196\n",
       "75%         0.628737    1.535797\n",
       "max         0.939272    2.661137"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(shap_metrics.describe())\n",
    "display(lime_metrics.describe())\n",
    "display(anchor_metrics.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count how many times anchor_metrics['complexity'] is lower than shap_metrics['complexity'] and lime_metrics['complexity']\n",
    "anchor_complexity_lower_than_shap = anchor_metrics['complexity'] < shap_metrics['complexity']\n",
    "anchor_complexity_lower_than_lime = anchor_metrics['complexity'] < lime_metrics['complexity']\n",
    "anchor_lower_than_both = anchor_complexity_lower_than_shap & anchor_complexity_lower_than_lime\n",
    "\n",
    "display(anchor_lower_than_both.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
